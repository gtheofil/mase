{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7535,"status":"ok","timestamp":1739206357953,"user":{"displayName":"george theof","userId":"15049073496200557337"},"user_tz":0},"id":"x5YC15tBPvR6","outputId":"014aa148-0817-4413-f19c-693d975ddcf6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Python 3.11.11\n","Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (24.1.2)\n","Collecting pip\n","  Downloading pip-25.0.1-py3-none-any.whl.metadata (3.7 kB)\n","Downloading pip-25.0.1-py3-none-any.whl (1.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m56.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: pip\n","\u001b[33m  WARNING: The scripts pip, pip3 and pip3.11 are installed in '/root/.local/bin' which is not on PATH.\n","  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n","\u001b[0mSuccessfully installed pip-25.0.1\n","3.11.11 (main, Dec  4 2024, 08:55:07) [GCC 11.4.0]\n"]}],"source":[" # Check the current python version (It should be using Python 3.11) and update pip to the latest version.\n","!python3 --version\n","!python -m pip install --user --upgrade pip\n","\n","import sys\n","print(sys.version)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":29210,"status":"ok","timestamp":1739206387178,"user":{"displayName":"george theof","userId":"15049073496200557337"},"user_tz":0},"id":"Ql52TMM3PyDH","outputId":"8df175e3-699e-472c-d0de-bef3cc7dc8a8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","\n","# Mount Google Drive\n","drive.mount('/content/drive')\n","\n","# Path to the file in Google Drive\n","config_file_path = '/content/drive/My Drive/auth_config.txt'\n","\n","# Read the token and shortcode\n","auth_config = {}\n","with open(config_file_path, 'r') as file:\n","    for line in file:\n","        key, value = line.strip().split('=')\n","        auth_config[key] = value\n","\n","# Extract token and shortcode\n","git_token = auth_config.get('token')\n","short_code = auth_config.get('shortcode')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13219,"status":"ok","timestamp":1739206400399,"user":{"displayName":"george theof","userId":"15049073496200557337"},"user_tz":0},"id":"tPC3SzCtPzux","outputId":"3c3352f4-b428-4c38-9389-fbe892fb15bf"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'mase'...\n","remote: Enumerating objects: 26720, done.\u001b[K\n","remote: Counting objects: 100% (3771/3771), done.\u001b[K\n","remote: Compressing objects: 100% (555/555), done.\u001b[K\n","remote: Total 26720 (delta 3434), reused 3222 (delta 3216), pack-reused 22949 (from 2)\u001b[K\n","Receiving objects: 100% (26720/26720), 114.73 MiB | 13.17 MiB/s, done.\n","Resolving deltas: 100% (17099/17099), done.\n"]}],"source":["!git clone https://{git_token}@github.com/DeepWok/mase.git"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3Zgdkii1P1t7"},"outputs":[],"source":["%%bash\n","# Check if the branch exists. Branch exists, so checkout to that branch otherwise create a new one.\n","cd mase\n","# if git show-ref --quiet --verify \"refs/heads/lab1_${short_code}\"; then\n","#     # Branch exists, so checkout to that branch\n","#     git checkout lab1_${short_code}\n","# else\n","#     # Branch doesn't exist, so create it\n","#     git branch lab1_{short_code} git checkout lab1_\n","# {short_code}\n","# fi"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","output_embedded_package_id":"1OG4VAvkwCm_EYLo8yIcs7_GFghvX0C7J"},"executionInfo":{"elapsed":197114,"status":"ok","timestamp":1739206597546,"user":{"displayName":"george theof","userId":"15049073496200557337"},"user_tz":0},"id":"6gHpWNBDP5rg","outputId":"9da7460b-7845-4539-bcd9-1b68e5c45c13"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["%%bash\n","cd mase\n","python -m pip install -e . -vvv"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":46,"status":"ok","timestamp":1739206597986,"user":{"displayName":"george theof","userId":"15049073496200557337"},"user_tz":0},"id":"feOyJYXOP8dN","outputId":"6facefd1-2b99-44dc-db0c-cc2f41e6f12b"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/mase/src\n"]}],"source":["%cd ./mase/src/"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yDZAyityJ05G"},"outputs":[],"source":["checkpoint = \"prajjwal1/bert-tiny\"\n","tokenizer_checkpoint = \"bert-base-uncased\"\n","dataset_name = \"imdb\""]},{"cell_type":"markdown","metadata":{"id":"hqsMcxfQJ05I"},"source":["## Importing the model"]},{"cell_type":"markdown","metadata":{"id":"bFJ_rlzTJ05J"},"source":["If you are starting from scratch, you can load the Bert checkpoint directly from HuggingFace."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":205,"referenced_widgets":["4985c1de4e904c0a8aae3e3f0c7186cf","7e781d40e5a44d0cac226ff41e70e6b9","8720e0da5f364049a1d2dec3f9b44f93","d512ede0a3fc47f9afa699ad051e8bab","4ca4446475e1456ab2f4e7d48af66e24","0fcf9268779641e0a613da8f05708821","a1aebf08073f4683a7a3f9d96d24b23a","b23cb28f50404483bd04a6109eefc522","bf8ffae4f7fb4faf8e9245e0f68edc31","237a89d8e7ea44629b78c59113edc41b","d8e822c46f7c4c6c9378ae907d26a86a","4bdb10caca49409e95903539ef3a2d57","d817aa555c074874920d80a31da20404","185f544c0d4a4277993c022abc595a4d","ce2fb24ff435467fa85f917ea3ac9f79","d0aacec934814dbb81d52f289614b6e5","104919436f284f588dc83b95271eeb2f","83360339bab6416a994916e3f33c5e0e","7de5b532363b4beea02f209be88d91e6","70daed713f62419699fd08e162a4cb4b","7e9b88c1ab93441fa50ab70cf5a1eeed","8f13bcf9392441f5ad086384eedc64ae"]},"executionInfo":{"elapsed":24994,"status":"ok","timestamp":1739206623114,"user":{"displayName":"george theof","userId":"15049073496200557337"},"user_tz":0},"id":"U1_p3acFJ05J","outputId":"8a8f7897-7680-46a0-a3d7-6072071a445c"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/285 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4985c1de4e904c0a8aae3e3f0c7186cf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["pytorch_model.bin:   0%|          | 0.00/17.8M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4bdb10caca49409e95903539ef3a2d57"}},"metadata":{}}],"source":["from transformers import AutoModel\n","\n","model = AutoModel.from_pretrained(checkpoint)"]},{"cell_type":"markdown","metadata":{"id":"cUCj_g9dJ05K"},"source":["If you have previously ran the tutorial on Neural Architecture Search (NAS), run the following cell to import the best model obtained from the search process."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KAMdfEj5J05K"},"outputs":[],"source":["# from pathlib import Path\n","# import dill\n","\n","# with open(f\"{Path.home()}/tutorial_5_best_model.pkl\", \"rb\") as f:\n","#     base_model = dill.load(f)"]},{"cell_type":"markdown","metadata":{"id":"dw0o9peIJ05L"},"source":["First, fetch the dataset using the `get_tokenized_dataset` utility."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":514,"referenced_widgets":["8fa7f69d42644eb4b01a3286aa4a48fa","fbfb5b5b524b4f2bb6a0972ca5c2ee9b","b77870a34d2c44d391bec148149301c3","6ba6b74022d8432ab1422531b16c8054","a44170557bcc4441b8781e00952d25a9","9c96f0c74ed34f898078432a419326ec","7cb7c6fd261b41cb9272d09195a26d11","a86a468908a74815b85f6e8980ad6c24","5eb2734f479b46c99f5d3595a626b195","04360d9bcbc645f7b044e86d77b8150a","0d7e4dd56cd74d7dbd556a3f69850cf1","daf5e89ca30041e3a34c655982ac2ba5","23a32ae335014fa38b8b6dddb6910a5e","faf5d52ed1ab4629b1d771a7584a95ae","ffcb98c663d347f4b63cab0cf9248bc1","8fa4bbe3202a41f982936520dab17183","5b12419e752045a18987ee7843ba1627","9fdc97e32f0649cba37b9d2a3e08b2be","33dc22689933474e8b301bb77d046402","86bcf5428a4247bf8a56ddd7d73f528c","ded28a3588644a468f9449b985d94162","dfac797828554062a5e32e5e59cf20d8","0606ff59d8e54d13bdbbf666a9c5d406","6377be3b43c24160ba473463cc52a381","667b4fbc6e1940e78da9dfdd811ea08f","5e121a528ebd4ee7839337d108725711","e50d5ac9eacd4570b2c62bac468f663f","6054ea1b5c0f4b028c69fc73a5319104","41960a6da10a4abf99c48775e9945cb0","e919c893f9af4aae9a537532875681a1","67957df6de93437e858bbc48f256d5d4","a464203926c142268d1bf364073c8808","1d694e66fa47476e9e3bee7810fe5824","001c2f555cda4cdfb3405a5a80591962","029eb6fc9e474aeda4eda8c351a99642","ac56983ef01f4f3aa807fc458312b85b","d85779dbe0844791a24e32b734f451a3","343d2025538840b7ba3dfd9c13d73afa","486e8403fd1b4652a8b6e5601ed9b3c1","cd88b97c62294ac0a9e1b57374ae3888","19ebaf168afb4f90bd95ebd6856170be","0c00c33bb0f74840a87e9d298ad7c5ef","e57bcc4a8bc941bfbaf86a3ba61de7e6","e3896ee1199c41fabea1833ff55b5b95","eb2310b0751e435a9f00b150363abe2f","885cf7ab6f8149aeaa7c36b4df50e886","4d97c46543c447d8b648969da34db675","4ceae4161a8944f085fc876791d1a6bc","5bd8b360bf1e4ea8b3c35fd29599182e","e8715b393767400f8a7b97e32354f16d","874d30946b614543818170db6ba1a085","89617f043d3345b7983ceab94e8258c7","4912a318cef94789ac637ab4b60cbd51","fa5ade32f5af42958f69b84f1119b628","c3e1f43856e840d7bac85a32b2a07557","833e8d089ed44a6c881bacab8ffce4f1","6a6ef2f9a543456ab1c8caae6a67139c","f4c2cc4ff73c4d67a880145315d14295","522d31f9d89142f4a0047e822d08a445","a36729c0c60049149a5453d6db17d9cc","63334775b8e54b52b175c7cceb48ff3b","e06c01a92fd447a8b8983cc446441cf2","cfb78767d0dd4f76902a07003a4a7287","42e5ac852ccf477ba5e33bef71d9cb8e","38e3da9545f4429b94e8db46f676def8","d0fbfb436d584904a3c82e3742079107","66ab779a352e4b58994f67f048980cfe","ba868c25adfa4bc19152744cf7fa4dd6","d029eaff40674a3db02226e424ca9f58","3845c408b3b14433a30d652af354b132","9897cc4887ea4a3bb7e05cdc01864045","4cf49eb808e94e1fac62ba0bbdd636f7","beca4a8d79bc4d0089db961e425e4ee8","9d13780667e14063b826338b01b6ec31","099e9b67eeb04294b68538359a1d064a","8ac84c47d93d42699926940569ccac1c","20037fe3308043ecae050e8e31ff2cfb","5a9e25e127a749fd958bec97da0ad93b","ba700e0a69cf46efa70426312e6a3140","b67c180c1e0945a48b2fcf9f517ad232","b52ef5c2124c40ef8e7a661e947d3d35","60b6b91d91444aafa036c359a9db872c","719bc133c26c478b8cdf288a4bfa170f","d71099f8ba774d5983bea01c2dfe57b5","e310978bdd924e0daf7434a8a7fae101","7106f3946d18433fad639e5c397b4d85","ae28b7d6ea9a4a5dad73214c10510390","e0bf2769dec34e058ac7d13efa321fd8","221c08d4b7d54506ac8b849a95667f81","3ac7c83f653c429c91bdb95228c69ba5","16624b2fa5b2449ebc28b6572d025ad1","d412ed81601b457e8b77df2a657fa968","c51782c6333f4d779bd251b9f17ae0c5","3ae7939c82244f68a298be42e269ec31","048429b8243c48579f0bcab5dc789527","ffa1654320564abaae7e68df8445d5b7","ffd53cbd56564cca94fb1aec7da28e20","4bfaa17f80404bdba2ff4b4097393076","f2501667fd554c3b8c5f87e3cf3a24ef","613a3db399af439eb1f3499dd5d87c15","2f7282a9ae824298a3725ca663e74ca7","4182d7dc94a34cdf95fe003314a9f554","847f5334548946cb9ee26f2f6e1ba664","0673c93bd00f4a609a86df105ad1db9f","dea1cb33f7804aff97f29cd65fb65f1f","57afed6d5b7d45e3ac909199e003176d","d1273d3b7e984566af499694b72b97d9","acbaf720bac24105b8b57fad9cb55578","a0c918cdf14b4d3781468f9007c9536a","fdc08856ee7c49e3b1ddf508afd76ecc","4725d84ef9eb4e749f8b896224eb3115","bdf57014546c43d1808779847f0daf99","b77b1066179b4bceb599ef598dbbf6fa","9d4423b01c5545e896a1503b24a89071","faee065eba7e42d0bfd00154e19b5f36","e67c3df7dbe143f4982e9ca77a28697f","5aebef14b2a14c0db50012cc6f4a9d54","c371dd51e9e94b9ba48a9ee429db8813","671e3aef8fbf488f86e2b72d058fd892","e95d193ec23d4a26baf5b7859ca24327","671fd81bb57145eab93a79ca8e68ac1a","173076887af346e0a10810fc54918346","9b49631babb64dc8b69d41586741853e","647ab9973c36439290efe3d990deda22","1c7323cdbce0453890853a25c74d76fd","9d1c9690070a43829a4af992f6cf3ef0","012c04c5a88045ff936476ab10281a7b","5156dd5956054e67b07762bc74808e4f","1b22ff7ccdd9446ea83127bf311e9be2","29a00b7411954f17a427081fc6bd031c","2a8081c61dbd4799a3bdc356a0eaa628","37bf44fd484b404f90b1b2ed018f15ef","0cea9626ff464b67a5da45d08ac6c359","eb3bde2a638d4dbfb6c617dd9cf03a06","b2a6a85bbbff498ba5155609fac9f5af","49b08c28396a41948a0bb34230827071","321bad42444f4ff69b237225d03c0fb0","91497854ecc7414fae1f5ad8965dd415","1991157bc6d64185946313288b6aae03","9627a8c90ad045e9ae6665b25981f15e","3d685520be0e45efb940a0bceb0f2e0d","3f3bafee9e2b49ec8db578739d744917","2f2b00df348c4443a38159b554c4da34","d07ba3e3ac5a460ba74447cfee7411b2","a54f76afbcdb464caf49f9de39bbbe51","afe712f073a04ee0bee6c26d82932a3a","6c6432f06bd14e548bda87ac145dca56","87896aaa439040409e851f8e14c6465d","43ce4e82cca64d18a996f2e5b95b7281","2dbdcb3d788544a881d4b202bb9dd523","46d515aa470844beaea1c6cdc91374ce","f43c68f6b37041e89af6129466080977","f2e8ea4b505b41aeb2c56e58084b8868","d9e17d38545848af90b79f202006525d","90e20fd5ada84b2597137a40ff09d413","db2bec212a1f4301a3505b77e981788d","cfff565bb63e4154926d31b3be2ec9b6","7768885e97cd4b618358cd37ef9d3261","5ffc443c38ae41389cbf9fae71443aa8","b362eb62a87f4f6594e448dc2e21d5ec","c6d8352dc6354b4d9ee781b866ffca5f","23651dd9b6684d68988bedab0c94cdd3","436897133fbf46689fba6b5928c0bff5","dfa4f8ddfa4c4081a00df888bb0cb166","715147986d324ec3a95323640086e395"]},"id":"tLWm6k3nJ05L","outputId":"85dc6ca6-31fa-48fa-b5b4-6925284e38a4","executionInfo":{"status":"ok","timestamp":1739206755627,"user_tz":0,"elapsed":132508,"user":{"displayName":"george theof","userId":"15049073496200557337"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/17.7M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8fa7f69d42644eb4b01a3286aa4a48fa"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["\u001b[32mINFO    \u001b[0m \u001b[34mTokenizing dataset imdb with AutoTokenizer for bert-base-uncased.\u001b[0m\n"]},{"output_type":"display_data","data":{"text/plain":["README.md:   0%|          | 0.00/7.81k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"daf5e89ca30041e3a34c655982ac2ba5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["train-00000-of-00001.parquet:   0%|          | 0.00/21.0M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0606ff59d8e54d13bdbbf666a9c5d406"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["test-00000-of-00001.parquet:   0%|          | 0.00/20.5M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"001c2f555cda4cdfb3405a5a80591962"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["unsupervised-00000-of-00001.parquet:   0%|          | 0.00/42.0M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eb2310b0751e435a9f00b150363abe2f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"833e8d089ed44a6c881bacab8ffce4f1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"66ab779a352e4b58994f67f048980cfe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5a9e25e127a749fd958bec97da0ad93b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"221c08d4b7d54506ac8b849a95667f81"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"613a3db399af439eb1f3499dd5d87c15"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4725d84ef9eb4e749f8b896224eb3115"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"173076887af346e0a10810fc54918346"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0cea9626ff464b67a5da45d08ac6c359"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d07ba3e3ac5a460ba74447cfee7411b2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"90e20fd5ada84b2597137a40ff09d413"}},"metadata":{}}],"source":["from chop.tools import get_tokenized_dataset\n","\n","dataset, tokenizer = get_tokenized_dataset(\n","    dataset=dataset_name,\n","    checkpoint=tokenizer_checkpoint,\n","    return_tokenizer=True,\n",")"]},{"cell_type":"markdown","metadata":{"id":"sH4azGdQJ05M"},"source":["## 1. Defining the Search Space"]},{"cell_type":"markdown","metadata":{"id":"Gp7VMqmCJ05M"},"source":["We'll start by defining a search space, i.e. enumerating the possible combinations of hyperparameters that Optuna can choose during search. We'll explore the following range of values for the model's hidden size, intermediate size, number of layers and number of heads."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tvj0jLnEJ05M"},"outputs":[],"source":["\"\"\"\n","Instead of applying a single precision format to all layers, mixed-precision allows different layers to use different numeric representations.\n","This improves the trade-off between accuracy and efficiency.\n","\"\"\"\n","\n","import torch\n","from chop.nn.quantized.modules.linear import (\n","    LinearInteger,\n","    LinearMinifloatDenorm,\n","    LinearMinifloatIEEE,\n","    LinearLog,\n","    LinearBlockFP,\n","    LinearBlockMinifloat,\n","    LinearBlockLog,\n","    LinearBinary,\n","    LinearBinaryScaling,\n","    LinearBinaryResidualSign,\n",")\n","\n","search_space = {\n","    \"linear_layer_choices\": [\n","        torch.nn.Linear, # Standard 32-bit floating point linear layer.\n","        LinearInteger, # Integer-based fixed-point quantization.\n","    ],\n","    \"widths\": [8, 16, 32],\n","    \"frac_widths\": [2, 4, 8]\n","\n","}"]},{"cell_type":"markdown","metadata":{"id":"e0mfjhriJ05N"},"source":["## 2. Writing a Model Constructor"]},{"cell_type":"markdown","metadata":{"id":"xmtnWRe9J05N"},"source":["We define the following function, which will get called in each iteration of the search process. The function is passed the `trial` argument, which is an Optuna object that comes with many functionalities - see the [Trial documentation](https://optuna.readthedocs.io/en/stable/reference/trial.html) for more details. Here, we use the `trial.suggest_categorical` function, which triggers the chosen sampler to choose a layer type. The suggested integer is the index into the search space for each parameter, which we defined in the previous cell."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8hm2GeRxJ05N"},"outputs":[],"source":["from chop.tools.utils import deepsetattr\n","from copy import deepcopy\n","from transformers import AutoConfig, AutoModelForSequenceClassification\n","from chop.tools.utils import deepsetattr\n","\n","\n","\n","def construct_model(trial):\n","\n","    # Fetch the model\n","    # trial_model = deepcopy(model)\n","    config = AutoConfig.from_pretrained(checkpoint)\n","    trial_model = AutoModelForSequenceClassification.from_config(config)\n","\n","\n","    for param in [\"widths\", \"frac_widths\",]:\n","      chosen_idx = trial.suggest_int(param, 0, len(search_space[param]) - 1)\n","\n","    # Quantize layers according to optuna suggestions\n","    for name, layer in trial_model.named_modules():\n","        if isinstance(layer, torch.nn.Linear):\n","            new_layer_cls = trial.suggest_categorical(\n","                f\"{name}_type\",\n","                search_space[\"linear_layer_choices\"],\n","            )\n","\n","            if new_layer_cls == torch.nn.Linear:\n","                continue\n","\n","            kwargs = {\n","                \"in_features\": layer.in_features,\n","                \"out_features\": layer.out_features,\n","            }\n","\n","            # If the chosen layer is integer, define the low precision config\n","            if new_layer_cls == LinearInteger:\n","                kwargs[\"config\"] = {\n","                    \"data_in_width\": search_space[\"widths\"][chosen_idx],\n","                    \"data_in_frac_width\": search_space[\"frac_widths\"][chosen_idx],\n","                    \"weight_width\": search_space[\"widths\"][chosen_idx],\n","                    \"weight_frac_width\": search_space[\"frac_widths\"][chosen_idx],\n","                    \"bias_width\": search_space[\"widths\"][chosen_idx],\n","                    \"bias_frac_width\": search_space[\"frac_widths\"][chosen_idx],\n","                }\n","            # elif... (other precisions)\n","\n","            # Create the new layer (copy the weights)\n","            new_layer = new_layer_cls(**kwargs)\n","            new_layer.weight.data = layer.weight.data # ensures that the model structure remains the same, except for numeric precision.\n","\n","\n","            # Replace the layer in the model\n","            deepsetattr(trial_model, name, new_layer)\n","\n","    return trial_model"]},{"cell_type":"markdown","metadata":{"id":"QbeZKcY_J05O"},"source":["## 3. Defining the Objective Function"]},{"cell_type":"markdown","metadata":{"id":"CNyIjbo3J05O"},"source":["Next, we define the objective function for the search, which gets called on each trial. In each trial, we create a new model instace with chosen hyperparameters according to the defined sampler. We then use the `get_trainer` utility in Mase to run a training loop on the IMDb dataset for a number of epochs. Finally, we use `evaluate` to report back the classification accuracy on the test split."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1kCJpCCRJ05O"},"outputs":[],"source":["from chop.tools import get_trainer\n","import random\n","\n","\n","def objective(trial):\n","\n","    # Define the model\n","    model = construct_model(trial)\n","\n","    trainer = get_trainer(\n","        model=model,\n","        tokenized_dataset=dataset,\n","        tokenizer=tokenizer,\n","        evaluate_metric=\"accuracy\",\n","        num_train_epochs=1,\n","    )\n","\n","    trainer.train()\n","    eval_results = trainer.evaluate()\n","\n","    trial.set_user_attr(\"model\", model)\n","\n","    return eval_results[\"eval_accuracy\"]"]},{"cell_type":"markdown","metadata":{"id":"t-01YtIkJ05O"},"source":["## 4. Launching the Search"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B-kyUS6-J05P"},"outputs":[],"source":["from optuna.samplers import GridSampler, RandomSampler, TPESampler\n","\n","sampler = TPESampler()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":2340498,"status":"ok","timestamp":1738422052956,"user":{"displayName":"george theof","userId":"15049073496200557337"},"user_tz":0},"id":"LpoDb6p4J05P","outputId":"7c6effff-a936-4338-a8ca-d6181a5bdbc6"},"outputs":[{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["[I 2025-02-01 14:05:21,761] A new study created in memory with name: bert-tiny-nas-study\n","/usr/local/lib/python3.11/dist-packages/optuna/distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.nn.modules.linear.Linear'> which is of type type.\n","  warnings.warn(message)\n","/usr/local/lib/python3.11/dist-packages/optuna/distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'chop.nn.quantized.modules.linear.LinearInteger'> which is of type type.\n","  warnings.warn(message)\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3125/3125 01:32, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.693700</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.642300</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>0.497100</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>0.431000</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>0.397300</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>0.399600</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3125/3125 00:58]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["[I 2025-02-01 14:07:54,181] Trial 0 finished with value: 0.8394 and parameters: {'widths': 0, 'frac_widths': 1, 'bert.encoder.layer.0.attention.self.query_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.attention.self.key_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.attention.self.value_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.attention.output.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.intermediate.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.self.query_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.attention.self.key_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.self.value_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.intermediate.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.output.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.pooler.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'classifier_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>}. Best is trial 0 with value: 0.8394.\n","/usr/local/lib/python3.11/dist-packages/optuna/distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.nn.modules.linear.Linear'> which is of type type.\n","  warnings.warn(message)\n","/usr/local/lib/python3.11/dist-packages/optuna/distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'chop.nn.quantized.modules.linear.LinearInteger'> which is of type type.\n","  warnings.warn(message)\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3125/3125 01:16, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.693100</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.693100</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>0.693100</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>0.693100</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>0.693100</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>0.693100</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3125/3125 00:36]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["[I 2025-02-01 14:09:48,181] Trial 1 finished with value: 0.5 and parameters: {'widths': 1, 'frac_widths': 0, 'bert.encoder.layer.0.attention.self.query_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.attention.self.key_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.attention.self.value_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.attention.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.intermediate.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.output.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.attention.self.query_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.attention.self.key_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.attention.self.value_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.intermediate.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.output.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.pooler.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'classifier_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>}. Best is trial 0 with value: 0.8394.\n","/usr/local/lib/python3.11/dist-packages/optuna/distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.nn.modules.linear.Linear'> which is of type type.\n","  warnings.warn(message)\n","/usr/local/lib/python3.11/dist-packages/optuna/distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'chop.nn.quantized.modules.linear.LinearInteger'> which is of type type.\n","  warnings.warn(message)\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3125/3125 01:15, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.693400</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.634500</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>0.487800</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>0.426800</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>0.393400</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>0.397800</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3125/3125 00:36]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["[I 2025-02-01 14:11:40,947] Trial 2 finished with value: 0.83952 and parameters: {'widths': 2, 'frac_widths': 1, 'bert.encoder.layer.0.attention.self.query_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.attention.self.key_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.attention.self.value_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.attention.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.intermediate.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.self.query_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.attention.self.key_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.attention.self.value_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.intermediate.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.output.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.pooler.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'classifier_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>}. Best is trial 2 with value: 0.83952.\n","/usr/local/lib/python3.11/dist-packages/optuna/distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.nn.modules.linear.Linear'> which is of type type.\n","  warnings.warn(message)\n","/usr/local/lib/python3.11/dist-packages/optuna/distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'chop.nn.quantized.modules.linear.LinearInteger'> which is of type type.\n","  warnings.warn(message)\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3125/3125 01:12, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.693700</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.617900</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>0.477800</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>0.407500</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>0.371400</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>0.376900</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3125/3125 00:35]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["[I 2025-02-01 14:13:30,313] Trial 3 finished with value: 0.84288 and parameters: {'widths': 1, 'frac_widths': 2, 'bert.encoder.layer.0.attention.self.query_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.attention.self.key_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.attention.self.value_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.attention.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.intermediate.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.output.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.attention.self.query_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.attention.self.key_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.self.value_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.attention.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.intermediate.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.pooler.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'classifier_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>}. Best is trial 3 with value: 0.84288.\n","/usr/local/lib/python3.11/dist-packages/optuna/distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.nn.modules.linear.Linear'> which is of type type.\n","  warnings.warn(message)\n","/usr/local/lib/python3.11/dist-packages/optuna/distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'chop.nn.quantized.modules.linear.LinearInteger'> which is of type type.\n","  warnings.warn(message)\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3125/3125 01:16, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.693100</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.693100</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>0.693100</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>0.693100</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>0.693100</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>0.693100</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3125/3125 00:37]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["[I 2025-02-01 14:15:25,703] Trial 4 finished with value: 0.5 and parameters: {'widths': 0, 'frac_widths': 0, 'bert.encoder.layer.0.attention.self.query_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.attention.self.key_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.attention.self.value_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.attention.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.intermediate.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.self.query_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.self.key_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.attention.self.value_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.attention.output.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.intermediate.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.output.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.pooler.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'classifier_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>}. Best is trial 3 with value: 0.84288.\n","/usr/local/lib/python3.11/dist-packages/optuna/distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.nn.modules.linear.Linear'> which is of type type.\n","  warnings.warn(message)\n","/usr/local/lib/python3.11/dist-packages/optuna/distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'chop.nn.quantized.modules.linear.LinearInteger'> which is of type type.\n","  warnings.warn(message)\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3125/3125 01:10, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.693800</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.619700</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>0.473300</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>0.406600</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>0.371900</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>0.376300</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3125/3125 00:34]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["[I 2025-02-01 14:17:11,440] Trial 5 finished with value: 0.8424 and parameters: {'widths': 0, 'frac_widths': 2, 'bert.encoder.layer.0.attention.self.query_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.attention.self.key_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.attention.self.value_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.attention.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.intermediate.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.output.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.attention.self.query_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.self.key_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.self.value_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.intermediate.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.pooler.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'classifier_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>}. Best is trial 3 with value: 0.84288.\n","/usr/local/lib/python3.11/dist-packages/optuna/distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.nn.modules.linear.Linear'> which is of type type.\n","  warnings.warn(message)\n","/usr/local/lib/python3.11/dist-packages/optuna/distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'chop.nn.quantized.modules.linear.LinearInteger'> which is of type type.\n","  warnings.warn(message)\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3125/3125 01:13, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.693100</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.693100</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>0.693100</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>0.693100</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>0.693100</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>0.693100</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3125/3125 00:37]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["[I 2025-02-01 14:19:04,273] Trial 6 finished with value: 0.5 and parameters: {'widths': 2, 'frac_widths': 0, 'bert.encoder.layer.0.attention.self.query_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.attention.self.key_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.attention.self.value_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.attention.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.intermediate.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.self.query_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.self.key_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.self.value_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.attention.output.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.intermediate.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.pooler.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'classifier_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>}. Best is trial 3 with value: 0.84288.\n","/usr/local/lib/python3.11/dist-packages/optuna/distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.nn.modules.linear.Linear'> which is of type type.\n","  warnings.warn(message)\n","/usr/local/lib/python3.11/dist-packages/optuna/distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'chop.nn.quantized.modules.linear.LinearInteger'> which is of type type.\n","  warnings.warn(message)\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3125/3125 01:10, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.693800</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.643500</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>0.492600</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>0.433400</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>0.400500</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>0.394000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3125/3125 00:33]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["[I 2025-02-01 14:20:49,134] Trial 7 finished with value: 0.83616 and parameters: {'widths': 1, 'frac_widths': 1, 'bert.encoder.layer.0.attention.self.query_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.attention.self.key_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.attention.self.value_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.attention.output.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.intermediate.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.self.query_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.attention.self.key_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.self.value_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.output.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.intermediate.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.pooler.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'classifier_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>}. Best is trial 3 with value: 0.84288.\n","/usr/local/lib/python3.11/dist-packages/optuna/distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.nn.modules.linear.Linear'> which is of type type.\n","  warnings.warn(message)\n","/usr/local/lib/python3.11/dist-packages/optuna/distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'chop.nn.quantized.modules.linear.LinearInteger'> which is of type type.\n","  warnings.warn(message)\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='2510' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [2510/3125 01:02 < 00:15, 40.42 it/s, Epoch 0.80/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.693100</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.693100</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>0.693100</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>0.693100</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>0.693100</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3125/3125 01:17, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.693100</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.693100</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>0.693100</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>0.693100</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>0.693100</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>0.693100</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3125/3125 00:38]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["[I 2025-02-01 14:22:45,636] Trial 8 finished with value: 0.5 and parameters: {'widths': 1, 'frac_widths': 0, 'bert.encoder.layer.0.attention.self.query_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.attention.self.key_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.attention.self.value_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.attention.output.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.intermediate.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.self.query_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.attention.self.key_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.attention.self.value_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.output.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.intermediate.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.pooler.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'classifier_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>}. Best is trial 3 with value: 0.84288.\n","/usr/local/lib/python3.11/dist-packages/optuna/distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.nn.modules.linear.Linear'> which is of type type.\n","  warnings.warn(message)\n","/usr/local/lib/python3.11/dist-packages/optuna/distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'chop.nn.quantized.modules.linear.LinearInteger'> which is of type type.\n","  warnings.warn(message)\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3125/3125 01:14, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.693800</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.694100</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>0.693700</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>0.693800</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>0.693200</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>0.693400</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3125/3125 00:36]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["[I 2025-02-01 14:24:37,937] Trial 9 finished with value: 0.5 and parameters: {'widths': 2, 'frac_widths': 0, 'bert.encoder.layer.0.attention.self.query_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.attention.self.key_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.attention.self.value_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.attention.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.intermediate.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.output.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.attention.self.query_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.attention.self.key_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.self.value_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.attention.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.intermediate.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.output.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.pooler.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'classifier_type': <class 'torch.nn.modules.linear.Linear'>}. Best is trial 3 with value: 0.84288.\n","/usr/local/lib/python3.11/dist-packages/optuna/distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.nn.modules.linear.Linear'> which is of type type.\n","  warnings.warn(message)\n","/usr/local/lib/python3.11/dist-packages/optuna/distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'chop.nn.quantized.modules.linear.LinearInteger'> which is of type type.\n","  warnings.warn(message)\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3125/3125 01:10, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.693800</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.619400</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>0.469200</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>0.405900</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>0.369000</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>0.375600</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3125/3125 00:33]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["[I 2025-02-01 14:26:23,368] Trial 10 finished with value: 0.8432 and parameters: {'widths': 1, 'frac_widths': 2, 'bert.encoder.layer.0.attention.self.query_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.attention.self.key_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.attention.self.value_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.attention.output.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.intermediate.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.output.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.attention.self.query_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.self.key_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.self.value_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.attention.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.intermediate.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.pooler.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'classifier_type': <class 'torch.nn.modules.linear.Linear'>}. Best is trial 10 with value: 0.8432.\n","/usr/local/lib/python3.11/dist-packages/optuna/distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.nn.modules.linear.Linear'> which is of type type.\n","  warnings.warn(message)\n","/usr/local/lib/python3.11/dist-packages/optuna/distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'chop.nn.quantized.modules.linear.LinearInteger'> which is of type type.\n","  warnings.warn(message)\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3125/3125 01:12, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.693800</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.619400</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>0.469200</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>0.405900</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>0.369000</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>0.375600</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3125/3125 00:36]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["[I 2025-02-01 14:28:13,852] Trial 11 finished with value: 0.8432 and parameters: {'widths': 1, 'frac_widths': 2, 'bert.encoder.layer.0.attention.self.query_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.attention.self.key_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.attention.self.value_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.attention.output.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.intermediate.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.output.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.attention.self.query_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.self.key_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.self.value_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.attention.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.intermediate.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.pooler.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'classifier_type': <class 'torch.nn.modules.linear.Linear'>}. Best is trial 10 with value: 0.8432.\n","/usr/local/lib/python3.11/dist-packages/optuna/distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.nn.modules.linear.Linear'> which is of type type.\n","  warnings.warn(message)\n","/usr/local/lib/python3.11/dist-packages/optuna/distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'chop.nn.quantized.modules.linear.LinearInteger'> which is of type type.\n","  warnings.warn(message)\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3125/3125 01:12, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.693800</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.619400</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>0.469200</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>0.405900</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>0.369000</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>0.375600</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3125/3125 00:35]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["[I 2025-02-01 14:30:02,985] Trial 12 finished with value: 0.84316 and parameters: {'widths': 1, 'frac_widths': 2, 'bert.encoder.layer.0.attention.self.query_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.attention.self.key_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.attention.self.value_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.attention.output.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.intermediate.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.output.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.attention.self.query_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.self.key_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.self.value_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.attention.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.intermediate.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.pooler.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'classifier_type': <class 'torch.nn.modules.linear.Linear'>}. Best is trial 10 with value: 0.8432.\n","/usr/local/lib/python3.11/dist-packages/optuna/distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.nn.modules.linear.Linear'> which is of type type.\n","  warnings.warn(message)\n","/usr/local/lib/python3.11/dist-packages/optuna/distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'chop.nn.quantized.modules.linear.LinearInteger'> which is of type type.\n","  warnings.warn(message)\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3125/3125 01:12, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.693800</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.619400</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>0.469200</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>0.405900</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>0.369000</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>0.375700</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3125/3125 00:34]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["[I 2025-02-01 14:31:51,435] Trial 13 finished with value: 0.8432 and parameters: {'widths': 1, 'frac_widths': 2, 'bert.encoder.layer.0.attention.self.query_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.attention.self.key_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.attention.self.value_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.attention.output.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.intermediate.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.output.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.attention.self.query_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.self.key_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.self.value_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.attention.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.intermediate.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.pooler.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'classifier_type': <class 'torch.nn.modules.linear.Linear'>}. Best is trial 10 with value: 0.8432.\n","/usr/local/lib/python3.11/dist-packages/optuna/distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.nn.modules.linear.Linear'> which is of type type.\n","  warnings.warn(message)\n","/usr/local/lib/python3.11/dist-packages/optuna/distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'chop.nn.quantized.modules.linear.LinearInteger'> which is of type type.\n","  warnings.warn(message)\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3125/3125 01:13, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.693800</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.619400</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>0.469200</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>0.405900</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>0.369000</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>0.375700</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3125/3125 00:34]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["[I 2025-02-01 14:33:41,104] Trial 14 finished with value: 0.84324 and parameters: {'widths': 2, 'frac_widths': 2, 'bert.encoder.layer.0.attention.self.query_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.attention.self.key_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.attention.self.value_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.attention.output.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.intermediate.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.output.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.attention.self.query_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.self.key_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.self.value_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.attention.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.intermediate.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.pooler.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'classifier_type': <class 'torch.nn.modules.linear.Linear'>}. Best is trial 14 with value: 0.84324.\n","/usr/local/lib/python3.11/dist-packages/optuna/distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.nn.modules.linear.Linear'> which is of type type.\n","  warnings.warn(message)\n","/usr/local/lib/python3.11/dist-packages/optuna/distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'chop.nn.quantized.modules.linear.LinearInteger'> which is of type type.\n","  warnings.warn(message)\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3125/3125 01:13, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.693800</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.619400</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>0.469200</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>0.405900</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>0.369000</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>0.375700</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3125/3125 00:34]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["[I 2025-02-01 14:35:30,095] Trial 15 finished with value: 0.84312 and parameters: {'widths': 2, 'frac_widths': 2, 'bert.encoder.layer.0.attention.self.query_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.attention.self.key_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.attention.self.value_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.attention.output.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.intermediate.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.output.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.attention.self.query_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.self.key_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.self.value_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.attention.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.intermediate.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.pooler.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'classifier_type': <class 'torch.nn.modules.linear.Linear'>}. Best is trial 14 with value: 0.84324.\n","/usr/local/lib/python3.11/dist-packages/optuna/distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.nn.modules.linear.Linear'> which is of type type.\n","  warnings.warn(message)\n","/usr/local/lib/python3.11/dist-packages/optuna/distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'chop.nn.quantized.modules.linear.LinearInteger'> which is of type type.\n","  warnings.warn(message)\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3125/3125 01:10, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.693800</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.646300</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>0.466100</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>0.411100</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>0.380800</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>0.378900</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3125/3125 00:35]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["[I 2025-02-01 14:37:17,423] Trial 16 finished with value: 0.84084 and parameters: {'widths': 2, 'frac_widths': 1, 'bert.encoder.layer.0.attention.self.query_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.attention.self.key_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.attention.self.value_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.attention.output.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.intermediate.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.output.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.attention.self.query_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.self.key_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.self.value_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.attention.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.intermediate.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.pooler.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'classifier_type': <class 'torch.nn.modules.linear.Linear'>}. Best is trial 14 with value: 0.84324.\n","/usr/local/lib/python3.11/dist-packages/optuna/distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.nn.modules.linear.Linear'> which is of type type.\n","  warnings.warn(message)\n","/usr/local/lib/python3.11/dist-packages/optuna/distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'chop.nn.quantized.modules.linear.LinearInteger'> which is of type type.\n","  warnings.warn(message)\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3125/3125 01:11, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.693800</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.619400</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>0.469200</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>0.405900</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>0.369000</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>0.375600</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3125/3125 00:33]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["[I 2025-02-01 14:39:03,314] Trial 17 finished with value: 0.84316 and parameters: {'widths': 2, 'frac_widths': 2, 'bert.encoder.layer.0.attention.self.query_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.attention.self.key_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.attention.self.value_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.attention.output.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.intermediate.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.output.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.attention.self.query_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.self.key_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.self.value_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.attention.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.intermediate.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.pooler.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'classifier_type': <class 'torch.nn.modules.linear.Linear'>}. Best is trial 14 with value: 0.84324.\n","/usr/local/lib/python3.11/dist-packages/optuna/distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.nn.modules.linear.Linear'> which is of type type.\n","  warnings.warn(message)\n","/usr/local/lib/python3.11/dist-packages/optuna/distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'chop.nn.quantized.modules.linear.LinearInteger'> which is of type type.\n","  warnings.warn(message)\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3125/3125 01:14, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.693800</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.654300</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>0.477000</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>0.416100</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>0.384700</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>0.384600</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3125/3125 00:35]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["[I 2025-02-01 14:40:54,181] Trial 18 finished with value: 0.83816 and parameters: {'widths': 0, 'frac_widths': 1, 'bert.encoder.layer.0.attention.self.query_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.attention.self.key_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.attention.self.value_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.attention.output.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.intermediate.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.output.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.attention.self.query_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.self.key_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.attention.self.value_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.attention.output.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.intermediate.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.pooler.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'classifier_type': <class 'torch.nn.modules.linear.Linear'>}. Best is trial 14 with value: 0.84324.\n","/usr/local/lib/python3.11/dist-packages/optuna/distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.nn.modules.linear.Linear'> which is of type type.\n","  warnings.warn(message)\n","/usr/local/lib/python3.11/dist-packages/optuna/distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'chop.nn.quantized.modules.linear.LinearInteger'> which is of type type.\n","  warnings.warn(message)\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3125/3125 01:11, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.693800</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.619400</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>0.469200</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>0.405900</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>0.369000</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>0.375600</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3125/3125 00:35]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["[I 2025-02-01 14:42:42,152] Trial 19 finished with value: 0.8432 and parameters: {'widths': 1, 'frac_widths': 2, 'bert.encoder.layer.0.attention.self.query_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.attention.self.key_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.attention.self.value_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.attention.output.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.intermediate.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.output.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.attention.self.query_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.self.key_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.self.value_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.attention.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.intermediate.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.pooler.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'classifier_type': <class 'torch.nn.modules.linear.Linear'>}. Best is trial 14 with value: 0.84324.\n","/usr/local/lib/python3.11/dist-packages/optuna/distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.nn.modules.linear.Linear'> which is of type type.\n","  warnings.warn(message)\n","/usr/local/lib/python3.11/dist-packages/optuna/distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'chop.nn.quantized.modules.linear.LinearInteger'> which is of type type.\n","  warnings.warn(message)\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3125/3125 01:11, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.693800</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.619400</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>0.469200</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>0.405900</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>0.369000</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>0.375600</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3125/3125 00:34]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["[I 2025-02-01 14:44:29,097] Trial 20 finished with value: 0.8432 and parameters: {'widths': 2, 'frac_widths': 2, 'bert.encoder.layer.0.attention.self.query_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.attention.self.key_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.attention.self.value_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.attention.output.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.intermediate.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.output.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.attention.self.query_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.self.key_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.self.value_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.attention.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.intermediate.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.pooler.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'classifier_type': <class 'torch.nn.modules.linear.Linear'>}. Best is trial 14 with value: 0.84324.\n","/usr/local/lib/python3.11/dist-packages/optuna/distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.nn.modules.linear.Linear'> which is of type type.\n","  warnings.warn(message)\n","/usr/local/lib/python3.11/dist-packages/optuna/distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'chop.nn.quantized.modules.linear.LinearInteger'> which is of type type.\n","  warnings.warn(message)\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3125/3125 01:11, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.693800</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.619400</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>0.469200</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>0.405900</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>0.369000</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>0.375700</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3125/3125 00:33]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["[I 2025-02-01 14:46:15,312] Trial 21 finished with value: 0.8432 and parameters: {'widths': 1, 'frac_widths': 2, 'bert.encoder.layer.0.attention.self.query_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.attention.self.key_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.attention.self.value_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.attention.output.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.intermediate.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.output.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.attention.self.query_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.self.key_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.self.value_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.attention.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.intermediate.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.pooler.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'classifier_type': <class 'torch.nn.modules.linear.Linear'>}. Best is trial 14 with value: 0.84324.\n","/usr/local/lib/python3.11/dist-packages/optuna/distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.nn.modules.linear.Linear'> which is of type type.\n","  warnings.warn(message)\n","/usr/local/lib/python3.11/dist-packages/optuna/distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'chop.nn.quantized.modules.linear.LinearInteger'> which is of type type.\n","  warnings.warn(message)\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3125/3125 01:11, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.693800</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.619400</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>0.469200</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>0.405900</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>0.369000</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>0.375600</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3125/3125 00:35]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["[I 2025-02-01 14:48:03,555] Trial 22 finished with value: 0.84324 and parameters: {'widths': 1, 'frac_widths': 2, 'bert.encoder.layer.0.attention.self.query_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.attention.self.key_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.attention.self.value_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.attention.output.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.intermediate.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.output.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.attention.self.query_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.self.key_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.self.value_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.attention.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.intermediate.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.pooler.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'classifier_type': <class 'torch.nn.modules.linear.Linear'>}. Best is trial 14 with value: 0.84324.\n","/usr/local/lib/python3.11/dist-packages/optuna/distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.nn.modules.linear.Linear'> which is of type type.\n","  warnings.warn(message)\n","/usr/local/lib/python3.11/dist-packages/optuna/distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'chop.nn.quantized.modules.linear.LinearInteger'> which is of type type.\n","  warnings.warn(message)\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3125/3125 01:10, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.693800</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.619400</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>0.469200</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>0.405900</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>0.369000</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>0.375700</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3125/3125 00:33]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["[I 2025-02-01 14:49:49,583] Trial 23 finished with value: 0.84312 and parameters: {'widths': 1, 'frac_widths': 2, 'bert.encoder.layer.0.attention.self.query_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.attention.self.key_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.attention.self.value_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.attention.output.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.intermediate.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.output.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.attention.self.query_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.self.key_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.self.value_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.attention.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.intermediate.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.pooler.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'classifier_type': <class 'torch.nn.modules.linear.Linear'>}. Best is trial 14 with value: 0.84324.\n","/usr/local/lib/python3.11/dist-packages/optuna/distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.nn.modules.linear.Linear'> which is of type type.\n","  warnings.warn(message)\n","/usr/local/lib/python3.11/dist-packages/optuna/distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'chop.nn.quantized.modules.linear.LinearInteger'> which is of type type.\n","  warnings.warn(message)\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3125/3125 01:11, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.693800</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.646300</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>0.466100</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>0.411100</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>0.380900</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>0.378900</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3125/3125 00:35]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["[I 2025-02-01 14:51:37,835] Trial 24 finished with value: 0.84088 and parameters: {'widths': 1, 'frac_widths': 1, 'bert.encoder.layer.0.attention.self.query_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.attention.self.key_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.attention.self.value_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.attention.output.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.intermediate.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.output.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.attention.self.query_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.self.key_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.self.value_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.attention.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.intermediate.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.pooler.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'classifier_type': <class 'torch.nn.modules.linear.Linear'>}. Best is trial 14 with value: 0.84324.\n","/usr/local/lib/python3.11/dist-packages/optuna/distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.nn.modules.linear.Linear'> which is of type type.\n","  warnings.warn(message)\n","/usr/local/lib/python3.11/dist-packages/optuna/distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'chop.nn.quantized.modules.linear.LinearInteger'> which is of type type.\n","  warnings.warn(message)\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3125/3125 01:11, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.693800</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.619400</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>0.469200</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>0.405900</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>0.369000</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>0.375700</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3125/3125 00:35]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["[I 2025-02-01 14:53:26,278] Trial 25 finished with value: 0.8432 and parameters: {'widths': 0, 'frac_widths': 2, 'bert.encoder.layer.0.attention.self.query_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.attention.self.key_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.attention.self.value_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.attention.output.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.intermediate.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.output.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.attention.self.query_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.self.key_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.self.value_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.attention.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.intermediate.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.pooler.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'classifier_type': <class 'torch.nn.modules.linear.Linear'>}. Best is trial 14 with value: 0.84324.\n","/usr/local/lib/python3.11/dist-packages/optuna/distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.nn.modules.linear.Linear'> which is of type type.\n","  warnings.warn(message)\n","/usr/local/lib/python3.11/dist-packages/optuna/distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'chop.nn.quantized.modules.linear.LinearInteger'> which is of type type.\n","  warnings.warn(message)\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3125/3125 01:11, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.693800</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.646300</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>0.465900</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>0.410100</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>0.382600</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>0.379200</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3125/3125 00:35]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["[I 2025-02-01 14:55:14,633] Trial 26 finished with value: 0.84088 and parameters: {'widths': 2, 'frac_widths': 1, 'bert.encoder.layer.0.attention.self.query_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.attention.self.key_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.attention.self.value_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.attention.output.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.intermediate.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.output.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.attention.self.query_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.self.key_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.self.value_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.attention.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.intermediate.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.pooler.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'classifier_type': <class 'torch.nn.modules.linear.Linear'>}. Best is trial 14 with value: 0.84324.\n","/usr/local/lib/python3.11/dist-packages/optuna/distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.nn.modules.linear.Linear'> which is of type type.\n","  warnings.warn(message)\n","/usr/local/lib/python3.11/dist-packages/optuna/distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'chop.nn.quantized.modules.linear.LinearInteger'> which is of type type.\n","  warnings.warn(message)\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3125/3125 01:16, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.693900</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.620500</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>0.471600</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>0.408800</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>0.372600</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>0.375800</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3125/3125 00:39]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["[I 2025-02-01 14:57:12,138] Trial 27 finished with value: 0.84256 and parameters: {'widths': 1, 'frac_widths': 2, 'bert.encoder.layer.0.attention.self.query_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.attention.self.key_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.attention.self.value_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.attention.output.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.intermediate.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.output.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.attention.self.query_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.self.key_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.attention.self.value_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.attention.output.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.intermediate.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.output.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.pooler.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'classifier_type': <class 'torch.nn.modules.linear.Linear'>}. Best is trial 14 with value: 0.84324.\n","/usr/local/lib/python3.11/dist-packages/optuna/distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.nn.modules.linear.Linear'> which is of type type.\n","  warnings.warn(message)\n","/usr/local/lib/python3.11/dist-packages/optuna/distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'chop.nn.quantized.modules.linear.LinearInteger'> which is of type type.\n","  warnings.warn(message)\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3125/3125 01:11, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.693800</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.619400</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>0.469200</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>0.405900</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>0.369000</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>0.375600</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3125/3125 00:33]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["[I 2025-02-01 14:58:58,989] Trial 28 finished with value: 0.8432 and parameters: {'widths': 1, 'frac_widths': 2, 'bert.encoder.layer.0.attention.self.query_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.attention.self.key_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.attention.self.value_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.attention.output.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.intermediate.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.output.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.attention.self.query_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.self.key_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.self.value_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.attention.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.intermediate.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.pooler.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'classifier_type': <class 'torch.nn.modules.linear.Linear'>}. Best is trial 14 with value: 0.84324.\n","/usr/local/lib/python3.11/dist-packages/optuna/distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.nn.modules.linear.Linear'> which is of type type.\n","  warnings.warn(message)\n","/usr/local/lib/python3.11/dist-packages/optuna/distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'chop.nn.quantized.modules.linear.LinearInteger'> which is of type type.\n","  warnings.warn(message)\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3125/3125 01:12, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.693600</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.639100</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>0.499100</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>0.425000</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>0.387200</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>0.392800</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3125/3125 00:35]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["[I 2025-02-01 15:00:53,285] Trial 29 finished with value: 0.83992 and parameters: {'widths': 0, 'frac_widths': 1, 'bert.encoder.layer.0.attention.self.query_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.attention.self.key_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.attention.self.value_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.attention.output.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.intermediate.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.self.query_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.self.key_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.self.value_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.intermediate.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.output.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.pooler.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'classifier_type': <class 'torch.nn.modules.linear.Linear'>}. Best is trial 14 with value: 0.84324.\n"]}],"source":["import optuna\n","\n","study = optuna.create_study(\n","    direction=\"maximize\",\n","    study_name=\"bert-tiny-nas-study\",\n","    sampler=sampler,\n",")\n","\n","study.optimize(\n","    objective,\n","    n_trials=30,\n","    timeout=60 * 60 * 24,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":222,"status":"ok","timestamp":1738422517079,"user":{"displayName":"george theof","userId":"15049073496200557337"},"user_tz":0},"id":"PQgxUogQrRd2","outputId":"f302fa6a-5351-48dd-9c78-5c50881b60a9"},"outputs":[{"output_type":"stream","name":"stdout","text":["[0.8394, 0.5, 0.83952, 0.84288, 0.5, 0.8424, 0.5, 0.83616, 0.5, 0.5, 0.8432, 0.8432, 0.84316, 0.8432, 0.84324, 0.84312, 0.84084, 0.84316, 0.83816, 0.8432, 0.8432, 0.8432, 0.84324, 0.84312, 0.84088, 0.8432, 0.84088, 0.84256, 0.8432, 0.83992]\n"]}],"source":["values = [t.value for t in study.trials if t.value is not None]\n","\n","print(values)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":718},"executionInfo":{"elapsed":712,"status":"ok","timestamp":1738422520517,"user":{"displayName":"george theof","userId":"15049073496200557337"},"user_tz":0},"id":"m5c59O1zo_06","outputId":"16e38d4c-0606-42da-f9ba-b401542271de"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 1200x800 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}],"source":["import matplotlib.pyplot as plt\n","\n","# Extract trial numbers and their corresponding values\n","trials = [t.number for t in study.trials if t.value is not None]\n","values = [t.value for t in study.trials if t.value is not None]\n","\n","# Compute cumulative max accuracy\n","cumulative_max = [max(values[:i+1]) for i in range(len(values))]\n","\n","# Plot\n","plt.figure(figsize=(12, 8))\n","plt.plot(trials, cumulative_max, marker='o', linestyle='-', color='b', label=\"Max Accuracy Achieved\")\n","\n","# Labels and title\n","plt.xlabel(\"Number of Trials\")\n","plt.ylabel(\"Maximum Achieved Accuracy\")\n","plt.title(\"Optuna Hyperparameter Tuning Progress\")\n","plt.grid(True)\n","plt.legend()\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xCxia4wyeqaU"},"outputs":[],"source":["from chop.tools import get_trainer\n","import random\n","\n","\n","def objective(trial):\n","\n","    # Define the model\n","    model = construct_model(trial)\n","\n","    trainer = get_trainer(\n","        model=model,\n","        tokenized_dataset=dataset,\n","        tokenizer=tokenizer,\n","        evaluate_metric=\"accuracy\",\n","        num_train_epochs=1,\n","    )\n","\n","    trainer.train()\n","    eval_results = trainer.evaluate()\n","\n","    trial.set_user_attr(\"model\", model)\n","\n","    return eval_results[\"eval_accuracy\"]"]},{"cell_type":"code","source":["from chop.tools.utils import deepsetattr\n","from transformers import AutoConfig, AutoModelForSequenceClassification\n","import torch\n","from chop.nn.quantized.modules.linear import (\n","    LinearInteger,\n","    LinearMinifloatDenorm,\n","    LinearMinifloatIEEE,\n","    LinearLog,\n","    LinearBlockFP,\n","    LinearBlockMinifloat,\n","    LinearBlockLog,\n","    LinearBinary,\n","    LinearBinaryScaling,\n","    LinearBinaryResidualSign,\n",")\n","\n","def construct_model(trial):\n","    # Fetch the model\n","    config = AutoConfig.from_pretrained(checkpoint)\n","    trial_model = AutoModelForSequenceClassification.from_config(config)\n","\n","    # Sample global parameters using your requested structure\n","    chosen_params = {}\n","\n","    # Quantize layers according to Optuna suggestions\n","    for name, layer in trial_model.named_modules():\n","        if isinstance(layer, torch.nn.Linear):\n","            # Select the layer type\n","            new_layer_cls_str = trial.suggest_categorical(\n","                f\"{name}_type\",\n","                search_space[\"linear_layer_choices\"],\n","            )\n","            new_layer_cls = layer_mapping[new_layer_cls_str]\n","\n","            # Skip if using standard torch.nn.Linear\n","            if new_layer_cls == torch.nn.Linear:\n","                continue\n","\n","            # Define default kwargs\n","            kwargs = {\n","                \"in_features\": layer.in_features,\n","                \"out_features\": layer.out_features,\n","            }\n","\n","            # Assign width & frac width\n","            for param in [\"widths\", \"frac_widths\"]:\n","                chosen_idx = trial.suggest_int(param, 0, len(search_space[param]) - 1)\n","\n","            kwargs[\"config\"] = {\n","                \"data_in_width\": search_space[\"widths\"][chosen_idx],\n","                \"data_in_frac_width\": search_space[\"frac_widths\"][chosen_idx],\n","                \"weight_width\": search_space[\"widths\"][chosen_idx],\n","                \"weight_frac_width\": search_space[\"frac_widths\"][chosen_idx],\n","                \"bias_width\": search_space[\"widths\"][chosen_idx],\n","                \"bias_frac_width\": search_space[\"frac_widths\"][chosen_idx],\n","            }\n","\n","            # Add stochastic and bipolar options for all except LinearInteger\n","            if new_layer_cls != LinearInteger:\n","                for param in [\"stochastic\", \"bipolar\"]:\n","                    chosen_idx = trial.suggest_int(param, 0, len(search_space[param]) - 1)\n","\n","                for param in [\"data_in_stochastic\", \"weight_stochastic\", \"bias_stochastic\"]:\n","                    kwargs[\"config\"][param] = search_space[\"stochastic\"][chosen_idx]\n","\n","                for param in [\"data_in_bipolar\", \"weight_bipolar\", \"bias_bipolar\"]:\n","                    kwargs[\"config\"][param] = search_space[\"bipolar\"][chosen_idx]\n","\n","                if new_layer_cls in [LinearBlockLog]:\n","                    for param in [\"data_in_exponent_bias_width\", \"bias_exponent_bias_width\", \"weight_exponent_bias_width\"]:\n","                        chosen_idx = trial.suggest_int(param, 0, len(search_space[\"exponent_widths\"]) - 1)\n","                        kwargs[\"config\"][param] = search_space[\"exponent_widths\"][chosen_idx]\n","\n","\n","                for param in [\"data_in_exponent_width\", \"weight_exponent_width\", \"bias_exponent_width\",]:\n","                  chosen_idx = trial.suggest_int(param, 0, len(search_space[\"exponent_widths\"]) - 1)\n","                  kwargs[\"config\"][param] = search_space[\"exponent_widths\"][chosen_idx]\n","\n","                for param in [\"data_in_exponent_bias\", \"weight_exponent_bias\", \"bias_exponent_bias\"]:\n","                    chosen_idx = trial.suggest_int(param, 0, len(search_space[\"exponent_bias\"]) - 1)\n","                    kwargs[\"config\"][param] = search_space[\"exponent_bias\"][chosen_idx]\n","\n","            # Block-based layers (FP, Log, Minifloat)\n","            if new_layer_cls in [LinearBlockFP, LinearBlockLog, LinearBlockMinifloat]:\n","                for param in [\"weight_block_size\", \"data_in_block_size\", \"bias_block_size\"]:\n","                    # Sample block sizes separately (only for block-based layers)\n","                    chosen_idx = trial.suggest_int(param, 0, len(search_space[\"block_sizes\"]) - 1)\n","                    kwargs[\"config\"][param] = search_space[\"block_sizes\"][chosen_idx]\n","\n","            # Binary Layers\n","            if new_layer_cls in [LinearBinaryScaling, LinearBinaryResidualSign]:\n","                  chosen_idx = trial.suggest_int(\"binary_training\", 0, len(search_space[\"binary\"]) - 1)\n","                  kwargs[\"config\"][\"binary_training\"] = search_space[\"binary\"][chosen_idx]\n","\n","            # Create the new layer (copy the weights)\n","            new_layer = new_layer_cls(**kwargs)\n","            new_layer.weight.data = layer.weight.data.clone()\n","\n","            # Replace the layer in the model\n","            deepsetattr(trial_model, name, new_layer)\n","\n","    return trial_model\n","\n"],"metadata":{"id":"3oKDksLsyPzC"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["8e1087806b4a40f3bf6eed3fa3b57445","bba89ab2dbc149429d5225960b9dcf9f","867b3cabf4034aa7bc0b2f693fbb130a","a6c6d31160e240628c1bd7fd19750833","2d949af6aaa04e6aa47391a46c1448af","1bcdf61d1c6f4e33965526ed9c22656a","ba3bbe67a1a5411cb883ba46ee103a40","612c42def73041cbbd9309b328e13e9b","bf82fc3729114b639a155e45c0b3f149","d08868fb9ce54f31be7f6a4bbaeacbf8","670cc9cb18364fb0ae92d35ba5640185"]},"id":"xu-KW_WbetZo","outputId":"7230c280-877b-4dd1-d865-e76c99c41e2e","executionInfo":{"status":"error","timestamp":1739185946927,"user_tz":0,"elapsed":113855,"user":{"displayName":"george theof","userId":"15049073496200557337"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["[I 2025-02-10 11:10:32,951] A new study created in memory with name: bert-tiny-nas-LinearBinary\n"]},{"output_type":"stream","name":"stdout","text":["\n","Running study for LinearBinary...\n","\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e1087806b4a40f3bf6eed3fa3b57445"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3125/3125 01:12, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.693700</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.694100</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>0.693500</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>0.694500</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>0.694400</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>0.694200</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3125/3125 00:35]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["[I 2025-02-10 11:12:24,819] Trial 0 finished with value: 0.49068 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.self.key_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.self.value_type': 'LinearBinary', 'widths': 0, 'frac_widths': 2, 'stochastic': 0, 'bipolar': 0, 'data_in_exponent_width': 2, 'weight_exponent_width': 0, 'bias_exponent_width': 2, 'data_in_exponent_bias': 2, 'weight_exponent_bias': 1, 'bias_exponent_bias': 1, 'bert.encoder.layer.0.attention.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.intermediate.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.query_type': 'LinearBinary', 'bert.encoder.layer.1.attention.self.key_type': 'LinearBinary', 'bert.encoder.layer.1.attention.self.value_type': 'LinearBinary', 'bert.encoder.layer.1.attention.output.dense_type': 'LinearBinary', 'bert.encoder.layer.1.intermediate.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.output.dense_type': 'torch.nn.Linear', 'bert.pooler.dense_type': 'torch.nn.Linear', 'classifier_type': 'torch.nn.Linear'}. Best is trial 0 with value: 0.49068.\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n","[W 2025-02-10 11:12:26,254] Trial 1 failed with parameters: {'bert.encoder.layer.0.attention.self.query_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.self.key_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.self.value_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.output.dense_type': 'LinearBinary', 'widths': 2, 'frac_widths': 0, 'stochastic': 1, 'bipolar': 1, 'data_in_exponent_width': 2, 'weight_exponent_width': 2, 'bias_exponent_width': 2, 'data_in_exponent_bias': 1, 'weight_exponent_bias': 2, 'bias_exponent_bias': 2, 'bert.encoder.layer.0.intermediate.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.output.dense_type': 'LinearBinary', 'bert.encoder.layer.1.attention.self.query_type': 'LinearBinary', 'bert.encoder.layer.1.attention.self.key_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.value_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.intermediate.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.output.dense_type': 'LinearBinary', 'bert.pooler.dense_type': 'torch.nn.Linear', 'classifier_type': 'LinearBinary'} because of the following error: IndexError('Dimension out of range (expected to be in range of [-2, 1], but got 2)').\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n","    value_or_values = func(trial)\n","                      ^^^^^^^^^^^\n","  File \"<ipython-input-10-2c770aca15cb>\", line 18, in objective\n","    trainer.train()\n","  File \"/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\", line 2171, in train\n","    return inner_training_loop(\n","           ^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\", line 2531, in _inner_training_loop\n","    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)\n","                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\", line 3675, in training_step\n","    loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\", line 3731, in compute_loss\n","    outputs = model(**inputs)\n","              ^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n","    return forward_call(*args, **kwargs)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\", line 1665, in forward\n","    outputs = self.bert(\n","              ^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n","    return forward_call(*args, **kwargs)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\", line 1142, in forward\n","    encoder_outputs = self.encoder(\n","                      ^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n","    return forward_call(*args, **kwargs)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\", line 695, in forward\n","    layer_outputs = layer_module(\n","                    ^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n","    return forward_call(*args, **kwargs)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\", line 585, in forward\n","    self_attention_outputs = self.attention(\n","                             ^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n","    return forward_call(*args, **kwargs)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\", line 524, in forward\n","    attention_output = self.output(self_outputs[0], hidden_states)\n","                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n","    return forward_call(*args, **kwargs)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\", line 466, in forward\n","    hidden_states = self.dense(hidden_states)\n","                    ^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n","    return forward_call(*args, **kwargs)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/content/mase/src/chop/nn/quantized/modules/linear.py\", line 288, in forward\n","    return linearBinary(x, self.weight, self.bias, self.config)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/content/mase/src/chop/nn/quantized/functional/linear.py\", line 458, in linearBinary\n","    weight = w_quantizer(weight)\n","             ^^^^^^^^^^^^^^^^^^^\n","  File \"/content/mase/src/chop/nn/quantizers/binary.py\", line 51, in binary_quantizer\n","    x = binarised_bipolar_op(x, 0) if bipolar else binarised_zeroScaled_op(x, 0)\n","                                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/torch/autograd/function.py\", line 575, in apply\n","    return super().apply(*args, **kwargs)  # type: ignore[misc]\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/content/mase/src/chop/nn/quantizers/utils.py\", line 163, in forward\n","    alpha = BinaryZeroScaled.alpha(input)\n","            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/content/mase/src/chop/nn/quantizers/utils.py\", line 158, in alpha\n","    alpha = absvalue.mean(dim=(1, 2, 3), keepdims=True)\n","            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","IndexError: Dimension out of range (expected to be in range of [-2, 1], but got 2)\n","[W 2025-02-10 11:12:26,263] Trial 1 failed with value None.\n"]},{"output_type":"error","ename":"IndexError","evalue":"Dimension out of range (expected to be in range of [-2, 1], but got 2)","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-74a67f774197>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;31m# Run the study\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;31m# Store raw results for this precision\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    473\u001b[0m                 \u001b[0mIf\u001b[0m \u001b[0mnested\u001b[0m \u001b[0minvocation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0moccurs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \"\"\"\n\u001b[0;32m--> 475\u001b[0;31m         _optimize(\n\u001b[0m\u001b[1;32m    476\u001b[0m             \u001b[0mstudy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             _optimize_sequential(\n\u001b[0m\u001b[1;32m     64\u001b[0m                 \u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             \u001b[0mfrozen_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;31m# The following line mitigates memory problems that can be occurred in some\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     ):\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfunc_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrozen_trial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mget_heartbeat_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m             \u001b[0mvalue_or_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-10-2c770aca15cb>\u001b[0m in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     16\u001b[0m     )\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0meval_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2169\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2170\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2171\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2172\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2173\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2529\u001b[0m                     )\n\u001b[1;32m   2530\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2531\u001b[0;31m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2533\u001b[0m                     if (\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3673\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3674\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3675\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3677\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3729\u001b[0m                 \u001b[0mloss_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"num_items_in_batch\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3730\u001b[0m             \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mloss_kwargs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3731\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3732\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3733\u001b[0m         \u001b[0;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1663\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1664\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1665\u001b[0;31m         outputs = self.bert(\n\u001b[0m\u001b[1;32m   1666\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1140\u001b[0m         \u001b[0mhead_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_head_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1142\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m   1143\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1144\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    693\u001b[0m                 )\n\u001b[1;32m    694\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 695\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    696\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    583\u001b[0m         \u001b[0;31m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0mself_attn_past_key_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpast_key_value\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpast_key_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 585\u001b[0;31m         self_attention_outputs = self.attention(\n\u001b[0m\u001b[1;32m    586\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    522\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m         )\n\u001b[0;32m--> 524\u001b[0;31m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    525\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# add attentions if we output them\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/mase/src/chop/nn/quantized/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbypass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlinearBinary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/mase/src/chop/nn/quantized/functional/linear.py\u001b[0m in \u001b[0;36mlinearBinary\u001b[0;34m(x, weight, bias, config)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_quantizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m     \u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw_quantizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m     \u001b[0mbias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb_quantizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/mase/src/chop/nn/quantizers/binary.py\u001b[0m in \u001b[0;36mbinary_quantizer\u001b[0;34m(x, stochastic, bipolar)\u001b[0m\n\u001b[1;32m     49\u001b[0m         )\n\u001b[1;32m     50\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbinarised_bipolar_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mbipolar\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mbinarised_zeroScaled_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/function.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    573\u001b[0m             \u001b[0;31m# See NOTE: [functorch vjp and autograd interaction]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_functorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap_dead_wrappers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_setup_ctx_defined\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/mase/src/chop/nn/quantizers/utils.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, input, _threshold)\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_threshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m         \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBinaryZeroScaled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0mpos_one\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/mase/src/chop/nn/quantizers/utils.py\u001b[0m in \u001b[0;36malpha\u001b[0;34m(tensor)\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# determine batch means\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0mabsvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m         \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mabsvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-2, 1], but got 2)"]}],"source":["import optuna\n","import matplotlib.pyplot as plt\n","from optuna.samplers import TPESampler\n","\n","# Define all precision layers to test\n","precision_layers = [\n","    \"LinearMinifloatDenorm\", # Reduced-precision floating-point (e.g., 16-bit or lower).\n","    \"LinearMinifloatIEEE\",\n","    \"LinearLog\", # Stores values in a logarithmic scale (better range for small values). Handles small values well, Not hardware-efficient.\n","    \"LinearBlockFP\", # Groups values into blocks with a shared exponent. Good compromise Limited flexibility.\n","    \"LinearBlockLog\",\n","    \"LinearBinary\",\n","    \"LinearBinaryScaling\",\n","    \"LinearBinaryResidualSign\",\n","]\n","\n","# Dictionary to store raw results for each precision\n","all_results = {}\n","\n","\n","# Loop through each precision layer and run Optuna\n","for precision in precision_layers:\n","    print()\n","    print(f\"Running study for {precision}...\")\n","    print()\n","\n","    # Set the layer mapping for this run\n","    layer_mapping = {\n","        \"torch.nn.Linear\": torch.nn.Linear,\n","        precision: globals()[precision],  # Dynamically get the class\n","    }\n","\n","    # Update search_space dynamically for the current precision\n","    search_space = {\n","        \"linear_layer_choices\": list(layer_mapping.keys()),\n","        \"widths\": [8, 16, 32],\n","        \"frac_widths\": [2, 4, 8],\n","        \"exponent_widths\": [3, 4, 5],\n","        \"exponent_bias\": [0, 1, 2],\n","        \"exponent_bias_width\": [1, 2, 3],\n","        \"block_sizes\": [[2], [4], [8]],\n","        \"stochastic\": [True, False],\n","        \"bipolar\": [True, False],\n","        \"binary\": [True, False],\n","    }\n","\n","    # Create a new study\n","    sampler = TPESampler()\n","    study = optuna.create_study(\n","        direction=\"maximize\",\n","        study_name=f\"bert-tiny-nas-{precision}\",\n","        sampler=sampler,\n","    )\n","\n","    # Run the study\n","    study.optimize(objective, n_trials=20)\n","\n","    # Store raw results for this precision\n","    all_results[precision] = [trial.value for trial in study.trials]\n","    print(all_results)"]},{"cell_type":"code","source":["all_results = {'LinearMinifloatDenorm': [0.8438, 0.83252, 0.8426, 0.8408, 0.84124, 0.8414, 0.84128, 0.84388, 0.84268, 0.8192, 0.84316, 0.84212, 0.84212, 0.84472, 0.84448, 0.84472, 0.84472, 0.84472, 0.84328, 0.84472],\n","               'LinearMinifloatIEEE': [0.84264, 0.8422, 0.84316, 0.8182, 0.5, 0.84032, 0.84132, 0.84216, 0.84248, 0.83932, 0.84156, 0.84244, 0.84232, 0.8432, 0.84148, 0.84336, 0.84356, 0.84336, 0.84344, 0.84264],\n","               'LinearLog': [0.50124, 0.5, 0.49984, 0.6574, 0.50472, 0.5, 0.5, 0.5, 0.49604, 0.50052, 0.5, 0.4986, 0.50432, 0.5, 0.50244, 0.4958, 0.50392, 0.81572, 0.5, 0.5558],\n","               'LinearBlockFP': [0.84208, 0.84268, 0.8428, 0.84168, 0.84136, 0.84184, 0.84196, 0.84248, 0.84296, 0.84344, 0.8426, 0.84312, 0.84384, 0.84212, 0.84208, 0.84212, 0.84344, 0.8448, 0.84192, 0.84468],\n","               'LinearBlockLog': [0.8412, 0.84084, 0.8418, 0.84276, 0.8366, 0.84072, 0.83784, 0.84028, 0.83984, 0.843, 0.84332, 0.84344, 0.8432, 0.84352, 0.84348, 0.84384, 0.84384, 0.8438, 0.843, 0.84352]}\n","\n","print(all_results)"],"metadata":{"id":"KP5mpqY3LHyD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1739206756317,"user_tz":0,"elapsed":14,"user":{"displayName":"george theof","userId":"15049073496200557337"}},"outputId":"a119bd16-c818-4cc2-97dd-447e35100b1a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'LinearMinifloatDenorm': [0.8438, 0.83252, 0.8426, 0.8408, 0.84124, 0.8414, 0.84128, 0.84388, 0.84268, 0.8192, 0.84316, 0.84212, 0.84212, 0.84472, 0.84448, 0.84472, 0.84472, 0.84472, 0.84328, 0.84472], 'LinearMinifloatIEEE': [0.84264, 0.8422, 0.84316, 0.8182, 0.5, 0.84032, 0.84132, 0.84216, 0.84248, 0.83932, 0.84156, 0.84244, 0.84232, 0.8432, 0.84148, 0.84336, 0.84356, 0.84336, 0.84344, 0.84264], 'LinearLog': [0.50124, 0.5, 0.49984, 0.6574, 0.50472, 0.5, 0.5, 0.5, 0.49604, 0.50052, 0.5, 0.4986, 0.50432, 0.5, 0.50244, 0.4958, 0.50392, 0.81572, 0.5, 0.5558], 'LinearBlockFP': [0.84208, 0.84268, 0.8428, 0.84168, 0.84136, 0.84184, 0.84196, 0.84248, 0.84296, 0.84344, 0.8426, 0.84312, 0.84384, 0.84212, 0.84208, 0.84212, 0.84344, 0.8448, 0.84192, 0.84468], 'LinearBlockLog': [0.8412, 0.84084, 0.8418, 0.84276, 0.8366, 0.84072, 0.83784, 0.84028, 0.83984, 0.843, 0.84332, 0.84344, 0.8432, 0.84352, 0.84348, 0.84384, 0.84384, 0.8438, 0.843, 0.84352]}\n"]}]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","plt.figure(figsize=(12, 6))\n","\n","for precision, accuracies in all_results.items():\n","    max_accuracies = [max(accuracies[:i+1]) for i in range(len(accuracies))]  # Track max accuracy at each trial\n","    plt.plot(range(1, len(max_accuracies) + 1), max_accuracies, label=precision)\n","\n","plt.xlabel(\"Number of Trials\")\n","plt.ylabel(\"Max Accuracy\")\n","plt.title(\"Comparison of Precision Layers\")\n","plt.legend()\n","plt.grid()\n","plt.show()"],"metadata":{"id":"EighSsDkLGcg","colab":{"base_uri":"https://localhost:8080/","height":564},"executionInfo":{"status":"ok","timestamp":1739206779919,"user_tz":0,"elapsed":440,"user":{"displayName":"george theof","userId":"15049073496200557337"}},"outputId":"125dd681-01a1-4058-ae49-c6a287e3812b"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 1200x600 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"kho8JJQNdj0T"},"source":["# Appendx\n","\n"]},{"cell_type":"code","source":["[I 2025-02-08 10:44:01,567] A new study created in memory with name: bert-tiny-nas-LinearMinifloatDenorm\n","\n","Running study for LinearMinifloatDenorm...\n","\n","Downloading builder script: 100%\n"," 4.20k/4.20k [00:00<00:00, 276kB/s]\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"," [3125/3125 02:00, Epoch 1/1]\n","Step\tTraining Loss\n","500\t0.693700\n","1000\t0.622200\n","1500\t0.457200\n","2000\t0.405000\n","2500\t0.375800\n","3000\t0.381100\n"," [3125/3125 01:23]\n","[I 2025-02-08 10:47:30,402] Trial 0 finished with value: 0.8438 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'LinearMinifloatDenorm', 'widths': 2, 'frac_widths': 2, 'stochastic': 0, 'bipolar': 0, 'data_in_exponent_width': 2, 'weight_exponent_width': 1, 'bias_exponent_width': 0, 'data_in_exponent_bias': 1, 'weight_exponent_bias': 2, 'bias_exponent_bias': 2, 'bert.encoder.layer.0.attention.self.key_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.self.value_type': 'LinearMinifloatDenorm', 'bert.encoder.layer.0.attention.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.intermediate.dense_type': 'LinearMinifloatDenorm', 'bert.encoder.layer.0.output.dense_type': 'LinearMinifloatDenorm', 'bert.encoder.layer.1.attention.self.query_type': 'LinearMinifloatDenorm', 'bert.encoder.layer.1.attention.self.key_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.value_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.output.dense_type': 'LinearMinifloatDenorm', 'bert.encoder.layer.1.intermediate.dense_type': 'LinearMinifloatDenorm', 'bert.encoder.layer.1.output.dense_type': 'torch.nn.Linear', 'bert.pooler.dense_type': 'torch.nn.Linear', 'classifier_type': 'torch.nn.Linear'}. Best is trial 0 with value: 0.8438.\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"," [3125/3125 01:52, Epoch 1/1]\n","Step\tTraining Loss\n","500\t0.693700\n","1000\t0.617400\n","1500\t0.506800\n","2000\t0.461300\n","2500\t0.405200\n","3000\t0.395700\n"," [3125/3125 01:15]\n","[I 2025-02-08 10:50:41,156] Trial 1 finished with value: 0.83252 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'LinearMinifloatDenorm', 'widths': 2, 'frac_widths': 0, 'stochastic': 0, 'bipolar': 1, 'data_in_exponent_width': 0, 'weight_exponent_width': 0, 'bias_exponent_width': 2, 'data_in_exponent_bias': 1, 'weight_exponent_bias': 1, 'bias_exponent_bias': 1, 'bert.encoder.layer.0.attention.self.key_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.self.value_type': 'LinearMinifloatDenorm', 'bert.encoder.layer.0.attention.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.intermediate.dense_type': 'LinearMinifloatDenorm', 'bert.encoder.layer.0.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.query_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.key_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.value_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.output.dense_type': 'LinearMinifloatDenorm', 'bert.encoder.layer.1.intermediate.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.output.dense_type': 'torch.nn.Linear', 'bert.pooler.dense_type': 'LinearMinifloatDenorm', 'classifier_type': 'LinearMinifloatDenorm'}. Best is trial 0 with value: 0.8438.\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"," [3125/3125 01:45, Epoch 1/1]\n","Step\tTraining Loss\n","500\t0.693800\n","1000\t0.620600\n","1500\t0.469700\n","2000\t0.406900\n","2500\t0.370200\n","3000\t0.376100\n"," [3125/3125 01:07]\n","[I 2025-02-08 10:53:36,145] Trial 2 finished with value: 0.8426 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.self.key_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.self.value_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.output.dense_type': 'LinearMinifloatDenorm', 'widths': 1, 'frac_widths': 1, 'stochastic': 0, 'bipolar': 0, 'data_in_exponent_width': 1, 'weight_exponent_width': 1, 'bias_exponent_width': 1, 'data_in_exponent_bias': 0, 'weight_exponent_bias': 2, 'bias_exponent_bias': 0, 'bert.encoder.layer.0.intermediate.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.query_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.key_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.value_type': 'LinearMinifloatDenorm', 'bert.encoder.layer.1.attention.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.intermediate.dense_type': 'LinearMinifloatDenorm', 'bert.encoder.layer.1.output.dense_type': 'LinearMinifloatDenorm', 'bert.pooler.dense_type': 'LinearMinifloatDenorm', 'classifier_type': 'torch.nn.Linear'}. Best is trial 0 with value: 0.8438.\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"," [3125/3125 01:40, Epoch 1/1]\n","Step\tTraining Loss\n","500\t0.693100\n","1000\t0.691800\n","1500\t0.566100\n","2000\t0.427400\n","2500\t0.399900\n","3000\t0.406000\n"," [3125/3125 01:05]\n","[I 2025-02-08 10:56:23,878] Trial 3 finished with value: 0.8408 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.self.key_type': 'LinearMinifloatDenorm', 'widths': 2, 'frac_widths': 0, 'stochastic': 1, 'bipolar': 0, 'data_in_exponent_width': 2, 'weight_exponent_width': 2, 'bias_exponent_width': 2, 'data_in_exponent_bias': 2, 'weight_exponent_bias': 1, 'bias_exponent_bias': 0, 'bert.encoder.layer.0.attention.self.value_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.intermediate.dense_type': 'LinearMinifloatDenorm', 'bert.encoder.layer.0.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.query_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.key_type': 'LinearMinifloatDenorm', 'bert.encoder.layer.1.attention.self.value_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.intermediate.dense_type': 'LinearMinifloatDenorm', 'bert.encoder.layer.1.output.dense_type': 'torch.nn.Linear', 'bert.pooler.dense_type': 'LinearMinifloatDenorm', 'classifier_type': 'torch.nn.Linear'}. Best is trial 0 with value: 0.8438.\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"," [3125/3125 01:56, Epoch 1/1]\n","Step\tTraining Loss\n","500\t0.693800\n","1000\t0.629500\n","1500\t0.478000\n","2000\t0.411700\n","2500\t0.377400\n","3000\t0.375100\n"," [3125/3125 01:15]\n","[I 2025-02-08 10:59:38,384] Trial 4 finished with value: 0.84124 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.self.key_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.self.value_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.intermediate.dense_type': 'LinearMinifloatDenorm', 'widths': 1, 'frac_widths': 0, 'stochastic': 1, 'bipolar': 1, 'data_in_exponent_width': 0, 'weight_exponent_width': 1, 'bias_exponent_width': 2, 'data_in_exponent_bias': 2, 'weight_exponent_bias': 1, 'bias_exponent_bias': 1, 'bert.encoder.layer.0.output.dense_type': 'LinearMinifloatDenorm', 'bert.encoder.layer.1.attention.self.query_type': 'LinearMinifloatDenorm', 'bert.encoder.layer.1.attention.self.key_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.value_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.output.dense_type': 'LinearMinifloatDenorm', 'bert.encoder.layer.1.intermediate.dense_type': 'LinearMinifloatDenorm', 'bert.encoder.layer.1.output.dense_type': 'LinearMinifloatDenorm', 'bert.pooler.dense_type': 'torch.nn.Linear', 'classifier_type': 'torch.nn.Linear'}. Best is trial 0 with value: 0.8438.\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"," [3125/3125 02:26, Epoch 1/1]\n","Step\tTraining Loss\n","500\t0.693800\n","1000\t0.622500\n","1500\t0.471900\n","2000\t0.409600\n","2500\t0.372600\n","3000\t0.378400\n"," [3125/3125 01:49]\n","[I 2025-02-08 11:03:56,277] Trial 5 finished with value: 0.8414 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.self.key_type': 'LinearMinifloatDenorm', 'widths': 0, 'frac_widths': 1, 'stochastic': 1, 'bipolar': 1, 'data_in_exponent_width': 1, 'weight_exponent_width': 0, 'bias_exponent_width': 0, 'data_in_exponent_bias': 1, 'weight_exponent_bias': 0, 'bias_exponent_bias': 0, 'bert.encoder.layer.0.attention.self.value_type': 'LinearMinifloatDenorm', 'bert.encoder.layer.0.attention.output.dense_type': 'LinearMinifloatDenorm', 'bert.encoder.layer.0.intermediate.dense_type': 'LinearMinifloatDenorm', 'bert.encoder.layer.0.output.dense_type': 'LinearMinifloatDenorm', 'bert.encoder.layer.1.attention.self.query_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.key_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.value_type': 'LinearMinifloatDenorm', 'bert.encoder.layer.1.attention.output.dense_type': 'LinearMinifloatDenorm', 'bert.encoder.layer.1.intermediate.dense_type': 'LinearMinifloatDenorm', 'bert.encoder.layer.1.output.dense_type': 'LinearMinifloatDenorm', 'bert.pooler.dense_type': 'LinearMinifloatDenorm', 'classifier_type': 'torch.nn.Linear'}. Best is trial 0 with value: 0.8438.\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"," [3125/3125 02:40, Epoch 1/1]\n","Step\tTraining Loss\n","500\t0.693800\n","1000\t0.623500\n","1500\t0.470200\n","2000\t0.409500\n","2500\t0.373300\n","3000\t0.378000\n"," [3125/3125 01:56]\n","[I 2025-02-08 11:08:35,558] Trial 6 finished with value: 0.84128 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.self.key_type': 'LinearMinifloatDenorm', 'widths': 1, 'frac_widths': 2, 'stochastic': 1, 'bipolar': 1, 'data_in_exponent_width': 2, 'weight_exponent_width': 1, 'bias_exponent_width': 0, 'data_in_exponent_bias': 1, 'weight_exponent_bias': 0, 'bias_exponent_bias': 1, 'bert.encoder.layer.0.attention.self.value_type': 'LinearMinifloatDenorm', 'bert.encoder.layer.0.attention.output.dense_type': 'LinearMinifloatDenorm', 'bert.encoder.layer.0.intermediate.dense_type': 'LinearMinifloatDenorm', 'bert.encoder.layer.0.output.dense_type': 'LinearMinifloatDenorm', 'bert.encoder.layer.1.attention.self.query_type': 'LinearMinifloatDenorm', 'bert.encoder.layer.1.attention.self.key_type': 'LinearMinifloatDenorm', 'bert.encoder.layer.1.attention.self.value_type': 'LinearMinifloatDenorm', 'bert.encoder.layer.1.attention.output.dense_type': 'LinearMinifloatDenorm', 'bert.encoder.layer.1.intermediate.dense_type': 'LinearMinifloatDenorm', 'bert.encoder.layer.1.output.dense_type': 'LinearMinifloatDenorm', 'bert.pooler.dense_type': 'LinearMinifloatDenorm', 'classifier_type': 'torch.nn.Linear'}. Best is trial 0 with value: 0.8438.\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"," [3125/3125 01:53, Epoch 1/1]\n","Step\tTraining Loss\n","500\t0.693900\n","1000\t0.620600\n","1500\t0.476000\n","2000\t0.408600\n","2500\t0.371100\n","3000\t0.376700\n"," [3125/3125 01:17]\n","[I 2025-02-08 11:11:48,195] Trial 7 finished with value: 0.84388 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'LinearMinifloatDenorm', 'widths': 1, 'frac_widths': 2, 'stochastic': 1, 'bipolar': 1, 'data_in_exponent_width': 2, 'weight_exponent_width': 0, 'bias_exponent_width': 0, 'data_in_exponent_bias': 2, 'weight_exponent_bias': 1, 'bias_exponent_bias': 0, 'bert.encoder.layer.0.attention.self.key_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.self.value_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.output.dense_type': 'LinearMinifloatDenorm', 'bert.encoder.layer.0.intermediate.dense_type': 'LinearMinifloatDenorm', 'bert.encoder.layer.0.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.query_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.key_type': 'LinearMinifloatDenorm', 'bert.encoder.layer.1.attention.self.value_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.output.dense_type': 'LinearMinifloatDenorm', 'bert.encoder.layer.1.intermediate.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.output.dense_type': 'torch.nn.Linear', 'bert.pooler.dense_type': 'LinearMinifloatDenorm', 'classifier_type': 'LinearMinifloatDenorm'}. Best is trial 7 with value: 0.84388.\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"," [3125/3125 01:35, Epoch 1/1]\n","Step\tTraining Loss\n","500\t0.693800\n","1000\t0.621500\n","1500\t0.467700\n","2000\t0.406400\n","2500\t0.371000\n","3000\t0.375500\n"," [3125/3125 00:57]\n","[I 2025-02-08 11:14:24,870] Trial 8 finished with value: 0.84268 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.self.key_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.self.value_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.intermediate.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.output.dense_type': 'LinearMinifloatDenorm', 'widths': 1, 'frac_widths': 1, 'stochastic': 1, 'bipolar': 0, 'data_in_exponent_width': 2, 'weight_exponent_width': 2, 'bias_exponent_width': 0, 'data_in_exponent_bias': 0, 'weight_exponent_bias': 0, 'bias_exponent_bias': 2, 'bert.encoder.layer.1.attention.self.query_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.key_type': 'LinearMinifloatDenorm', 'bert.encoder.layer.1.attention.self.value_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.intermediate.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.output.dense_type': 'LinearMinifloatDenorm', 'bert.pooler.dense_type': 'torch.nn.Linear', 'classifier_type': 'torch.nn.Linear'}. Best is trial 7 with value: 0.84388.\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"," [3125/3125 02:01, Epoch 1/1]\n","Step\tTraining Loss\n","500\t0.693300\n","1000\t0.693200\n","1500\t0.682300\n","2000\t0.650700\n","2500\t0.586100\n","3000\t0.576500\n"," [3125/3125 01:25]\n","[I 2025-02-08 11:17:54,676] Trial 9 finished with value: 0.8192 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.self.key_type': 'LinearMinifloatDenorm', 'widths': 1, 'frac_widths': 0, 'stochastic': 0, 'bipolar': 0, 'data_in_exponent_width': 1, 'weight_exponent_width': 1, 'bias_exponent_width': 0, 'data_in_exponent_bias': 1, 'weight_exponent_bias': 0, 'bias_exponent_bias': 1, 'bert.encoder.layer.0.attention.self.value_type': 'LinearMinifloatDenorm', 'bert.encoder.layer.0.attention.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.intermediate.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.query_type': 'LinearMinifloatDenorm', 'bert.encoder.layer.1.attention.self.key_type': 'LinearMinifloatDenorm', 'bert.encoder.layer.1.attention.self.value_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.intermediate.dense_type': 'LinearMinifloatDenorm', 'bert.encoder.layer.1.output.dense_type': 'LinearMinifloatDenorm', 'bert.pooler.dense_type': 'torch.nn.Linear', 'classifier_type': 'LinearMinifloatDenorm'}. Best is trial 7 with value: 0.84388.\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"," [3125/3125 01:56, Epoch 1/1]\n","Step\tTraining Loss\n","500\t0.693800\n","1000\t0.620400\n","1500\t0.477800\n","2000\t0.410100\n","2500\t0.371000\n","3000\t0.377000\n"," [3125/3125 01:22]\n","[I 2025-02-08 11:21:16,114] Trial 10 finished with value: 0.84316 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'LinearMinifloatDenorm', 'widths': 0, 'frac_widths': 2, 'stochastic': 1, 'bipolar': 1, 'data_in_exponent_width': 2, 'weight_exponent_width': 0, 'bias_exponent_width': 1, 'data_in_exponent_bias': 2, 'weight_exponent_bias': 2, 'bias_exponent_bias': 0, 'bert.encoder.layer.0.attention.self.key_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.self.value_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.output.dense_type': 'LinearMinifloatDenorm', 'bert.encoder.layer.0.intermediate.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.query_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.key_type': 'LinearMinifloatDenorm', 'bert.encoder.layer.1.attention.self.value_type': 'LinearMinifloatDenorm', 'bert.encoder.layer.1.attention.output.dense_type': 'LinearMinifloatDenorm', 'bert.encoder.layer.1.intermediate.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.output.dense_type': 'torch.nn.Linear', 'bert.pooler.dense_type': 'LinearMinifloatDenorm', 'classifier_type': 'LinearMinifloatDenorm'}. Best is trial 7 with value: 0.84388.\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"," [3125/3125 02:04, Epoch 1/1]\n","Step\tTraining Loss\n","500\t0.693800\n","1000\t0.622700\n","1500\t0.466900\n","2000\t0.407800\n","2500\t0.373300\n","3000\t0.377600\n"," [3125/3125 01:43]\n","[I 2025-02-08 11:25:07,369] Trial 11 finished with value: 0.84212 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'LinearMinifloatDenorm', 'widths': 2, 'frac_widths': 2, 'stochastic': 0, 'bipolar': 0, 'data_in_exponent_width': 2, 'weight_exponent_width': 0, 'bias_exponent_width': 1, 'data_in_exponent_bias': 2, 'weight_exponent_bias': 2, 'bias_exponent_bias': 2, 'bert.encoder.layer.0.attention.self.key_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.self.value_type': 'LinearMinifloatDenorm', 'bert.encoder.layer.0.attention.output.dense_type': 'LinearMinifloatDenorm', 'bert.encoder.layer.0.intermediate.dense_type': 'LinearMinifloatDenorm', 'bert.encoder.layer.0.output.dense_type': 'LinearMinifloatDenorm', 'bert.encoder.layer.1.attention.self.query_type': 'LinearMinifloatDenorm', 'bert.encoder.layer.1.attention.self.key_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.value_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.output.dense_type': 'LinearMinifloatDenorm', 'bert.encoder.layer.1.intermediate.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.output.dense_type': 'torch.nn.Linear', 'bert.pooler.dense_type': 'torch.nn.Linear', 'classifier_type': 'LinearMinifloatDenorm'}. Best is trial 7 with value: 0.84388.\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"," [3125/3125 02:15, Epoch 1/1]\n","Step\tTraining Loss\n","500\t0.693800\n","1000\t0.622700\n","1500\t0.466900\n","2000\t0.407800\n","2500\t0.373300\n","3000\t0.377600\n"," [3125/3125 01:30]\n","[I 2025-02-08 11:28:57,123] Trial 12 finished with value: 0.84212 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'LinearMinifloatDenorm', 'widths': 2, 'frac_widths': 2, 'stochastic': 0, 'bipolar': 1, 'data_in_exponent_width': 2, 'weight_exponent_width': 2, 'bias_exponent_width': 0, 'data_in_exponent_bias': 0, 'weight_exponent_bias': 2, 'bias_exponent_bias': 2, 'bert.encoder.layer.0.attention.self.key_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.self.value_type': 'LinearMinifloatDenorm', 'bert.encoder.layer.0.attention.output.dense_type': 'LinearMinifloatDenorm', 'bert.encoder.layer.0.intermediate.dense_type': 'LinearMinifloatDenorm', 'bert.encoder.layer.0.output.dense_type': 'LinearMinifloatDenorm', 'bert.encoder.layer.1.attention.self.query_type': 'LinearMinifloatDenorm', 'bert.encoder.layer.1.attention.self.key_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.value_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.output.dense_type': 'LinearMinifloatDenorm', 'bert.encoder.layer.1.intermediate.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.output.dense_type': 'torch.nn.Linear', 'bert.pooler.dense_type': 'torch.nn.Linear', 'classifier_type': 'LinearMinifloatDenorm'}. Best is trial 7 with value: 0.84388.\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"," [3125/3125 01:48, Epoch 1/1]\n","Step\tTraining Loss\n","500\t0.693800\n","1000\t0.619700\n","1500\t0.472200\n","2000\t0.405200\n","2500\t0.370000\n","3000\t0.375300\n"," [3125/3125 01:11]\n","[I 2025-02-08 11:31:59,670] Trial 13 finished with value: 0.84472 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'LinearMinifloatDenorm', 'widths': 0, 'frac_widths': 2, 'stochastic': 0, 'bipolar': 0, 'data_in_exponent_width': 1, 'weight_exponent_width': 0, 'bias_exponent_width': 0, 'data_in_exponent_bias': 1, 'weight_exponent_bias': 1, 'bias_exponent_bias': 2, 'bert.encoder.layer.0.attention.self.key_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.self.value_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.intermediate.dense_type': 'LinearMinifloatDenorm', 'bert.encoder.layer.0.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.query_type': 'LinearMinifloatDenorm', 'bert.encoder.layer.1.attention.self.key_type': 'LinearMinifloatDenorm', 'bert.encoder.layer.1.attention.self.value_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.output.dense_type': 'LinearMinifloatDenorm', 'bert.encoder.layer.1.intermediate.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.output.dense_type': 'torch.nn.Linear', 'bert.pooler.dense_type': 'torch.nn.Linear', 'classifier_type': 'LinearMinifloatDenorm'}. Best is trial 13 with value: 0.84472.\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"," [3125/3125 01:53, Epoch 1/1]\n","Step\tTraining Loss\n","500\t0.693800\n","1000\t0.617000\n","1500\t0.474300\n","2000\t0.405300\n","2500\t0.369500\n","3000\t0.376600\n"," [3125/3125 01:15]\n","[I 2025-02-08 11:35:11,175] Trial 14 finished with value: 0.84448 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'LinearMinifloatDenorm', 'widths': 0, 'frac_widths': 2, 'stochastic': 0, 'bipolar': 0, 'data_in_exponent_width': 1, 'weight_exponent_width': 0, 'bias_exponent_width': 1, 'data_in_exponent_bias': 2, 'weight_exponent_bias': 1, 'bias_exponent_bias': 1, 'bert.encoder.layer.0.attention.self.key_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.self.value_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.intermediate.dense_type': 'LinearMinifloatDenorm', 'bert.encoder.layer.0.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.query_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.key_type': 'LinearMinifloatDenorm', 'bert.encoder.layer.1.attention.self.value_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.output.dense_type': 'LinearMinifloatDenorm', 'bert.encoder.layer.1.intermediate.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.output.dense_type': 'torch.nn.Linear', 'bert.pooler.dense_type': 'LinearMinifloatDenorm', 'classifier_type': 'LinearMinifloatDenorm'}. Best is trial 13 with value: 0.84472.\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"," [3125/3125 01:52, Epoch 1/1]\n","Step\tTraining Loss\n","500\t0.693800\n","1000\t0.619700\n","1500\t0.472300\n","2000\t0.405200\n","2500\t0.370100\n","3000\t0.375300\n"," [3125/3125 01:15]\n","[I 2025-02-08 11:38:21,214] Trial 15 finished with value: 0.84472 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'LinearMinifloatDenorm', 'widths': 0, 'frac_widths': 1, 'stochastic': 0, 'bipolar': 0, 'data_in_exponent_width': 1, 'weight_exponent_width': 0, 'bias_exponent_width': 1, 'data_in_exponent_bias': 1, 'weight_exponent_bias': 1, 'bias_exponent_bias': 1, 'bert.encoder.layer.0.attention.self.key_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.self.value_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.intermediate.dense_type': 'LinearMinifloatDenorm', 'bert.encoder.layer.0.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.query_type': 'LinearMinifloatDenorm', 'bert.encoder.layer.1.attention.self.key_type': 'LinearMinifloatDenorm', 'bert.encoder.layer.1.attention.self.value_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.output.dense_type': 'LinearMinifloatDenorm', 'bert.encoder.layer.1.intermediate.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.output.dense_type': 'torch.nn.Linear', 'bert.pooler.dense_type': 'torch.nn.Linear', 'classifier_type': 'LinearMinifloatDenorm'}. Best is trial 13 with value: 0.84472.\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"," [3125/3125 01:51, Epoch 1/1]\n","Step\tTraining Loss\n","500\t0.693800\n","1000\t0.619700\n","1500\t0.472200\n","2000\t0.405200\n","2500\t0.370100\n","3000\t0.375300\n"," [3125/3125 01:10]\n","[I 2025-02-08 11:41:24,949] Trial 16 finished with value: 0.84472 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'LinearMinifloatDenorm', 'widths': 0, 'frac_widths': 1, 'stochastic': 0, 'bipolar': 0, 'data_in_exponent_width': 0, 'weight_exponent_width': 0, 'bias_exponent_width': 1, 'data_in_exponent_bias': 1, 'weight_exponent_bias': 1, 'bias_exponent_bias': 2, 'bert.encoder.layer.0.attention.self.key_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.self.value_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.intermediate.dense_type': 'LinearMinifloatDenorm', 'bert.encoder.layer.0.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.query_type': 'LinearMinifloatDenorm', 'bert.encoder.layer.1.attention.self.key_type': 'LinearMinifloatDenorm', 'bert.encoder.layer.1.attention.self.value_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.output.dense_type': 'LinearMinifloatDenorm', 'bert.encoder.layer.1.intermediate.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.output.dense_type': 'torch.nn.Linear', 'bert.pooler.dense_type': 'torch.nn.Linear', 'classifier_type': 'LinearMinifloatDenorm'}. Best is trial 13 with value: 0.84472.\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"," [3125/3125 01:49, Epoch 1/1]\n","Step\tTraining Loss\n","500\t0.693800\n","1000\t0.619700\n","1500\t0.472300\n","2000\t0.405200\n","2500\t0.370100\n","3000\t0.375300\n"," [3125/3125 01:13]\n","[I 2025-02-08 11:44:30,111] Trial 17 finished with value: 0.84472 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'LinearMinifloatDenorm', 'widths': 0, 'frac_widths': 1, 'stochastic': 0, 'bipolar': 0, 'data_in_exponent_width': 1, 'weight_exponent_width': 0, 'bias_exponent_width': 2, 'data_in_exponent_bias': 0, 'weight_exponent_bias': 1, 'bias_exponent_bias': 1, 'bert.encoder.layer.0.attention.self.key_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.self.value_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.intermediate.dense_type': 'LinearMinifloatDenorm', 'bert.encoder.layer.0.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.query_type': 'LinearMinifloatDenorm', 'bert.encoder.layer.1.attention.self.key_type': 'LinearMinifloatDenorm', 'bert.encoder.layer.1.attention.self.value_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.output.dense_type': 'LinearMinifloatDenorm', 'bert.encoder.layer.1.intermediate.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.output.dense_type': 'torch.nn.Linear', 'bert.pooler.dense_type': 'torch.nn.Linear', 'classifier_type': 'LinearMinifloatDenorm'}. Best is trial 13 with value: 0.84472.\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"," [3125/3125 01:43, Epoch 1/1]\n","Step\tTraining Loss\n","500\t0.693800\n","1000\t0.620900\n","1500\t0.466300\n","2000\t0.405600\n","2500\t0.369700\n","3000\t0.375900\n"," [3125/3125 01:16]\n","[I 2025-02-08 11:47:32,582] Trial 18 finished with value: 0.84328 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'LinearMinifloatDenorm', 'widths': 0, 'frac_widths': 1, 'stochastic': 0, 'bipolar': 0, 'data_in_exponent_width': 1, 'weight_exponent_width': 1, 'bias_exponent_width': 1, 'data_in_exponent_bias': 1, 'weight_exponent_bias': 1, 'bias_exponent_bias': 2, 'bert.encoder.layer.0.attention.self.key_type': 'LinearMinifloatDenorm', 'bert.encoder.layer.0.attention.self.value_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.intermediate.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.query_type': 'LinearMinifloatDenorm', 'bert.encoder.layer.1.attention.self.key_type': 'LinearMinifloatDenorm', 'bert.encoder.layer.1.attention.self.value_type': 'LinearMinifloatDenorm', 'bert.encoder.layer.1.attention.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.intermediate.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.output.dense_type': 'torch.nn.Linear', 'bert.pooler.dense_type': 'torch.nn.Linear', 'classifier_type': 'LinearMinifloatDenorm'}. Best is trial 13 with value: 0.84472.\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"," [3125/3125 01:49, Epoch 1/1]\n","Step\tTraining Loss\n","500\t0.693800\n","1000\t0.619700\n","1500\t0.472300\n","2000\t0.405200\n","2500\t0.370000\n","3000\t0.375300\n"," [3125/3125 01:14]\n","[I 2025-02-08 11:50:39,193] Trial 19 finished with value: 0.84472 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'LinearMinifloatDenorm', 'widths': 0, 'frac_widths': 1, 'stochastic': 0, 'bipolar': 0, 'data_in_exponent_width': 0, 'weight_exponent_width': 0, 'bias_exponent_width': 1, 'data_in_exponent_bias': 0, 'weight_exponent_bias': 0, 'bias_exponent_bias': 1, 'bert.encoder.layer.0.attention.self.key_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.self.value_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.intermediate.dense_type': 'LinearMinifloatDenorm', 'bert.encoder.layer.0.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.query_type': 'LinearMinifloatDenorm', 'bert.encoder.layer.1.attention.self.key_type': 'LinearMinifloatDenorm', 'bert.encoder.layer.1.attention.self.value_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.output.dense_type': 'LinearMinifloatDenorm', 'bert.encoder.layer.1.intermediate.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.output.dense_type': 'torch.nn.Linear', 'bert.pooler.dense_type': 'torch.nn.Linear', 'classifier_type': 'LinearMinifloatDenorm'}. Best is trial 13 with value: 0.84472.\n","[I 2025-02-08 11:50:39,310] A new study created in memory with name: bert-tiny-nas-LinearMinifloatIEEE\n","{'LinearMinifloatDenorm': [0.8438, 0.83252, 0.8426, 0.8408, 0.84124, 0.8414, 0.84128, 0.84388, 0.84268, 0.8192, 0.84316, 0.84212, 0.84212, 0.84472, 0.84448, 0.84472, 0.84472, 0.84472, 0.84328, 0.84472]}\n","\n","Running study for LinearMinifloatIEEE...\n","\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"," [3125/3125 02:21, Epoch 1/1]\n","Step\tTraining Loss\n","500\t0.693800\n","1000\t0.622100\n","1500\t0.468800\n","2000\t0.406100\n","2500\t0.370700\n","3000\t0.375600\n"," [3125/3125 01:40]\n","[I 2025-02-08 11:54:43,048] Trial 0 finished with value: 0.84264 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'LinearMinifloatIEEE', 'widths': 1, 'frac_widths': 1, 'stochastic': 1, 'bipolar': 0, 'data_in_exponent_width': 0, 'weight_exponent_width': 2, 'bias_exponent_width': 2, 'data_in_exponent_bias': 0, 'weight_exponent_bias': 1, 'bias_exponent_bias': 1, 'bert.encoder.layer.0.attention.self.key_type': 'LinearMinifloatIEEE', 'bert.encoder.layer.0.attention.self.value_type': 'LinearMinifloatIEEE', 'bert.encoder.layer.0.attention.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.intermediate.dense_type': 'LinearMinifloatIEEE', 'bert.encoder.layer.0.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.query_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.key_type': 'LinearMinifloatIEEE', 'bert.encoder.layer.1.attention.self.value_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.intermediate.dense_type': 'LinearMinifloatIEEE', 'bert.encoder.layer.1.output.dense_type': 'torch.nn.Linear', 'bert.pooler.dense_type': 'torch.nn.Linear', 'classifier_type': 'torch.nn.Linear'}. Best is trial 0 with value: 0.84264.\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"," [3125/3125 03:22, Epoch 1/1]\n","Step\tTraining Loss\n","500\t0.693800\n","1000\t0.621300\n","1500\t0.468500\n","2000\t0.407700\n","2500\t0.371900\n","3000\t0.377200\n"," [3125/3125 02:41]\n","[I 2025-02-08 12:00:49,624] Trial 1 finished with value: 0.8422 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.self.key_type': 'LinearMinifloatIEEE', 'widths': 2, 'frac_widths': 1, 'stochastic': 0, 'bipolar': 0, 'data_in_exponent_width': 2, 'weight_exponent_width': 0, 'bias_exponent_width': 0, 'data_in_exponent_bias': 0, 'weight_exponent_bias': 0, 'bias_exponent_bias': 0, 'bert.encoder.layer.0.attention.self.value_type': 'LinearMinifloatIEEE', 'bert.encoder.layer.0.attention.output.dense_type': 'LinearMinifloatIEEE', 'bert.encoder.layer.0.intermediate.dense_type': 'LinearMinifloatIEEE', 'bert.encoder.layer.0.output.dense_type': 'LinearMinifloatIEEE', 'bert.encoder.layer.1.attention.self.query_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.key_type': 'LinearMinifloatIEEE', 'bert.encoder.layer.1.attention.self.value_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.output.dense_type': 'LinearMinifloatIEEE', 'bert.encoder.layer.1.intermediate.dense_type': 'LinearMinifloatIEEE', 'bert.encoder.layer.1.output.dense_type': 'LinearMinifloatIEEE', 'bert.pooler.dense_type': 'LinearMinifloatIEEE', 'classifier_type': 'torch.nn.Linear'}. Best is trial 0 with value: 0.84264.\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"," [3125/3125 02:58, Epoch 1/1]\n","Step\tTraining Loss\n","500\t0.693800\n","1000\t0.622000\n","1500\t0.475800\n","2000\t0.408600\n","2500\t0.370200\n","3000\t0.376200\n"," [3125/3125 02:18]\n","[I 2025-02-08 12:06:09,319] Trial 2 finished with value: 0.84316 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'LinearMinifloatIEEE', 'widths': 0, 'frac_widths': 1, 'stochastic': 1, 'bipolar': 1, 'data_in_exponent_width': 2, 'weight_exponent_width': 0, 'bias_exponent_width': 0, 'data_in_exponent_bias': 0, 'weight_exponent_bias': 1, 'bias_exponent_bias': 1, 'bert.encoder.layer.0.attention.self.key_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.self.value_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.intermediate.dense_type': 'LinearMinifloatIEEE', 'bert.encoder.layer.0.output.dense_type': 'LinearMinifloatIEEE', 'bert.encoder.layer.1.attention.self.query_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.key_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.value_type': 'LinearMinifloatIEEE', 'bert.encoder.layer.1.attention.output.dense_type': 'LinearMinifloatIEEE', 'bert.encoder.layer.1.intermediate.dense_type': 'LinearMinifloatIEEE', 'bert.encoder.layer.1.output.dense_type': 'LinearMinifloatIEEE', 'bert.pooler.dense_type': 'torch.nn.Linear', 'classifier_type': 'LinearMinifloatIEEE'}. Best is trial 2 with value: 0.84316.\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"," [3125/3125 01:45, Epoch 1/1]\n","Step\tTraining Loss\n","500\t0.693900\n","1000\t0.686700\n","1500\t0.526400\n","2000\t0.438300\n","2500\t0.412200\n","3000\t0.408200\n"," [3125/3125 01:06]\n","[I 2025-02-08 12:09:04,307] Trial 3 finished with value: 0.8182 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.self.key_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.self.value_type': 'LinearMinifloatIEEE', 'widths': 2, 'frac_widths': 0, 'stochastic': 1, 'bipolar': 0, 'data_in_exponent_width': 2, 'weight_exponent_width': 1, 'bias_exponent_width': 0, 'data_in_exponent_bias': 1, 'weight_exponent_bias': 0, 'bias_exponent_bias': 1, 'bert.encoder.layer.0.attention.output.dense_type': 'LinearMinifloatIEEE', 'bert.encoder.layer.0.intermediate.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.query_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.key_type': 'LinearMinifloatIEEE', 'bert.encoder.layer.1.attention.self.value_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.intermediate.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.output.dense_type': 'torch.nn.Linear', 'bert.pooler.dense_type': 'torch.nn.Linear', 'classifier_type': 'torch.nn.Linear'}. Best is trial 2 with value: 0.84316.\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"," [3125/3125 02:20, Epoch 1/1]\n","Step\tTraining Loss\n","500\t0.692900\n","1000\t0.693300\n","1500\t0.693300\n","2000\t0.694000\n","2500\t0.694900\n","3000\t0.693900\n"," [3125/3125 01:42]\n","[I 2025-02-08 12:13:09,925] Trial 4 finished with value: 0.5 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.self.key_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.self.value_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.intermediate.dense_type': 'LinearMinifloatIEEE', 'widths': 1, 'frac_widths': 0, 'stochastic': 1, 'bipolar': 1, 'data_in_exponent_width': 2, 'weight_exponent_width': 0, 'bias_exponent_width': 0, 'data_in_exponent_bias': 2, 'weight_exponent_bias': 0, 'bias_exponent_bias': 2, 'bert.encoder.layer.0.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.query_type': 'LinearMinifloatIEEE', 'bert.encoder.layer.1.attention.self.key_type': 'LinearMinifloatIEEE', 'bert.encoder.layer.1.attention.self.value_type': 'LinearMinifloatIEEE', 'bert.encoder.layer.1.attention.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.intermediate.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.output.dense_type': 'torch.nn.Linear', 'bert.pooler.dense_type': 'LinearMinifloatIEEE', 'classifier_type': 'LinearMinifloatIEEE'}. Best is trial 2 with value: 0.84316.\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"," [3125/3125 02:12, Epoch 1/1]\n","Step\tTraining Loss\n","500\t0.693800\n","1000\t0.631000\n","1500\t0.488200\n","2000\t0.419000\n","2500\t0.379400\n","3000\t0.376900\n"," [3125/3125 01:33]\n","[I 2025-02-08 12:16:57,437] Trial 5 finished with value: 0.84032 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.self.key_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.self.value_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.intermediate.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.output.dense_type': 'LinearMinifloatIEEE', 'widths': 0, 'frac_widths': 0, 'stochastic': 1, 'bipolar': 1, 'data_in_exponent_width': 0, 'weight_exponent_width': 0, 'bias_exponent_width': 1, 'data_in_exponent_bias': 2, 'weight_exponent_bias': 1, 'bias_exponent_bias': 1, 'bert.encoder.layer.1.attention.self.query_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.key_type': 'LinearMinifloatIEEE', 'bert.encoder.layer.1.attention.self.value_type': 'LinearMinifloatIEEE', 'bert.encoder.layer.1.attention.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.intermediate.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.output.dense_type': 'LinearMinifloatIEEE', 'bert.pooler.dense_type': 'torch.nn.Linear', 'classifier_type': 'torch.nn.Linear'}. Best is trial 2 with value: 0.84316.\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"," [3125/3125 02:58, Epoch 1/1]\n","Step\tTraining Loss\n","500\t0.693800\n","1000\t0.621800\n","1500\t0.474100\n","2000\t0.409500\n","2500\t0.372800\n","3000\t0.376800\n"," [3125/3125 02:15]\n","[I 2025-02-08 12:22:13,988] Trial 6 finished with value: 0.84132 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.self.key_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.self.value_type': 'LinearMinifloatIEEE', 'widths': 0, 'frac_widths': 2, 'stochastic': 1, 'bipolar': 1, 'data_in_exponent_width': 2, 'weight_exponent_width': 0, 'bias_exponent_width': 1, 'data_in_exponent_bias': 0, 'weight_exponent_bias': 2, 'bias_exponent_bias': 0, 'bert.encoder.layer.0.attention.output.dense_type': 'LinearMinifloatIEEE', 'bert.encoder.layer.0.intermediate.dense_type': 'LinearMinifloatIEEE', 'bert.encoder.layer.0.output.dense_type': 'LinearMinifloatIEEE', 'bert.encoder.layer.1.attention.self.query_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.key_type': 'LinearMinifloatIEEE', 'bert.encoder.layer.1.attention.self.value_type': 'LinearMinifloatIEEE', 'bert.encoder.layer.1.attention.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.intermediate.dense_type': 'LinearMinifloatIEEE', 'bert.encoder.layer.1.output.dense_type': 'LinearMinifloatIEEE', 'bert.pooler.dense_type': 'torch.nn.Linear', 'classifier_type': 'torch.nn.Linear'}. Best is trial 2 with value: 0.84316.\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"," [3125/3125 03:33, Epoch 1/1]\n","Step\tTraining Loss\n","500\t0.693800\n","1000\t0.620900\n","1500\t0.471000\n","2000\t0.407500\n","2500\t0.371800\n","3000\t0.375800\n"," [3125/3125 02:50]\n","[I 2025-02-08 12:28:40,177] Trial 7 finished with value: 0.84216 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'LinearMinifloatIEEE', 'widths': 1, 'frac_widths': 1, 'stochastic': 0, 'bipolar': 0, 'data_in_exponent_width': 0, 'weight_exponent_width': 2, 'bias_exponent_width': 1, 'data_in_exponent_bias': 2, 'weight_exponent_bias': 2, 'bias_exponent_bias': 2, 'bert.encoder.layer.0.attention.self.key_type': 'LinearMinifloatIEEE', 'bert.encoder.layer.0.attention.self.value_type': 'LinearMinifloatIEEE', 'bert.encoder.layer.0.attention.output.dense_type': 'LinearMinifloatIEEE', 'bert.encoder.layer.0.intermediate.dense_type': 'LinearMinifloatIEEE', 'bert.encoder.layer.0.output.dense_type': 'LinearMinifloatIEEE', 'bert.encoder.layer.1.attention.self.query_type': 'LinearMinifloatIEEE', 'bert.encoder.layer.1.attention.self.key_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.value_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.intermediate.dense_type': 'LinearMinifloatIEEE', 'bert.encoder.layer.1.output.dense_type': 'LinearMinifloatIEEE', 'bert.pooler.dense_type': 'LinearMinifloatIEEE', 'classifier_type': 'LinearMinifloatIEEE'}. Best is trial 2 with value: 0.84316.\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"," [3125/3125 02:16, Epoch 1/1]\n","Step\tTraining Loss\n","500\t0.693900\n","1000\t0.620800\n","1500\t0.469900\n","2000\t0.407000\n","2500\t0.370900\n","3000\t0.376000\n"," [3125/3125 01:38]\n","[I 2025-02-08 12:32:37,849] Trial 8 finished with value: 0.84248 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'LinearMinifloatIEEE', 'widths': 2, 'frac_widths': 2, 'stochastic': 1, 'bipolar': 0, 'data_in_exponent_width': 0, 'weight_exponent_width': 1, 'bias_exponent_width': 1, 'data_in_exponent_bias': 0, 'weight_exponent_bias': 1, 'bias_exponent_bias': 2, 'bert.encoder.layer.0.attention.self.key_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.self.value_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.intermediate.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.query_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.key_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.value_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.output.dense_type': 'LinearMinifloatIEEE', 'bert.encoder.layer.1.intermediate.dense_type': 'LinearMinifloatIEEE', 'bert.encoder.layer.1.output.dense_type': 'LinearMinifloatIEEE', 'bert.pooler.dense_type': 'LinearMinifloatIEEE', 'classifier_type': 'torch.nn.Linear'}. Best is trial 2 with value: 0.84316.\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"," [3125/3125 01:58, Epoch 1/1]\n","Step\tTraining Loss\n","500\t0.693900\n","1000\t0.627900\n","1500\t0.501600\n","2000\t0.428800\n","2500\t0.382200\n","3000\t0.379600\n"," [3125/3125 01:16]\n","[I 2025-02-08 12:35:55,610] Trial 9 finished with value: 0.83932 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.self.key_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.self.value_type': 'LinearMinifloatIEEE', 'widths': 0, 'frac_widths': 0, 'stochastic': 0, 'bipolar': 1, 'data_in_exponent_width': 1, 'weight_exponent_width': 0, 'bias_exponent_width': 0, 'data_in_exponent_bias': 2, 'weight_exponent_bias': 2, 'bias_exponent_bias': 2, 'bert.encoder.layer.0.attention.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.intermediate.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.query_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.key_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.value_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.output.dense_type': 'LinearMinifloatIEEE', 'bert.encoder.layer.1.intermediate.dense_type': 'LinearMinifloatIEEE', 'bert.encoder.layer.1.output.dense_type': 'torch.nn.Linear', 'bert.pooler.dense_type': 'LinearMinifloatIEEE', 'classifier_type': 'torch.nn.Linear'}. Best is trial 2 with value: 0.84316.\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"," [3125/3125 03:06, Epoch 1/1]\n","Step\tTraining Loss\n","500\t0.693800\n","1000\t0.624200\n","1500\t0.468200\n","2000\t0.409300\n","2500\t0.373200\n","3000\t0.377800\n"," [3125/3125 02:26]\n","[I 2025-02-08 12:41:30,247] Trial 10 finished with value: 0.84156 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'LinearMinifloatIEEE', 'widths': 0, 'frac_widths': 2, 'stochastic': 0, 'bipolar': 1, 'data_in_exponent_width': 1, 'weight_exponent_width': 1, 'bias_exponent_width': 2, 'data_in_exponent_bias': 1, 'weight_exponent_bias': 1, 'bias_exponent_bias': 0, 'bert.encoder.layer.0.attention.self.key_type': 'LinearMinifloatIEEE', 'bert.encoder.layer.0.attention.self.value_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.intermediate.dense_type': 'LinearMinifloatIEEE', 'bert.encoder.layer.0.output.dense_type': 'LinearMinifloatIEEE', 'bert.encoder.layer.1.attention.self.query_type': 'LinearMinifloatIEEE', 'bert.encoder.layer.1.attention.self.key_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.value_type': 'LinearMinifloatIEEE', 'bert.encoder.layer.1.attention.output.dense_type': 'LinearMinifloatIEEE', 'bert.encoder.layer.1.intermediate.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.output.dense_type': 'LinearMinifloatIEEE', 'bert.pooler.dense_type': 'torch.nn.Linear', 'classifier_type': 'LinearMinifloatIEEE'}. Best is trial 2 with value: 0.84316.\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"," [3125/3125 02:27, Epoch 1/1]\n","Step\tTraining Loss\n","500\t0.693800\n","1000\t0.623400\n","1500\t0.466300\n","2000\t0.407500\n","2500\t0.371700\n","3000\t0.377200\n"," [3125/3125 01:50]\n","[I 2025-02-08 12:45:50,732] Trial 11 finished with value: 0.84244 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'LinearMinifloatIEEE', 'widths': 1, 'frac_widths': 1, 'stochastic': 1, 'bipolar': 0, 'data_in_exponent_width': 1, 'weight_exponent_width': 2, 'bias_exponent_width': 2, 'data_in_exponent_bias': 0, 'weight_exponent_bias': 1, 'bias_exponent_bias': 1, 'bert.encoder.layer.0.attention.self.key_type': 'LinearMinifloatIEEE', 'bert.encoder.layer.0.attention.self.value_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.intermediate.dense_type': 'LinearMinifloatIEEE', 'bert.encoder.layer.0.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.query_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.key_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.value_type': 'LinearMinifloatIEEE', 'bert.encoder.layer.1.attention.output.dense_type': 'LinearMinifloatIEEE', 'bert.encoder.layer.1.intermediate.dense_type': 'LinearMinifloatIEEE', 'bert.encoder.layer.1.output.dense_type': 'torch.nn.Linear', 'bert.pooler.dense_type': 'torch.nn.Linear', 'classifier_type': 'LinearMinifloatIEEE'}. Best is trial 2 with value: 0.84316.\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"," [3125/3125 02:39, Epoch 1/1]\n","Step\tTraining Loss\n","500\t0.694000\n","1000\t0.623600\n","1500\t0.463100\n","2000\t0.408200\n","2500\t0.373500\n","3000\t0.377200\n"," [3125/3125 02:00]\n","[I 2025-02-08 12:50:33,938] Trial 12 finished with value: 0.84232 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'LinearMinifloatIEEE', 'widths': 1, 'frac_widths': 1, 'stochastic': 1, 'bipolar': 1, 'data_in_exponent_width': 0, 'weight_exponent_width': 2, 'bias_exponent_width': 2, 'data_in_exponent_bias': 1, 'weight_exponent_bias': 1, 'bias_exponent_bias': 1, 'bert.encoder.layer.0.attention.self.key_type': 'LinearMinifloatIEEE', 'bert.encoder.layer.0.attention.self.value_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.intermediate.dense_type': 'LinearMinifloatIEEE', 'bert.encoder.layer.0.output.dense_type': 'LinearMinifloatIEEE', 'bert.encoder.layer.1.attention.self.query_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.key_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.value_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.output.dense_type': 'LinearMinifloatIEEE', 'bert.encoder.layer.1.intermediate.dense_type': 'LinearMinifloatIEEE', 'bert.encoder.layer.1.output.dense_type': 'torch.nn.Linear', 'bert.pooler.dense_type': 'torch.nn.Linear', 'classifier_type': 'LinearMinifloatIEEE'}. Best is trial 2 with value: 0.84316.\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"," [3125/3125 02:47, Epoch 1/1]\n","Step\tTraining Loss\n","500\t0.693800\n","1000\t0.623300\n","1500\t0.475700\n","2000\t0.407200\n","2500\t0.370300\n","3000\t0.375800\n"," [3125/3125 02:09]\n","[I 2025-02-08 12:55:33,039] Trial 13 finished with value: 0.8432 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'LinearMinifloatIEEE', 'widths': 0, 'frac_widths': 1, 'stochastic': 1, 'bipolar': 0, 'data_in_exponent_width': 1, 'weight_exponent_width': 2, 'bias_exponent_width': 2, 'data_in_exponent_bias': 0, 'weight_exponent_bias': 1, 'bias_exponent_bias': 1, 'bert.encoder.layer.0.attention.self.key_type': 'LinearMinifloatIEEE', 'bert.encoder.layer.0.attention.self.value_type': 'LinearMinifloatIEEE', 'bert.encoder.layer.0.attention.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.intermediate.dense_type': 'LinearMinifloatIEEE', 'bert.encoder.layer.0.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.query_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.key_type': 'LinearMinifloatIEEE', 'bert.encoder.layer.1.attention.self.value_type': 'LinearMinifloatIEEE', 'bert.encoder.layer.1.attention.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.intermediate.dense_type': 'LinearMinifloatIEEE', 'bert.encoder.layer.1.output.dense_type': 'torch.nn.Linear', 'bert.pooler.dense_type': 'torch.nn.Linear', 'classifier_type': 'LinearMinifloatIEEE'}. Best is trial 13 with value: 0.8432.\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"," [3125/3125 03:04, Epoch 1/1]\n","Step\tTraining Loss\n","500\t0.693800\n","1000\t0.623700\n","1500\t0.469100\n","2000\t0.408400\n","2500\t0.372500\n","3000\t0.377700\n"," [3125/3125 02:17]\n","[I 2025-02-08 13:00:56,892] Trial 14 finished with value: 0.84148 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'LinearMinifloatIEEE', 'widths': 0, 'frac_widths': 1, 'stochastic': 1, 'bipolar': 0, 'data_in_exponent_width': 1, 'weight_exponent_width': 1, 'bias_exponent_width': 1, 'data_in_exponent_bias': 0, 'weight_exponent_bias': 0, 'bias_exponent_bias': 1, 'bert.encoder.layer.0.attention.self.key_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.self.value_type': 'LinearMinifloatIEEE', 'bert.encoder.layer.0.attention.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.intermediate.dense_type': 'LinearMinifloatIEEE', 'bert.encoder.layer.0.output.dense_type': 'LinearMinifloatIEEE', 'bert.encoder.layer.1.attention.self.query_type': 'LinearMinifloatIEEE', 'bert.encoder.layer.1.attention.self.key_type': 'LinearMinifloatIEEE', 'bert.encoder.layer.1.attention.self.value_type': 'LinearMinifloatIEEE', 'bert.encoder.layer.1.attention.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.intermediate.dense_type': 'LinearMinifloatIEEE', 'bert.encoder.layer.1.output.dense_type': 'torch.nn.Linear', 'bert.pooler.dense_type': 'torch.nn.Linear', 'classifier_type': 'LinearMinifloatIEEE'}. Best is trial 13 with value: 0.8432.\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"," [3125/3125 02:50, Epoch 1/1]\n","Step\tTraining Loss\n","500\t0.693800\n","1000\t0.622800\n","1500\t0.468400\n","2000\t0.406800\n","2500\t0.370300\n","3000\t0.376700\n"," [3125/3125 02:11]\n","[I 2025-02-08 13:06:01,174] Trial 15 finished with value: 0.84336 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'LinearMinifloatIEEE', 'widths': 0, 'frac_widths': 2, 'stochastic': 1, 'bipolar': 1, 'data_in_exponent_width': 1, 'weight_exponent_width': 1, 'bias_exponent_width': 2, 'data_in_exponent_bias': 1, 'weight_exponent_bias': 2, 'bias_exponent_bias': 0, 'bert.encoder.layer.0.attention.self.key_type': 'LinearMinifloatIEEE', 'bert.encoder.layer.0.attention.self.value_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.intermediate.dense_type': 'LinearMinifloatIEEE', 'bert.encoder.layer.0.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.query_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.key_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.value_type': 'LinearMinifloatIEEE', 'bert.encoder.layer.1.attention.output.dense_type': 'LinearMinifloatIEEE', 'bert.encoder.layer.1.intermediate.dense_type': 'LinearMinifloatIEEE', 'bert.encoder.layer.1.output.dense_type': 'LinearMinifloatIEEE', 'bert.pooler.dense_type': 'torch.nn.Linear', 'classifier_type': 'LinearMinifloatIEEE'}. Best is trial 15 with value: 0.84336.\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"," [3125/3125 02:48, Epoch 1/1]\n","Step\tTraining Loss\n","500\t0.693800\n","1000\t0.622000\n","1500\t0.474400\n","2000\t0.406400\n","2500\t0.369200\n","3000\t0.376600\n"," [3125/3125 02:07]\n","[I 2025-02-08 13:11:00,080] Trial 16 finished with value: 0.84356 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'LinearMinifloatIEEE', 'widths': 0, 'frac_widths': 2, 'stochastic': 1, 'bipolar': 1, 'data_in_exponent_width': 1, 'weight_exponent_width': 1, 'bias_exponent_width': 2, 'data_in_exponent_bias': 1, 'weight_exponent_bias': 2, 'bias_exponent_bias': 0, 'bert.encoder.layer.0.attention.self.key_type': 'LinearMinifloatIEEE', 'bert.encoder.layer.0.attention.self.value_type': 'LinearMinifloatIEEE', 'bert.encoder.layer.0.attention.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.intermediate.dense_type': 'LinearMinifloatIEEE', 'bert.encoder.layer.0.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.query_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.key_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.value_type': 'LinearMinifloatIEEE', 'bert.encoder.layer.1.attention.output.dense_type': 'LinearMinifloatIEEE', 'bert.encoder.layer.1.intermediate.dense_type': 'LinearMinifloatIEEE', 'bert.encoder.layer.1.output.dense_type': 'torch.nn.Linear', 'bert.pooler.dense_type': 'torch.nn.Linear', 'classifier_type': 'LinearMinifloatIEEE'}. Best is trial 16 with value: 0.84356.\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"," [3125/3125 02:52, Epoch 1/1]\n","Step\tTraining Loss\n","500\t0.693800\n","1000\t0.622800\n","1500\t0.468400\n","2000\t0.406800\n","2500\t0.370300\n","3000\t0.376700\n"," [3125/3125 02:10]\n","[I 2025-02-08 13:16:05,191] Trial 17 finished with value: 0.84336 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'LinearMinifloatIEEE', 'widths': 0, 'frac_widths': 2, 'stochastic': 0, 'bipolar': 1, 'data_in_exponent_width': 1, 'weight_exponent_width': 1, 'bias_exponent_width': 2, 'data_in_exponent_bias': 1, 'weight_exponent_bias': 2, 'bias_exponent_bias': 0, 'bert.encoder.layer.0.attention.self.key_type': 'LinearMinifloatIEEE', 'bert.encoder.layer.0.attention.self.value_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.intermediate.dense_type': 'LinearMinifloatIEEE', 'bert.encoder.layer.0.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.query_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.key_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.value_type': 'LinearMinifloatIEEE', 'bert.encoder.layer.1.attention.output.dense_type': 'LinearMinifloatIEEE', 'bert.encoder.layer.1.intermediate.dense_type': 'LinearMinifloatIEEE', 'bert.encoder.layer.1.output.dense_type': 'LinearMinifloatIEEE', 'bert.pooler.dense_type': 'torch.nn.Linear', 'classifier_type': 'LinearMinifloatIEEE'}. Best is trial 16 with value: 0.84356.\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"," [3125/3125 02:59, Epoch 1/1]\n","Step\tTraining Loss\n","500\t0.693800\n","1000\t0.618400\n","1500\t0.474300\n","2000\t0.405300\n","2500\t0.369500\n","3000\t0.375900\n"," [3125/3125 02:20]\n","[I 2025-02-08 13:21:27,860] Trial 18 finished with value: 0.84344 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'LinearMinifloatIEEE', 'widths': 1, 'frac_widths': 2, 'stochastic': 1, 'bipolar': 1, 'data_in_exponent_width': 1, 'weight_exponent_width': 1, 'bias_exponent_width': 2, 'data_in_exponent_bias': 1, 'weight_exponent_bias': 2, 'bias_exponent_bias': 0, 'bert.encoder.layer.0.attention.self.key_type': 'LinearMinifloatIEEE', 'bert.encoder.layer.0.attention.self.value_type': 'LinearMinifloatIEEE', 'bert.encoder.layer.0.attention.output.dense_type': 'LinearMinifloatIEEE', 'bert.encoder.layer.0.intermediate.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.query_type': 'LinearMinifloatIEEE', 'bert.encoder.layer.1.attention.self.key_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.value_type': 'LinearMinifloatIEEE', 'bert.encoder.layer.1.attention.output.dense_type': 'LinearMinifloatIEEE', 'bert.encoder.layer.1.intermediate.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.output.dense_type': 'LinearMinifloatIEEE', 'bert.pooler.dense_type': 'torch.nn.Linear', 'classifier_type': 'LinearMinifloatIEEE'}. Best is trial 16 with value: 0.84356.\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"," [3125/3125 02:41, Epoch 1/1]\n","Step\tTraining Loss\n","500\t0.693800\n","1000\t0.619400\n","1500\t0.465800\n","2000\t0.404500\n","2500\t0.369400\n","3000\t0.374000\n"," [3125/3125 02:02]\n","[I 2025-02-08 13:26:13,436] Trial 19 finished with value: 0.84264 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'LinearMinifloatIEEE', 'widths': 1, 'frac_widths': 2, 'stochastic': 1, 'bipolar': 1, 'data_in_exponent_width': 1, 'weight_exponent_width': 1, 'bias_exponent_width': 2, 'data_in_exponent_bias': 1, 'weight_exponent_bias': 2, 'bias_exponent_bias': 0, 'bert.encoder.layer.0.attention.self.key_type': 'LinearMinifloatIEEE', 'bert.encoder.layer.0.attention.self.value_type': 'LinearMinifloatIEEE', 'bert.encoder.layer.0.attention.output.dense_type': 'LinearMinifloatIEEE', 'bert.encoder.layer.0.intermediate.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.query_type': 'LinearMinifloatIEEE', 'bert.encoder.layer.1.attention.self.key_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.value_type': 'LinearMinifloatIEEE', 'bert.encoder.layer.1.attention.output.dense_type': 'LinearMinifloatIEEE', 'bert.encoder.layer.1.intermediate.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.output.dense_type': 'torch.nn.Linear', 'bert.pooler.dense_type': 'torch.nn.Linear', 'classifier_type': 'LinearMinifloatIEEE'}. Best is trial 16 with value: 0.84356.\n","[I 2025-02-08 13:26:13,560] A new study created in memory with name: bert-tiny-nas-LinearLog\n","{'LinearMinifloatDenorm': [0.8438, 0.83252, 0.8426, 0.8408, 0.84124, 0.8414, 0.84128, 0.84388, 0.84268, 0.8192, 0.84316, 0.84212, 0.84212, 0.84472, 0.84448, 0.84472, 0.84472, 0.84472, 0.84328, 0.84472], 'LinearMinifloatIEEE': [0.84264, 0.8422, 0.84316, 0.8182, 0.5, 0.84032, 0.84132, 0.84216, 0.84248, 0.83932, 0.84156, 0.84244, 0.84232, 0.8432, 0.84148, 0.84336, 0.84356, 0.84336, 0.84344, 0.84264]}\n","\n","Running study for LinearLog...\n","\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"," [3125/3125 01:31, Epoch 1/1]\n","Step\tTraining Loss\n","500\t0.791400\n","1000\t0.744100\n","1500\t0.734000\n","2000\t0.731800\n","2500\t0.728300\n","3000\t0.726900\n"," [3125/3125 00:51]\n","[I 2025-02-08 13:28:39,405] Trial 0 finished with value: 0.50124 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'LinearLog', 'widths': 0, 'frac_widths': 0, 'stochastic': 1, 'bipolar': 0, 'data_in_exponent_width': 1, 'weight_exponent_width': 2, 'bias_exponent_width': 1, 'data_in_exponent_bias': 1, 'weight_exponent_bias': 2, 'bias_exponent_bias': 2, 'bert.encoder.layer.0.attention.self.key_type': 'LinearLog', 'bert.encoder.layer.0.attention.self.value_type': 'LinearLog', 'bert.encoder.layer.0.attention.output.dense_type': 'LinearLog', 'bert.encoder.layer.0.intermediate.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.output.dense_type': 'LinearLog', 'bert.encoder.layer.1.attention.self.query_type': 'LinearLog', 'bert.encoder.layer.1.attention.self.key_type': 'LinearLog', 'bert.encoder.layer.1.attention.self.value_type': 'LinearLog', 'bert.encoder.layer.1.attention.output.dense_type': 'LinearLog', 'bert.encoder.layer.1.intermediate.dense_type': 'LinearLog', 'bert.encoder.layer.1.output.dense_type': 'torch.nn.Linear', 'bert.pooler.dense_type': 'torch.nn.Linear', 'classifier_type': 'LinearLog'}. Best is trial 0 with value: 0.50124.\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"," [3125/3125 01:18, Epoch 1/1]\n","Step\tTraining Loss\n","500\t0.693900\n","1000\t0.693600\n","1500\t0.694000\n","2000\t0.694400\n","2500\t0.693900\n","3000\t0.694700\n"," [3125/3125 00:40]\n","[I 2025-02-08 13:30:40,778] Trial 1 finished with value: 0.5 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.self.key_type': 'LinearLog', 'widths': 1, 'frac_widths': 0, 'stochastic': 1, 'bipolar': 1, 'data_in_exponent_width': 0, 'weight_exponent_width': 0, 'bias_exponent_width': 0, 'data_in_exponent_bias': 1, 'weight_exponent_bias': 0, 'bias_exponent_bias': 2, 'bert.encoder.layer.0.attention.self.value_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.output.dense_type': 'LinearLog', 'bert.encoder.layer.0.intermediate.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.query_type': 'LinearLog', 'bert.encoder.layer.1.attention.self.key_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.value_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.output.dense_type': 'LinearLog', 'bert.encoder.layer.1.intermediate.dense_type': 'LinearLog', 'bert.encoder.layer.1.output.dense_type': 'LinearLog', 'bert.pooler.dense_type': 'torch.nn.Linear', 'classifier_type': 'torch.nn.Linear'}. Best is trial 0 with value: 0.50124.\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"," [3125/3125 01:20, Epoch 1/1]\n","Step\tTraining Loss\n","500\t0.696600\n","1000\t0.696900\n","1500\t0.695000\n","2000\t0.698000\n","2500\t0.694700\n","3000\t0.694700\n"," [3125/3125 00:40]\n","[I 2025-02-08 13:32:43,586] Trial 2 finished with value: 0.49984 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'LinearLog', 'widths': 0, 'frac_widths': 1, 'stochastic': 0, 'bipolar': 1, 'data_in_exponent_width': 2, 'weight_exponent_width': 1, 'bias_exponent_width': 0, 'data_in_exponent_bias': 0, 'weight_exponent_bias': 2, 'bias_exponent_bias': 2, 'bert.encoder.layer.0.attention.self.key_type': 'LinearLog', 'bert.encoder.layer.0.attention.self.value_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.intermediate.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.query_type': 'LinearLog', 'bert.encoder.layer.1.attention.self.key_type': 'LinearLog', 'bert.encoder.layer.1.attention.self.value_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.intermediate.dense_type': 'LinearLog', 'bert.encoder.layer.1.output.dense_type': 'torch.nn.Linear', 'bert.pooler.dense_type': 'LinearLog', 'classifier_type': 'torch.nn.Linear'}. Best is trial 0 with value: 0.50124.\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"," [3125/3125 01:21, Epoch 1/1]\n","Step\tTraining Loss\n","500\t1.104400\n","1000\t0.961700\n","1500\t0.888300\n","2000\t0.795900\n","2500\t0.717600\n","3000\t0.678200\n"," [3125/3125 00:44]\n","[I 2025-02-08 13:34:51,915] Trial 3 finished with value: 0.6574 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.self.key_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.self.value_type': 'LinearLog', 'widths': 0, 'frac_widths': 2, 'stochastic': 0, 'bipolar': 0, 'data_in_exponent_width': 0, 'weight_exponent_width': 0, 'bias_exponent_width': 1, 'data_in_exponent_bias': 0, 'weight_exponent_bias': 2, 'bias_exponent_bias': 2, 'bert.encoder.layer.0.attention.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.intermediate.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.query_type': 'LinearLog', 'bert.encoder.layer.1.attention.self.key_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.value_type': 'LinearLog', 'bert.encoder.layer.1.attention.output.dense_type': 'LinearLog', 'bert.encoder.layer.1.intermediate.dense_type': 'LinearLog', 'bert.encoder.layer.1.output.dense_type': 'LinearLog', 'bert.pooler.dense_type': 'torch.nn.Linear', 'classifier_type': 'LinearLog'}. Best is trial 3 with value: 0.6574.\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"," [3125/3125 01:23, Epoch 1/1]\n","Step\tTraining Loss\n","500\t0.698800\n","1000\t0.695700\n","1500\t0.697500\n","2000\t0.699000\n","2500\t0.695300\n","3000\t0.697200\n"," [3125/3125 00:44]\n","[I 2025-02-08 13:37:02,903] Trial 4 finished with value: 0.50472 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'LinearLog', 'widths': 0, 'frac_widths': 1, 'stochastic': 0, 'bipolar': 0, 'data_in_exponent_width': 2, 'weight_exponent_width': 0, 'bias_exponent_width': 2, 'data_in_exponent_bias': 1, 'weight_exponent_bias': 2, 'bias_exponent_bias': 2, 'bert.encoder.layer.0.attention.self.key_type': 'LinearLog', 'bert.encoder.layer.0.attention.self.value_type': 'LinearLog', 'bert.encoder.layer.0.attention.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.intermediate.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.query_type': 'LinearLog', 'bert.encoder.layer.1.attention.self.key_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.value_type': 'LinearLog', 'bert.encoder.layer.1.attention.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.intermediate.dense_type': 'LinearLog', 'bert.encoder.layer.1.output.dense_type': 'LinearLog', 'bert.pooler.dense_type': 'LinearLog', 'classifier_type': 'torch.nn.Linear'}. Best is trial 3 with value: 0.6574.\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"," [3125/3125 01:21, Epoch 1/1]\n","Step\tTraining Loss\n","500\t0.694200\n","1000\t0.693300\n","1500\t0.694000\n","2000\t0.694300\n","2500\t0.693900\n","3000\t0.694600\n"," [3125/3125 00:40]\n","[I 2025-02-08 13:39:07,621] Trial 5 finished with value: 0.5 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.self.key_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.self.value_type': 'LinearLog', 'widths': 2, 'frac_widths': 2, 'stochastic': 1, 'bipolar': 1, 'data_in_exponent_width': 1, 'weight_exponent_width': 2, 'bias_exponent_width': 2, 'data_in_exponent_bias': 0, 'weight_exponent_bias': 0, 'bias_exponent_bias': 2, 'bert.encoder.layer.0.attention.output.dense_type': 'LinearLog', 'bert.encoder.layer.0.intermediate.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.query_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.key_type': 'LinearLog', 'bert.encoder.layer.1.attention.self.value_type': 'LinearLog', 'bert.encoder.layer.1.attention.output.dense_type': 'LinearLog', 'bert.encoder.layer.1.intermediate.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.output.dense_type': 'LinearLog', 'bert.pooler.dense_type': 'torch.nn.Linear', 'classifier_type': 'torch.nn.Linear'}. Best is trial 3 with value: 0.6574.\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"," [3125/3125 01:15, Epoch 1/1]\n","Step\tTraining Loss\n","500\t0.718100\n","1000\t0.721500\n","1500\t0.721500\n","2000\t0.704500\n","2500\t0.693100\n","3000\t0.693100\n"," [3125/3125 00:37]\n","[I 2025-02-08 13:41:02,914] Trial 6 finished with value: 0.5 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.self.key_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.self.value_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.intermediate.dense_type': 'LinearLog', 'widths': 2, 'frac_widths': 2, 'stochastic': 0, 'bipolar': 0, 'data_in_exponent_width': 1, 'weight_exponent_width': 1, 'bias_exponent_width': 2, 'data_in_exponent_bias': 1, 'weight_exponent_bias': 0, 'bias_exponent_bias': 2, 'bert.encoder.layer.0.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.query_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.key_type': 'LinearLog', 'bert.encoder.layer.1.attention.self.value_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.intermediate.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.output.dense_type': 'LinearLog', 'bert.pooler.dense_type': 'torch.nn.Linear', 'classifier_type': 'LinearLog'}. Best is trial 3 with value: 0.6574.\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"," [3125/3125 01:22, Epoch 1/1]\n","Step\tTraining Loss\n","500\t0.694200\n","1000\t0.693300\n","1500\t0.694000\n","2000\t0.694300\n","2500\t0.693900\n","3000\t0.694600\n"," [3125/3125 00:43]\n","[I 2025-02-08 13:43:11,302] Trial 7 finished with value: 0.5 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'LinearLog', 'widths': 0, 'frac_widths': 1, 'stochastic': 0, 'bipolar': 0, 'data_in_exponent_width': 1, 'weight_exponent_width': 2, 'bias_exponent_width': 1, 'data_in_exponent_bias': 1, 'weight_exponent_bias': 0, 'bias_exponent_bias': 1, 'bert.encoder.layer.0.attention.self.key_type': 'LinearLog', 'bert.encoder.layer.0.attention.self.value_type': 'LinearLog', 'bert.encoder.layer.0.attention.output.dense_type': 'LinearLog', 'bert.encoder.layer.0.intermediate.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.output.dense_type': 'LinearLog', 'bert.encoder.layer.1.attention.self.query_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.key_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.value_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.output.dense_type': 'LinearLog', 'bert.encoder.layer.1.intermediate.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.output.dense_type': 'LinearLog', 'bert.pooler.dense_type': 'torch.nn.Linear', 'classifier_type': 'torch.nn.Linear'}. Best is trial 3 with value: 0.6574.\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"," [3125/3125 01:30, Epoch 1/1]\n","Step\tTraining Loss\n","500\t0.695600\n","1000\t0.695200\n","1500\t0.692900\n","2000\t0.696200\n","2500\t0.694900\n","3000\t0.694700\n"," [3125/3125 00:51]\n","[I 2025-02-08 13:45:35,040] Trial 8 finished with value: 0.49604 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'LinearLog', 'widths': 0, 'frac_widths': 2, 'stochastic': 0, 'bipolar': 0, 'data_in_exponent_width': 1, 'weight_exponent_width': 0, 'bias_exponent_width': 0, 'data_in_exponent_bias': 0, 'weight_exponent_bias': 1, 'bias_exponent_bias': 2, 'bert.encoder.layer.0.attention.self.key_type': 'LinearLog', 'bert.encoder.layer.0.attention.self.value_type': 'LinearLog', 'bert.encoder.layer.0.attention.output.dense_type': 'LinearLog', 'bert.encoder.layer.0.intermediate.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.output.dense_type': 'LinearLog', 'bert.encoder.layer.1.attention.self.query_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.key_type': 'LinearLog', 'bert.encoder.layer.1.attention.self.value_type': 'LinearLog', 'bert.encoder.layer.1.attention.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.intermediate.dense_type': 'LinearLog', 'bert.encoder.layer.1.output.dense_type': 'LinearLog', 'bert.pooler.dense_type': 'LinearLog', 'classifier_type': 'torch.nn.Linear'}. Best is trial 3 with value: 0.6574.\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"," [3125/3125 01:19, Epoch 1/1]\n","Step\tTraining Loss\n","500\t0.694500\n","1000\t0.693800\n","1500\t0.693600\n","2000\t0.693700\n","2500\t0.693700\n","3000\t0.694100\n"," [3125/3125 00:42]\n","[I 2025-02-08 13:47:39,910] Trial 9 finished with value: 0.50052 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'LinearLog', 'widths': 0, 'frac_widths': 1, 'stochastic': 1, 'bipolar': 1, 'data_in_exponent_width': 1, 'weight_exponent_width': 2, 'bias_exponent_width': 0, 'data_in_exponent_bias': 2, 'weight_exponent_bias': 0, 'bias_exponent_bias': 1, 'bert.encoder.layer.0.attention.self.key_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.self.value_type': 'LinearLog', 'bert.encoder.layer.0.attention.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.intermediate.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.output.dense_type': 'LinearLog', 'bert.encoder.layer.1.attention.self.query_type': 'LinearLog', 'bert.encoder.layer.1.attention.self.key_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.value_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.output.dense_type': 'LinearLog', 'bert.encoder.layer.1.intermediate.dense_type': 'LinearLog', 'bert.encoder.layer.1.output.dense_type': 'torch.nn.Linear', 'bert.pooler.dense_type': 'torch.nn.Linear', 'classifier_type': 'torch.nn.Linear'}. Best is trial 3 with value: 0.6574.\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"," [3125/3125 01:20, Epoch 1/1]\n","Step\tTraining Loss\n","500\t0.778400\n","1000\t0.706300\n","1500\t0.710500\n","2000\t0.701100\n","2500\t0.693100\n","3000\t0.693100\n"," [3125/3125 00:40]\n","[I 2025-02-08 13:49:42,952] Trial 10 finished with value: 0.5 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.self.key_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.self.value_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.intermediate.dense_type': 'LinearLog', 'widths': 1, 'frac_widths': 2, 'stochastic': 0, 'bipolar': 0, 'data_in_exponent_width': 0, 'weight_exponent_width': 0, 'bias_exponent_width': 1, 'data_in_exponent_bias': 2, 'weight_exponent_bias': 1, 'bias_exponent_bias': 0, 'bert.encoder.layer.0.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.query_type': 'LinearLog', 'bert.encoder.layer.1.attention.self.key_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.value_type': 'LinearLog', 'bert.encoder.layer.1.attention.output.dense_type': 'LinearLog', 'bert.encoder.layer.1.intermediate.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.output.dense_type': 'torch.nn.Linear', 'bert.pooler.dense_type': 'LinearLog', 'classifier_type': 'LinearLog'}. Best is trial 3 with value: 0.6574.\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"," [3125/3125 01:25, Epoch 1/1]\n","Step\tTraining Loss\n","500\t0.708100\n","1000\t0.703600\n","1500\t0.701300\n","2000\t0.698600\n","2500\t0.699000\n","3000\t0.695200\n"," [3125/3125 00:45]\n","[I 2025-02-08 13:51:56,644] Trial 11 finished with value: 0.4986 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.self.key_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.self.value_type': 'LinearLog', 'widths': 1, 'frac_widths': 1, 'stochastic': 0, 'bipolar': 0, 'data_in_exponent_width': 2, 'weight_exponent_width': 0, 'bias_exponent_width': 2, 'data_in_exponent_bias': 0, 'weight_exponent_bias': 2, 'bias_exponent_bias': 1, 'bert.encoder.layer.0.attention.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.intermediate.dense_type': 'LinearLog', 'bert.encoder.layer.0.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.query_type': 'LinearLog', 'bert.encoder.layer.1.attention.self.key_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.value_type': 'LinearLog', 'bert.encoder.layer.1.attention.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.intermediate.dense_type': 'LinearLog', 'bert.encoder.layer.1.output.dense_type': 'LinearLog', 'bert.pooler.dense_type': 'LinearLog', 'classifier_type': 'LinearLog'}. Best is trial 3 with value: 0.6574.\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"," [3125/3125 01:26, Epoch 1/1]\n","Step\tTraining Loss\n","500\t0.903400\n","1000\t0.864200\n","1500\t0.840400\n","2000\t0.823600\n","2500\t0.822900\n","3000\t0.832500\n"," [3125/3125 00:49]\n","[I 2025-02-08 13:54:15,406] Trial 12 finished with value: 0.50432 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'LinearLog', 'widths': 0, 'frac_widths': 0, 'stochastic': 0, 'bipolar': 0, 'data_in_exponent_width': 2, 'weight_exponent_width': 1, 'bias_exponent_width': 2, 'data_in_exponent_bias': 2, 'weight_exponent_bias': 2, 'bias_exponent_bias': 0, 'bert.encoder.layer.0.attention.self.key_type': 'LinearLog', 'bert.encoder.layer.0.attention.self.value_type': 'LinearLog', 'bert.encoder.layer.0.attention.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.intermediate.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.query_type': 'LinearLog', 'bert.encoder.layer.1.attention.self.key_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.value_type': 'LinearLog', 'bert.encoder.layer.1.attention.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.intermediate.dense_type': 'LinearLog', 'bert.encoder.layer.1.output.dense_type': 'LinearLog', 'bert.pooler.dense_type': 'LinearLog', 'classifier_type': 'LinearLog'}. Best is trial 3 with value: 0.6574.\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"," [3125/3125 01:22, Epoch 1/1]\n","Step\tTraining Loss\n","500\t0.740600\n","1000\t0.706300\n","1500\t0.697800\n","2000\t0.700400\n","2500\t0.701700\n","3000\t0.701200\n"," [3125/3125 00:42]\n","[I 2025-02-08 13:56:22,404] Trial 13 finished with value: 0.5 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.self.key_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.self.value_type': 'LinearLog', 'widths': 1, 'frac_widths': 1, 'stochastic': 0, 'bipolar': 0, 'data_in_exponent_width': 0, 'weight_exponent_width': 0, 'bias_exponent_width': 1, 'data_in_exponent_bias': 0, 'weight_exponent_bias': 1, 'bias_exponent_bias': 1, 'bert.encoder.layer.0.attention.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.intermediate.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.query_type': 'LinearLog', 'bert.encoder.layer.1.attention.self.key_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.value_type': 'LinearLog', 'bert.encoder.layer.1.attention.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.intermediate.dense_type': 'LinearLog', 'bert.encoder.layer.1.output.dense_type': 'LinearLog', 'bert.pooler.dense_type': 'LinearLog', 'classifier_type': 'LinearLog'}. Best is trial 3 with value: 0.6574.\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"," [3125/3125 01:27, Epoch 1/1]\n","Step\tTraining Loss\n","500\t0.694200\n","1000\t0.693300\n","1500\t0.694000\n","2000\t0.694300\n","2500\t0.693600\n","3000\t0.694700\n"," [3125/3125 00:47]\n","[I 2025-02-08 13:58:39,363] Trial 14 finished with value: 0.50244 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'LinearLog', 'widths': 0, 'frac_widths': 2, 'stochastic': 0, 'bipolar': 0, 'data_in_exponent_width': 0, 'weight_exponent_width': 0, 'bias_exponent_width': 1, 'data_in_exponent_bias': 1, 'weight_exponent_bias': 2, 'bias_exponent_bias': 2, 'bert.encoder.layer.0.attention.self.key_type': 'LinearLog', 'bert.encoder.layer.0.attention.self.value_type': 'LinearLog', 'bert.encoder.layer.0.attention.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.intermediate.dense_type': 'LinearLog', 'bert.encoder.layer.0.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.query_type': 'LinearLog', 'bert.encoder.layer.1.attention.self.key_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.value_type': 'LinearLog', 'bert.encoder.layer.1.attention.output.dense_type': 'LinearLog', 'bert.encoder.layer.1.intermediate.dense_type': 'LinearLog', 'bert.encoder.layer.1.output.dense_type': 'LinearLog', 'bert.pooler.dense_type': 'torch.nn.Linear', 'classifier_type': 'torch.nn.Linear'}. Best is trial 3 with value: 0.6574.\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"," [3125/3125 01:23, Epoch 1/1]\n","Step\tTraining Loss\n","500\t0.693600\n","1000\t0.694300\n","1500\t0.693600\n","2000\t0.696500\n","2500\t0.695800\n","3000\t0.693600\n"," [3125/3125 00:42]\n","[I 2025-02-08 14:00:47,463] Trial 15 finished with value: 0.4958 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.self.key_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.self.value_type': 'LinearLog', 'widths': 1, 'frac_widths': 1, 'stochastic': 0, 'bipolar': 0, 'data_in_exponent_width': 2, 'weight_exponent_width': 1, 'bias_exponent_width': 2, 'data_in_exponent_bias': 0, 'weight_exponent_bias': 2, 'bias_exponent_bias': 1, 'bert.encoder.layer.0.attention.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.intermediate.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.query_type': 'LinearLog', 'bert.encoder.layer.1.attention.self.key_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.value_type': 'LinearLog', 'bert.encoder.layer.1.attention.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.intermediate.dense_type': 'LinearLog', 'bert.encoder.layer.1.output.dense_type': 'LinearLog', 'bert.pooler.dense_type': 'LinearLog', 'classifier_type': 'LinearLog'}. Best is trial 3 with value: 0.6574.\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"," [3125/3125 01:24, Epoch 1/1]\n","Step\tTraining Loss\n","500\t0.694500\n","1000\t0.696400\n","1500\t0.696400\n","2000\t0.697400\n","2500\t0.693900\n","3000\t0.696600\n"," [3125/3125 00:44]\n","[I 2025-02-08 14:03:01,250] Trial 16 finished with value: 0.50392 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'LinearLog', 'widths': 0, 'frac_widths': 2, 'stochastic': 0, 'bipolar': 0, 'data_in_exponent_width': 2, 'weight_exponent_width': 0, 'bias_exponent_width': 1, 'data_in_exponent_bias': 2, 'weight_exponent_bias': 1, 'bias_exponent_bias': 2, 'bert.encoder.layer.0.attention.self.key_type': 'LinearLog', 'bert.encoder.layer.0.attention.self.value_type': 'LinearLog', 'bert.encoder.layer.0.attention.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.intermediate.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.query_type': 'LinearLog', 'bert.encoder.layer.1.attention.self.key_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.value_type': 'LinearLog', 'bert.encoder.layer.1.attention.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.intermediate.dense_type': 'LinearLog', 'bert.encoder.layer.1.output.dense_type': 'LinearLog', 'bert.pooler.dense_type': 'LinearLog', 'classifier_type': 'torch.nn.Linear'}. Best is trial 3 with value: 0.6574.\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"," [3125/3125 01:22, Epoch 1/1]\n","Step\tTraining Loss\n","500\t0.837900\n","1000\t0.783300\n","1500\t0.684200\n","2000\t0.552800\n","2500\t0.473400\n","3000\t0.452000\n"," [3125/3125 00:42]\n","[I 2025-02-08 14:05:08,057] Trial 17 finished with value: 0.81572 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.self.key_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.self.value_type': 'LinearLog', 'widths': 2, 'frac_widths': 0, 'stochastic': 1, 'bipolar': 1, 'data_in_exponent_width': 0, 'weight_exponent_width': 1, 'bias_exponent_width': 2, 'data_in_exponent_bias': 1, 'weight_exponent_bias': 2, 'bias_exponent_bias': 0, 'bert.encoder.layer.0.attention.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.intermediate.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.query_type': 'LinearLog', 'bert.encoder.layer.1.attention.self.key_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.value_type': 'LinearLog', 'bert.encoder.layer.1.attention.output.dense_type': 'LinearLog', 'bert.encoder.layer.1.intermediate.dense_type': 'LinearLog', 'bert.encoder.layer.1.output.dense_type': 'LinearLog', 'bert.pooler.dense_type': 'torch.nn.Linear', 'classifier_type': 'LinearLog'}. Best is trial 17 with value: 0.81572.\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"," [3125/3125 01:17, Epoch 1/1]\n","Step\tTraining Loss\n","500\t0.724000\n","1000\t0.697500\n","1500\t0.695600\n","2000\t0.693100\n","2500\t0.693100\n","3000\t0.693100\n"," [3125/3125 00:38]\n","[I 2025-02-08 14:07:06,856] Trial 18 finished with value: 0.5 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.self.key_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.self.value_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.intermediate.dense_type': 'LinearLog', 'widths': 2, 'frac_widths': 0, 'stochastic': 1, 'bipolar': 1, 'data_in_exponent_width': 0, 'weight_exponent_width': 1, 'bias_exponent_width': 1, 'data_in_exponent_bias': 1, 'weight_exponent_bias': 1, 'bias_exponent_bias': 0, 'bert.encoder.layer.0.output.dense_type': 'LinearLog', 'bert.encoder.layer.1.attention.self.query_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.key_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.value_type': 'LinearLog', 'bert.encoder.layer.1.attention.output.dense_type': 'LinearLog', 'bert.encoder.layer.1.intermediate.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.output.dense_type': 'torch.nn.Linear', 'bert.pooler.dense_type': 'torch.nn.Linear', 'classifier_type': 'LinearLog'}. Best is trial 17 with value: 0.81572.\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"," [3125/3125 01:23, Epoch 1/1]\n","Step\tTraining Loss\n","500\t1.107100\n","1000\t0.956500\n","1500\t0.908700\n","2000\t0.832500\n","2500\t0.771200\n","3000\t0.731400\n"," [3125/3125 00:42]\n","[I 2025-02-08 14:09:15,417] Trial 19 finished with value: 0.5558 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.self.key_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.self.value_type': 'LinearLog', 'widths': 2, 'frac_widths': 0, 'stochastic': 1, 'bipolar': 1, 'data_in_exponent_width': 0, 'weight_exponent_width': 1, 'bias_exponent_width': 2, 'data_in_exponent_bias': 0, 'weight_exponent_bias': 2, 'bias_exponent_bias': 0, 'bert.encoder.layer.0.attention.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.intermediate.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.query_type': 'LinearLog', 'bert.encoder.layer.1.attention.self.key_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.value_type': 'LinearLog', 'bert.encoder.layer.1.attention.output.dense_type': 'LinearLog', 'bert.encoder.layer.1.intermediate.dense_type': 'LinearLog', 'bert.encoder.layer.1.output.dense_type': 'LinearLog', 'bert.pooler.dense_type': 'torch.nn.Linear', 'classifier_type': 'LinearLog'}. Best is trial 17 with value: 0.81572.\n","[I 2025-02-08 14:09:15,536] A new study created in memory with name: bert-tiny-nas-LinearBlockFP\n","{'LinearMinifloatDenorm': [0.8438, 0.83252, 0.8426, 0.8408, 0.84124, 0.8414, 0.84128, 0.84388, 0.84268, 0.8192, 0.84316, 0.84212, 0.84212, 0.84472, 0.84448, 0.84472, 0.84472, 0.84472, 0.84328, 0.84472], 'LinearMinifloatIEEE': [0.84264, 0.8422, 0.84316, 0.8182, 0.5, 0.84032, 0.84132, 0.84216, 0.84248, 0.83932, 0.84156, 0.84244, 0.84232, 0.8432, 0.84148, 0.84336, 0.84356, 0.84336, 0.84344, 0.84264], 'LinearLog': [0.50124, 0.5, 0.49984, 0.6574, 0.50472, 0.5, 0.5, 0.5, 0.49604, 0.50052, 0.5, 0.4986, 0.50432, 0.5, 0.50244, 0.4958, 0.50392, 0.81572, 0.5, 0.5558]}\n","\n","Running study for LinearBlockFP...\n","\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"," [3125/3125 04:50, Epoch 1/1]\n","Step\tTraining Loss\n","500\t0.693700\n","1000\t0.622700\n","1500\t0.469300\n","2000\t0.408500\n","2500\t0.372400\n","3000\t0.377500\n"," [3125/3125 04:08]\n","[I 2025-02-08 14:18:17,164] Trial 0 finished with value: 0.84208 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.self.key_type': 'LinearBlockFP', 'widths': 1, 'frac_widths': 1, 'stochastic': 1, 'bipolar': 0, 'data_in_exponent_width': 1, 'weight_exponent_width': 2, 'bias_exponent_width': 2, 'data_in_exponent_bias': 1, 'weight_exponent_bias': 0, 'bias_exponent_bias': 0, 'weight_block_size': 0, 'data_in_block_size': 2, 'bias_block_size': 0, 'bert.encoder.layer.0.attention.self.value_type': 'LinearBlockFP', 'bert.encoder.layer.0.attention.output.dense_type': 'LinearBlockFP', 'bert.encoder.layer.0.intermediate.dense_type': 'LinearBlockFP', 'bert.encoder.layer.0.output.dense_type': 'LinearBlockFP', 'bert.encoder.layer.1.attention.self.query_type': 'LinearBlockFP', 'bert.encoder.layer.1.attention.self.key_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.value_type': 'LinearBlockFP', 'bert.encoder.layer.1.attention.output.dense_type': 'LinearBlockFP', 'bert.encoder.layer.1.intermediate.dense_type': 'LinearBlockFP', 'bert.encoder.layer.1.output.dense_type': 'LinearBlockFP', 'bert.pooler.dense_type': 'LinearBlockFP', 'classifier_type': 'LinearBlockFP'}. Best is trial 0 with value: 0.84208.\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"," [3125/3125 02:24, Epoch 1/1]\n","Step\tTraining Loss\n","500\t0.693800\n","1000\t0.622900\n","1500\t0.468400\n","2000\t0.407600\n","2500\t0.371000\n","3000\t0.376000\n"," [3125/3125 01:46]\n","[I 2025-02-08 14:22:30,669] Trial 1 finished with value: 0.84268 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.self.key_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.self.value_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.intermediate.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.output.dense_type': 'LinearBlockFP', 'widths': 0, 'frac_widths': 1, 'stochastic': 1, 'bipolar': 1, 'data_in_exponent_width': 0, 'weight_exponent_width': 0, 'bias_exponent_width': 1, 'data_in_exponent_bias': 1, 'weight_exponent_bias': 1, 'bias_exponent_bias': 2, 'weight_block_size': 0, 'data_in_block_size': 1, 'bias_block_size': 1, 'bert.encoder.layer.1.attention.self.query_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.key_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.value_type': 'LinearBlockFP', 'bert.encoder.layer.1.attention.output.dense_type': 'LinearBlockFP', 'bert.encoder.layer.1.intermediate.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.output.dense_type': 'LinearBlockFP', 'bert.pooler.dense_type': 'torch.nn.Linear', 'classifier_type': 'torch.nn.Linear'}. Best is trial 1 with value: 0.84268.\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"," [3125/3125 03:43, Epoch 1/1]\n","Step\tTraining Loss\n","500\t0.693800\n","1000\t0.621200\n","1500\t0.466500\n","2000\t0.406200\n","2500\t0.371300\n","3000\t0.374600\n"," [3125/3125 03:00]\n","[I 2025-02-08 14:29:17,581] Trial 2 finished with value: 0.8428 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'LinearBlockFP', 'widths': 1, 'frac_widths': 0, 'stochastic': 0, 'bipolar': 0, 'data_in_exponent_width': 2, 'weight_exponent_width': 1, 'bias_exponent_width': 1, 'data_in_exponent_bias': 2, 'weight_exponent_bias': 2, 'bias_exponent_bias': 2, 'weight_block_size': 0, 'data_in_block_size': 0, 'bias_block_size': 2, 'bert.encoder.layer.0.attention.self.key_type': 'LinearBlockFP', 'bert.encoder.layer.0.attention.self.value_type': 'LinearBlockFP', 'bert.encoder.layer.0.attention.output.dense_type': 'LinearBlockFP', 'bert.encoder.layer.0.intermediate.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.output.dense_type': 'LinearBlockFP', 'bert.encoder.layer.1.attention.self.query_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.key_type': 'LinearBlockFP', 'bert.encoder.layer.1.attention.self.value_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.intermediate.dense_type': 'LinearBlockFP', 'bert.encoder.layer.1.output.dense_type': 'LinearBlockFP', 'bert.pooler.dense_type': 'torch.nn.Linear', 'classifier_type': 'LinearBlockFP'}. Best is trial 2 with value: 0.8428.\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"," [3125/3125 01:58, Epoch 1/1]\n","Step\tTraining Loss\n","500\t0.693900\n","1000\t0.622700\n","1500\t0.466400\n","2000\t0.407700\n","2500\t0.372400\n","3000\t0.376400\n"," [3125/3125 01:19]\n","[I 2025-02-08 14:32:38,156] Trial 3 finished with value: 0.84168 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.self.key_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.self.value_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.intermediate.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.query_type': 'LinearBlockFP', 'widths': 0, 'frac_widths': 2, 'stochastic': 1, 'bipolar': 0, 'data_in_exponent_width': 1, 'weight_exponent_width': 1, 'bias_exponent_width': 1, 'data_in_exponent_bias': 2, 'weight_exponent_bias': 0, 'bias_exponent_bias': 0, 'weight_block_size': 0, 'data_in_block_size': 1, 'bias_block_size': 0, 'bert.encoder.layer.1.attention.self.key_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.value_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.intermediate.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.output.dense_type': 'LinearBlockFP', 'bert.pooler.dense_type': 'torch.nn.Linear', 'classifier_type': 'LinearBlockFP'}. Best is trial 2 with value: 0.8428.\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"," [3125/3125 03:16, Epoch 1/1]\n","Step\tTraining Loss\n","500\t0.693900\n","1000\t0.619500\n","1500\t0.468000\n","2000\t0.407800\n","2500\t0.373300\n","3000\t0.375100\n"," [3125/3125 02:38]\n","[I 2025-02-08 14:38:35,753] Trial 4 finished with value: 0.84136 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.self.key_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.self.value_type': 'LinearBlockFP', 'widths': 2, 'frac_widths': 2, 'stochastic': 1, 'bipolar': 0, 'data_in_exponent_width': 2, 'weight_exponent_width': 1, 'bias_exponent_width': 1, 'data_in_exponent_bias': 2, 'weight_exponent_bias': 2, 'bias_exponent_bias': 1, 'weight_block_size': 0, 'data_in_block_size': 2, 'bias_block_size': 0, 'bert.encoder.layer.0.attention.output.dense_type': 'LinearBlockFP', 'bert.encoder.layer.0.intermediate.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.output.dense_type': 'LinearBlockFP', 'bert.encoder.layer.1.attention.self.query_type': 'LinearBlockFP', 'bert.encoder.layer.1.attention.self.key_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.value_type': 'LinearBlockFP', 'bert.encoder.layer.1.attention.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.intermediate.dense_type': 'LinearBlockFP', 'bert.encoder.layer.1.output.dense_type': 'torch.nn.Linear', 'bert.pooler.dense_type': 'torch.nn.Linear', 'classifier_type': 'LinearBlockFP'}. Best is trial 2 with value: 0.8428.\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"," [3125/3125 02:31, Epoch 1/1]\n","Step\tTraining Loss\n","500\t0.693900\n","1000\t0.625300\n","1500\t0.463500\n","2000\t0.407500\n","2500\t0.375900\n","3000\t0.376500\n"," [3125/3125 01:52]\n","[I 2025-02-08 14:43:02,613] Trial 5 finished with value: 0.84184 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'LinearBlockFP', 'widths': 2, 'frac_widths': 0, 'stochastic': 0, 'bipolar': 1, 'data_in_exponent_width': 2, 'weight_exponent_width': 1, 'bias_exponent_width': 2, 'data_in_exponent_bias': 1, 'weight_exponent_bias': 2, 'bias_exponent_bias': 1, 'weight_block_size': 2, 'data_in_block_size': 1, 'bias_block_size': 0, 'bert.encoder.layer.0.attention.self.key_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.self.value_type': 'LinearBlockFP', 'bert.encoder.layer.0.attention.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.intermediate.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.output.dense_type': 'LinearBlockFP', 'bert.encoder.layer.1.attention.self.query_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.key_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.value_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.output.dense_type': 'LinearBlockFP', 'bert.encoder.layer.1.intermediate.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.output.dense_type': 'torch.nn.Linear', 'bert.pooler.dense_type': 'torch.nn.Linear', 'classifier_type': 'LinearBlockFP'}. Best is trial 2 with value: 0.8428.\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"," [3125/3125 03:14, Epoch 1/1]\n","Step\tTraining Loss\n","500\t0.693800\n","1000\t0.620100\n","1500\t0.472500\n","2000\t0.406800\n","2500\t0.370400\n","3000\t0.375200\n"," [3125/3125 02:37]\n","[I 2025-02-08 14:48:56,656] Trial 6 finished with value: 0.84196 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'LinearBlockFP', 'widths': 2, 'frac_widths': 1, 'stochastic': 0, 'bipolar': 0, 'data_in_exponent_width': 0, 'weight_exponent_width': 0, 'bias_exponent_width': 2, 'data_in_exponent_bias': 1, 'weight_exponent_bias': 1, 'bias_exponent_bias': 1, 'weight_block_size': 1, 'data_in_block_size': 0, 'bias_block_size': 1, 'bert.encoder.layer.0.attention.self.key_type': 'LinearBlockFP', 'bert.encoder.layer.0.attention.self.value_type': 'LinearBlockFP', 'bert.encoder.layer.0.attention.output.dense_type': 'LinearBlockFP', 'bert.encoder.layer.0.intermediate.dense_type': 'LinearBlockFP', 'bert.encoder.layer.0.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.query_type': 'LinearBlockFP', 'bert.encoder.layer.1.attention.self.key_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.value_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.output.dense_type': 'LinearBlockFP', 'bert.encoder.layer.1.intermediate.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.output.dense_type': 'torch.nn.Linear', 'bert.pooler.dense_type': 'LinearBlockFP', 'classifier_type': 'torch.nn.Linear'}. Best is trial 2 with value: 0.8428.\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"," [3125/3125 03:04, Epoch 1/1]\n","Step\tTraining Loss\n","500\t0.693800\n","1000\t0.622300\n","1500\t0.470000\n","2000\t0.408100\n","2500\t0.371300\n","3000\t0.375000\n"," [3125/3125 02:24]\n","[I 2025-02-08 14:54:28,846] Trial 7 finished with value: 0.84248 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.self.key_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.self.value_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.intermediate.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.output.dense_type': 'LinearBlockFP', 'widths': 2, 'frac_widths': 1, 'stochastic': 0, 'bipolar': 0, 'data_in_exponent_width': 2, 'weight_exponent_width': 1, 'bias_exponent_width': 0, 'data_in_exponent_bias': 0, 'weight_exponent_bias': 1, 'bias_exponent_bias': 0, 'weight_block_size': 1, 'data_in_block_size': 2, 'bias_block_size': 2, 'bert.encoder.layer.1.attention.self.query_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.key_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.value_type': 'LinearBlockFP', 'bert.encoder.layer.1.attention.output.dense_type': 'LinearBlockFP', 'bert.encoder.layer.1.intermediate.dense_type': 'LinearBlockFP', 'bert.encoder.layer.1.output.dense_type': 'LinearBlockFP', 'bert.pooler.dense_type': 'LinearBlockFP', 'classifier_type': 'torch.nn.Linear'}. Best is trial 2 with value: 0.8428.\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"," [3125/3125 03:39, Epoch 1/1]\n","Step\tTraining Loss\n","500\t0.693700\n","1000\t0.621200\n","1500\t0.467500\n","2000\t0.405200\n","2500\t0.370300\n","3000\t0.374500\n"," [3125/3125 03:03]\n","[I 2025-02-08 15:01:13,899] Trial 8 finished with value: 0.84296 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'LinearBlockFP', 'widths': 2, 'frac_widths': 1, 'stochastic': 1, 'bipolar': 1, 'data_in_exponent_width': 1, 'weight_exponent_width': 2, 'bias_exponent_width': 2, 'data_in_exponent_bias': 2, 'weight_exponent_bias': 0, 'bias_exponent_bias': 0, 'weight_block_size': 2, 'data_in_block_size': 0, 'bias_block_size': 1, 'bert.encoder.layer.0.attention.self.key_type': 'LinearBlockFP', 'bert.encoder.layer.0.attention.self.value_type': 'LinearBlockFP', 'bert.encoder.layer.0.attention.output.dense_type': 'LinearBlockFP', 'bert.encoder.layer.0.intermediate.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.output.dense_type': 'LinearBlockFP', 'bert.encoder.layer.1.attention.self.query_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.key_type': 'LinearBlockFP', 'bert.encoder.layer.1.attention.self.value_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.output.dense_type': 'LinearBlockFP', 'bert.encoder.layer.1.intermediate.dense_type': 'LinearBlockFP', 'bert.encoder.layer.1.output.dense_type': 'torch.nn.Linear', 'bert.pooler.dense_type': 'LinearBlockFP', 'classifier_type': 'torch.nn.Linear'}. Best is trial 8 with value: 0.84296.\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"," [3125/3125 02:42, Epoch 1/1]\n","Step\tTraining Loss\n","500\t0.693900\n","1000\t0.620400\n","1500\t0.470300\n","2000\t0.407200\n","2500\t0.370900\n","3000\t0.376100\n"," [3125/3125 02:02]\n","[I 2025-02-08 15:06:00,502] Trial 9 finished with value: 0.84344 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.self.key_type': 'LinearBlockFP', 'widths': 1, 'frac_widths': 2, 'stochastic': 1, 'bipolar': 0, 'data_in_exponent_width': 0, 'weight_exponent_width': 1, 'bias_exponent_width': 0, 'data_in_exponent_bias': 2, 'weight_exponent_bias': 2, 'bias_exponent_bias': 2, 'weight_block_size': 1, 'data_in_block_size': 2, 'bias_block_size': 2, 'bert.encoder.layer.0.attention.self.value_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.intermediate.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.query_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.key_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.value_type': 'LinearBlockFP', 'bert.encoder.layer.1.attention.output.dense_type': 'LinearBlockFP', 'bert.encoder.layer.1.intermediate.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.output.dense_type': 'LinearBlockFP', 'bert.pooler.dense_type': 'LinearBlockFP', 'classifier_type': 'torch.nn.Linear'}. Best is trial 9 with value: 0.84344.\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"," [3125/3125 02:58, Epoch 1/1]\n","Step\tTraining Loss\n","500\t0.693900\n","1000\t0.622700\n","1500\t0.469000\n","2000\t0.407800\n","2500\t0.370800\n","3000\t0.376700\n"," [3125/3125 02:20]\n","[I 2025-02-08 15:11:21,860] Trial 10 finished with value: 0.8426 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.self.key_type': 'LinearBlockFP', 'widths': 1, 'frac_widths': 2, 'stochastic': 1, 'bipolar': 1, 'data_in_exponent_width': 0, 'weight_exponent_width': 0, 'bias_exponent_width': 0, 'data_in_exponent_bias': 0, 'weight_exponent_bias': 2, 'bias_exponent_bias': 2, 'weight_block_size': 1, 'data_in_block_size': 2, 'bias_block_size': 2, 'bert.encoder.layer.0.attention.self.value_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.intermediate.dense_type': 'LinearBlockFP', 'bert.encoder.layer.0.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.query_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.key_type': 'LinearBlockFP', 'bert.encoder.layer.1.attention.self.value_type': 'LinearBlockFP', 'bert.encoder.layer.1.attention.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.intermediate.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.output.dense_type': 'LinearBlockFP', 'bert.pooler.dense_type': 'LinearBlockFP', 'classifier_type': 'torch.nn.Linear'}. Best is trial 9 with value: 0.84344.\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"," [3125/3125 02:59, Epoch 1/1]\n","Step\tTraining Loss\n","500\t0.693900\n","1000\t0.620700\n","1500\t0.470100\n","2000\t0.406200\n","2500\t0.369600\n","3000\t0.376500\n"," [3125/3125 02:21]\n","[I 2025-02-08 15:16:45,918] Trial 11 finished with value: 0.84312 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'LinearBlockFP', 'widths': 1, 'frac_widths': 2, 'stochastic': 1, 'bipolar': 1, 'data_in_exponent_width': 1, 'weight_exponent_width': 2, 'bias_exponent_width': 0, 'data_in_exponent_bias': 2, 'weight_exponent_bias': 0, 'bias_exponent_bias': 1, 'weight_block_size': 2, 'data_in_block_size': 0, 'bias_block_size': 1, 'bert.encoder.layer.0.attention.self.key_type': 'LinearBlockFP', 'bert.encoder.layer.0.attention.self.value_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.output.dense_type': 'LinearBlockFP', 'bert.encoder.layer.0.intermediate.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.query_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.key_type': 'LinearBlockFP', 'bert.encoder.layer.1.attention.self.value_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.output.dense_type': 'LinearBlockFP', 'bert.encoder.layer.1.intermediate.dense_type': 'LinearBlockFP', 'bert.encoder.layer.1.output.dense_type': 'torch.nn.Linear', 'bert.pooler.dense_type': 'LinearBlockFP', 'classifier_type': 'torch.nn.Linear'}. Best is trial 9 with value: 0.84344.\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"," [3125/3125 03:00, Epoch 1/1]\n","Step\tTraining Loss\n","500\t0.693800\n","1000\t0.620900\n","1500\t0.470700\n","2000\t0.406500\n","2500\t0.369000\n","3000\t0.376500\n"," [3125/3125 02:22]\n","[I 2025-02-08 15:22:10,663] Trial 12 finished with value: 0.84384 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'LinearBlockFP', 'widths': 1, 'frac_widths': 2, 'stochastic': 1, 'bipolar': 1, 'data_in_exponent_width': 0, 'weight_exponent_width': 2, 'bias_exponent_width': 0, 'data_in_exponent_bias': 2, 'weight_exponent_bias': 0, 'bias_exponent_bias': 2, 'weight_block_size': 2, 'data_in_block_size': 0, 'bias_block_size': 2, 'bert.encoder.layer.0.attention.self.key_type': 'LinearBlockFP', 'bert.encoder.layer.0.attention.self.value_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.output.dense_type': 'LinearBlockFP', 'bert.encoder.layer.0.intermediate.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.query_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.key_type': 'LinearBlockFP', 'bert.encoder.layer.1.attention.self.value_type': 'LinearBlockFP', 'bert.encoder.layer.1.attention.output.dense_type': 'LinearBlockFP', 'bert.encoder.layer.1.intermediate.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.output.dense_type': 'torch.nn.Linear', 'bert.pooler.dense_type': 'LinearBlockFP', 'classifier_type': 'torch.nn.Linear'}. Best is trial 12 with value: 0.84384.\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"," [3125/3125 02:44, Epoch 1/1]\n","Step\tTraining Loss\n","500\t0.693800\n","1000\t0.622000\n","1500\t0.473600\n","2000\t0.408600\n","2500\t0.370800\n","3000\t0.377200\n"," [3125/3125 02:07]\n","[I 2025-02-08 15:27:05,067] Trial 13 finished with value: 0.84212 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'LinearBlockFP', 'widths': 0, 'frac_widths': 2, 'stochastic': 1, 'bipolar': 1, 'data_in_exponent_width': 0, 'weight_exponent_width': 2, 'bias_exponent_width': 0, 'data_in_exponent_bias': 2, 'weight_exponent_bias': 1, 'bias_exponent_bias': 2, 'weight_block_size': 2, 'data_in_block_size': 1, 'bias_block_size': 2, 'bert.encoder.layer.0.attention.self.key_type': 'LinearBlockFP', 'bert.encoder.layer.0.attention.self.value_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.intermediate.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.query_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.key_type': 'LinearBlockFP', 'bert.encoder.layer.1.attention.self.value_type': 'LinearBlockFP', 'bert.encoder.layer.1.attention.output.dense_type': 'LinearBlockFP', 'bert.encoder.layer.1.intermediate.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.output.dense_type': 'torch.nn.Linear', 'bert.pooler.dense_type': 'LinearBlockFP', 'classifier_type': 'torch.nn.Linear'}. Best is trial 12 with value: 0.84384.\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"," [3125/3125 03:23, Epoch 1/1]\n","Step\tTraining Loss\n","500\t0.693800\n","1000\t0.622900\n","1500\t0.470300\n","2000\t0.408400\n","2500\t0.371100\n","3000\t0.377000\n"," [3125/3125 02:45]\n","[I 2025-02-08 15:33:16,123] Trial 14 finished with value: 0.84208 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.self.key_type': 'LinearBlockFP', 'widths': 1, 'frac_widths': 2, 'stochastic': 1, 'bipolar': 0, 'data_in_exponent_width': 0, 'weight_exponent_width': 2, 'bias_exponent_width': 0, 'data_in_exponent_bias': 2, 'weight_exponent_bias': 1, 'bias_exponent_bias': 2, 'weight_block_size': 1, 'data_in_block_size': 1, 'bias_block_size': 2, 'bert.encoder.layer.0.attention.self.value_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.output.dense_type': 'LinearBlockFP', 'bert.encoder.layer.0.intermediate.dense_type': 'LinearBlockFP', 'bert.encoder.layer.0.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.query_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.key_type': 'LinearBlockFP', 'bert.encoder.layer.1.attention.self.value_type': 'LinearBlockFP', 'bert.encoder.layer.1.attention.output.dense_type': 'LinearBlockFP', 'bert.encoder.layer.1.intermediate.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.output.dense_type': 'LinearBlockFP', 'bert.pooler.dense_type': 'LinearBlockFP', 'classifier_type': 'torch.nn.Linear'}. Best is trial 12 with value: 0.84384.\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"," [3125/3125 02:53, Epoch 1/1]\n","Step\tTraining Loss\n","500\t0.693800\n","1000\t0.622000\n","1500\t0.473600\n","2000\t0.408600\n","2500\t0.370800\n","3000\t0.377200\n"," [3125/3125 02:14]\n","[I 2025-02-08 15:38:26,405] Trial 15 finished with value: 0.84212 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'LinearBlockFP', 'widths': 1, 'frac_widths': 2, 'stochastic': 1, 'bipolar': 1, 'data_in_exponent_width': 0, 'weight_exponent_width': 1, 'bias_exponent_width': 0, 'data_in_exponent_bias': 1, 'weight_exponent_bias': 0, 'bias_exponent_bias': 2, 'weight_block_size': 1, 'data_in_block_size': 2, 'bias_block_size': 2, 'bert.encoder.layer.0.attention.self.key_type': 'LinearBlockFP', 'bert.encoder.layer.0.attention.self.value_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.intermediate.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.query_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.key_type': 'LinearBlockFP', 'bert.encoder.layer.1.attention.self.value_type': 'LinearBlockFP', 'bert.encoder.layer.1.attention.output.dense_type': 'LinearBlockFP', 'bert.encoder.layer.1.intermediate.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.output.dense_type': 'torch.nn.Linear', 'bert.pooler.dense_type': 'LinearBlockFP', 'classifier_type': 'torch.nn.Linear'}. Best is trial 12 with value: 0.84384.\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"," [3125/3125 02:30, Epoch 1/1]\n","Step\tTraining Loss\n","500\t0.693900\n","1000\t0.620400\n","1500\t0.470300\n","2000\t0.407200\n","2500\t0.370900\n","3000\t0.376100\n"," [3125/3125 01:53]\n","[I 2025-02-08 15:42:53,429] Trial 16 finished with value: 0.84344 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.self.key_type': 'LinearBlockFP', 'widths': 0, 'frac_widths': 2, 'stochastic': 1, 'bipolar': 0, 'data_in_exponent_width': 0, 'weight_exponent_width': 2, 'bias_exponent_width': 0, 'data_in_exponent_bias': 2, 'weight_exponent_bias': 2, 'bias_exponent_bias': 2, 'weight_block_size': 2, 'data_in_block_size': 0, 'bias_block_size': 2, 'bert.encoder.layer.0.attention.self.value_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.intermediate.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.query_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.key_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.value_type': 'LinearBlockFP', 'bert.encoder.layer.1.attention.output.dense_type': 'LinearBlockFP', 'bert.encoder.layer.1.intermediate.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.output.dense_type': 'LinearBlockFP', 'bert.pooler.dense_type': 'LinearBlockFP', 'classifier_type': 'torch.nn.Linear'}. Best is trial 12 with value: 0.84384.\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"," [3125/3125 03:00, Epoch 1/1]\n","Step\tTraining Loss\n","500\t0.693800\n","1000\t0.620000\n","1500\t0.470800\n","2000\t0.405700\n","2500\t0.368100\n","3000\t0.376700\n"," [3125/3125 02:24]\n","[I 2025-02-08 15:48:21,322] Trial 17 finished with value: 0.8448 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'LinearBlockFP', 'widths': 1, 'frac_widths': 0, 'stochastic': 0, 'bipolar': 1, 'data_in_exponent_width': 1, 'weight_exponent_width': 0, 'bias_exponent_width': 1, 'data_in_exponent_bias': 0, 'weight_exponent_bias': 1, 'bias_exponent_bias': 1, 'weight_block_size': 1, 'data_in_block_size': 1, 'bias_block_size': 1, 'bert.encoder.layer.0.attention.self.key_type': 'LinearBlockFP', 'bert.encoder.layer.0.attention.self.value_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.output.dense_type': 'LinearBlockFP', 'bert.encoder.layer.0.intermediate.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.query_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.key_type': 'LinearBlockFP', 'bert.encoder.layer.1.attention.self.value_type': 'LinearBlockFP', 'bert.encoder.layer.1.attention.output.dense_type': 'LinearBlockFP', 'bert.encoder.layer.1.intermediate.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.output.dense_type': 'torch.nn.Linear', 'bert.pooler.dense_type': 'LinearBlockFP', 'classifier_type': 'torch.nn.Linear'}. Best is trial 17 with value: 0.8448.\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"," [3125/3125 03:18, Epoch 1/1]\n","Step\tTraining Loss\n","500\t0.693800\n","1000\t0.621500\n","1500\t0.476900\n","2000\t0.408400\n","2500\t0.369800\n","3000\t0.376300\n"," [2466/3125 02:06 < 00:33, 19.44 it/s]\n"," [3125/3125 02:41]\n","[I 2025-02-08 15:54:23,526] Trial 18 finished with value: 0.84192 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'LinearBlockFP', 'widths': 1, 'frac_widths': 0, 'stochastic': 0, 'bipolar': 1, 'data_in_exponent_width': 1, 'weight_exponent_width': 0, 'bias_exponent_width': 1, 'data_in_exponent_bias': 0, 'weight_exponent_bias': 0, 'bias_exponent_bias': 1, 'weight_block_size': 1, 'data_in_block_size': 1, 'bias_block_size': 1, 'bert.encoder.layer.0.attention.self.key_type': 'LinearBlockFP', 'bert.encoder.layer.0.attention.self.value_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.output.dense_type': 'LinearBlockFP', 'bert.encoder.layer.0.intermediate.dense_type': 'LinearBlockFP', 'bert.encoder.layer.0.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.query_type': 'LinearBlockFP', 'bert.encoder.layer.1.attention.self.key_type': 'LinearBlockFP', 'bert.encoder.layer.1.attention.self.value_type': 'LinearBlockFP', 'bert.encoder.layer.1.attention.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.intermediate.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.output.dense_type': 'torch.nn.Linear', 'bert.pooler.dense_type': 'LinearBlockFP', 'classifier_type': 'torch.nn.Linear'}. Best is trial 17 with value: 0.8448.\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"," [3125/3125 02:59, Epoch 1/1]\n","Step\tTraining Loss\n","500\t0.693800\n","1000\t0.619900\n","1500\t0.470700\n","2000\t0.405800\n","2500\t0.368100\n","3000\t0.376600\n"," [3125/3125 02:20]\n","[I 2025-02-08 15:59:46,367] Trial 19 finished with value: 0.84468 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'LinearBlockFP', 'widths': 0, 'frac_widths': 0, 'stochastic': 0, 'bipolar': 1, 'data_in_exponent_width': 1, 'weight_exponent_width': 0, 'bias_exponent_width': 1, 'data_in_exponent_bias': 0, 'weight_exponent_bias': 1, 'bias_exponent_bias': 1, 'weight_block_size': 2, 'data_in_block_size': 0, 'bias_block_size': 1, 'bert.encoder.layer.0.attention.self.key_type': 'LinearBlockFP', 'bert.encoder.layer.0.attention.self.value_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.output.dense_type': 'LinearBlockFP', 'bert.encoder.layer.0.intermediate.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.query_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.key_type': 'LinearBlockFP', 'bert.encoder.layer.1.attention.self.value_type': 'LinearBlockFP', 'bert.encoder.layer.1.attention.output.dense_type': 'LinearBlockFP', 'bert.encoder.layer.1.intermediate.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.output.dense_type': 'torch.nn.Linear', 'bert.pooler.dense_type': 'LinearBlockFP', 'classifier_type': 'torch.nn.Linear'}. Best is trial 17 with value: 0.8448.\n","[I 2025-02-08 15:59:46,521] A new study created in memory with name: bert-tiny-nas-LinearBlockLog\n","{'LinearMinifloatDenorm': [0.8438, 0.83252, 0.8426, 0.8408, 0.84124, 0.8414, 0.84128, 0.84388, 0.84268, 0.8192, 0.84316, 0.84212, 0.84212, 0.84472, 0.84448, 0.84472, 0.84472, 0.84472, 0.84328, 0.84472], 'LinearMinifloatIEEE': [0.84264, 0.8422, 0.84316, 0.8182, 0.5, 0.84032, 0.84132, 0.84216, 0.84248, 0.83932, 0.84156, 0.84244, 0.84232, 0.8432, 0.84148, 0.84336, 0.84356, 0.84336, 0.84344, 0.84264], 'LinearLog': [0.50124, 0.5, 0.49984, 0.6574, 0.50472, 0.5, 0.5, 0.5, 0.49604, 0.50052, 0.5, 0.4986, 0.50432, 0.5, 0.50244, 0.4958, 0.50392, 0.81572, 0.5, 0.5558], 'LinearBlockFP': [0.84208, 0.84268, 0.8428, 0.84168, 0.84136, 0.84184, 0.84196, 0.84248, 0.84296, 0.84344, 0.8426, 0.84312, 0.84384, 0.84212, 0.84208, 0.84212, 0.84344, 0.8448, 0.84192, 0.84468]}\n","\n","Running study for LinearBlockLog...\n","\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"," [3125/3125 02:54, Epoch 1/1]\n","Step\tTraining Loss\n","500\t0.694200\n","1000\t0.624500\n","1500\t0.473400\n","2000\t0.408800\n","2500\t0.374200\n","3000\t0.377600\n"," [3125/3125 02:16]\n","[I 2025-02-08 16:04:59,550] Trial 0 finished with value: 0.8412 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.self.key_type': 'LinearBlockLog', 'widths': 1, 'frac_widths': 1, 'stochastic': 0, 'bipolar': 0, 'data_in_exponent_bias_width': 0, 'bias_exponent_bias_width': 2, 'weight_exponent_bias_width': 0, 'data_in_exponent_width': 0, 'weight_exponent_width': 2, 'bias_exponent_width': 0, 'data_in_exponent_bias': 2, 'weight_exponent_bias': 0, 'bias_exponent_bias': 2, 'weight_block_size': 2, 'data_in_block_size': 0, 'bias_block_size': 2, 'bert.encoder.layer.0.attention.self.value_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.output.dense_type': 'LinearBlockLog', 'bert.encoder.layer.0.intermediate.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.output.dense_type': 'LinearBlockLog', 'bert.encoder.layer.1.attention.self.query_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.key_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.value_type': 'LinearBlockLog', 'bert.encoder.layer.1.attention.output.dense_type': 'LinearBlockLog', 'bert.encoder.layer.1.intermediate.dense_type': 'LinearBlockLog', 'bert.encoder.layer.1.output.dense_type': 'torch.nn.Linear', 'bert.pooler.dense_type': 'torch.nn.Linear', 'classifier_type': 'LinearBlockLog'}. Best is trial 0 with value: 0.8412.\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"," [3125/3125 03:55, Epoch 1/1]\n","Step\tTraining Loss\n","500\t0.694100\n","1000\t0.627900\n","1500\t0.488600\n","2000\t0.412400\n","2500\t0.371200\n","3000\t0.378800\n"," [3125/3125 03:16]\n","[I 2025-02-08 16:12:13,874] Trial 1 finished with value: 0.84084 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'LinearBlockLog', 'widths': 2, 'frac_widths': 2, 'stochastic': 0, 'bipolar': 1, 'data_in_exponent_bias_width': 2, 'bias_exponent_bias_width': 1, 'weight_exponent_bias_width': 0, 'data_in_exponent_width': 0, 'weight_exponent_width': 2, 'bias_exponent_width': 2, 'data_in_exponent_bias': 0, 'weight_exponent_bias': 0, 'bias_exponent_bias': 0, 'weight_block_size': 2, 'data_in_block_size': 0, 'bias_block_size': 2, 'bert.encoder.layer.0.attention.self.key_type': 'LinearBlockLog', 'bert.encoder.layer.0.attention.self.value_type': 'LinearBlockLog', 'bert.encoder.layer.0.attention.output.dense_type': 'LinearBlockLog', 'bert.encoder.layer.0.intermediate.dense_type': 'LinearBlockLog', 'bert.encoder.layer.0.output.dense_type': 'LinearBlockLog', 'bert.encoder.layer.1.attention.self.query_type': 'LinearBlockLog', 'bert.encoder.layer.1.attention.self.key_type': 'LinearBlockLog', 'bert.encoder.layer.1.attention.self.value_type': 'LinearBlockLog', 'bert.encoder.layer.1.attention.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.intermediate.dense_type': 'LinearBlockLog', 'bert.encoder.layer.1.output.dense_type': 'torch.nn.Linear', 'bert.pooler.dense_type': 'torch.nn.Linear', 'classifier_type': 'LinearBlockLog'}. Best is trial 0 with value: 0.8412.\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"," [3125/3125 02:01, Epoch 1/1]\n","Step\tTraining Loss\n","500\t0.694000\n","1000\t0.622900\n","1500\t0.478500\n","2000\t0.409700\n","2500\t0.372200\n","3000\t0.375900\n"," [3125/3125 01:28]\n","[I 2025-02-08 16:15:45,550] Trial 2 finished with value: 0.8418 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.self.key_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.self.value_type': 'LinearBlockLog', 'widths': 2, 'frac_widths': 2, 'stochastic': 0, 'bipolar': 0, 'data_in_exponent_bias_width': 2, 'bias_exponent_bias_width': 1, 'weight_exponent_bias_width': 0, 'data_in_exponent_width': 2, 'weight_exponent_width': 0, 'bias_exponent_width': 2, 'data_in_exponent_bias': 1, 'weight_exponent_bias': 0, 'bias_exponent_bias': 1, 'weight_block_size': 0, 'data_in_block_size': 0, 'bias_block_size': 2, 'bert.encoder.layer.0.attention.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.intermediate.dense_type': 'LinearBlockLog', 'bert.encoder.layer.0.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.query_type': 'LinearBlockLog', 'bert.encoder.layer.1.attention.self.key_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.value_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.intermediate.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.output.dense_type': 'torch.nn.Linear', 'bert.pooler.dense_type': 'torch.nn.Linear', 'classifier_type': 'LinearBlockLog'}. Best is trial 2 with value: 0.8418.\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"," [3125/3125 03:27, Epoch 1/1]\n","Step\tTraining Loss\n","500\t0.693800\n","1000\t0.615400\n","1500\t0.472900\n","2000\t0.410200\n","2500\t0.371100\n","3000\t0.377600\n"," [3125/3125 02:49]\n","[I 2025-02-08 16:22:05,032] Trial 3 finished with value: 0.84276 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.self.key_type': 'LinearBlockLog', 'widths': 2, 'frac_widths': 2, 'stochastic': 1, 'bipolar': 1, 'data_in_exponent_bias_width': 1, 'bias_exponent_bias_width': 2, 'weight_exponent_bias_width': 0, 'data_in_exponent_width': 1, 'weight_exponent_width': 1, 'bias_exponent_width': 2, 'data_in_exponent_bias': 2, 'weight_exponent_bias': 0, 'bias_exponent_bias': 0, 'weight_block_size': 0, 'data_in_block_size': 0, 'bias_block_size': 0, 'bert.encoder.layer.0.attention.self.value_type': 'LinearBlockLog', 'bert.encoder.layer.0.attention.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.intermediate.dense_type': 'LinearBlockLog', 'bert.encoder.layer.0.output.dense_type': 'LinearBlockLog', 'bert.encoder.layer.1.attention.self.query_type': 'LinearBlockLog', 'bert.encoder.layer.1.attention.self.key_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.value_type': 'LinearBlockLog', 'bert.encoder.layer.1.attention.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.intermediate.dense_type': 'LinearBlockLog', 'bert.encoder.layer.1.output.dense_type': 'LinearBlockLog', 'bert.pooler.dense_type': 'LinearBlockLog', 'classifier_type': 'torch.nn.Linear'}. Best is trial 3 with value: 0.84276.\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"," [3125/3125 04:30, Epoch 1/1]\n","Step\tTraining Loss\n","500\t0.693800\n","1000\t0.619400\n","1500\t0.477300\n","2000\t0.413300\n","2500\t0.375600\n","3000\t0.380000\n"," [3125/3125 03:52]\n","[I 2025-02-08 16:30:29,884] Trial 4 finished with value: 0.8366 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.self.key_type': 'LinearBlockLog', 'widths': 1, 'frac_widths': 1, 'stochastic': 1, 'bipolar': 0, 'data_in_exponent_bias_width': 0, 'bias_exponent_bias_width': 2, 'weight_exponent_bias_width': 2, 'data_in_exponent_width': 0, 'weight_exponent_width': 2, 'bias_exponent_width': 1, 'data_in_exponent_bias': 1, 'weight_exponent_bias': 1, 'bias_exponent_bias': 1, 'weight_block_size': 0, 'data_in_block_size': 2, 'bias_block_size': 2, 'bert.encoder.layer.0.attention.self.value_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.output.dense_type': 'LinearBlockLog', 'bert.encoder.layer.0.intermediate.dense_type': 'LinearBlockLog', 'bert.encoder.layer.0.output.dense_type': 'LinearBlockLog', 'bert.encoder.layer.1.attention.self.query_type': 'LinearBlockLog', 'bert.encoder.layer.1.attention.self.key_type': 'LinearBlockLog', 'bert.encoder.layer.1.attention.self.value_type': 'LinearBlockLog', 'bert.encoder.layer.1.attention.output.dense_type': 'LinearBlockLog', 'bert.encoder.layer.1.intermediate.dense_type': 'LinearBlockLog', 'bert.encoder.layer.1.output.dense_type': 'LinearBlockLog', 'bert.pooler.dense_type': 'LinearBlockLog', 'classifier_type': 'LinearBlockLog'}. Best is trial 3 with value: 0.84276.\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"," [3125/3125 02:49, Epoch 1/1]\n","Step\tTraining Loss\n","500\t0.693800\n","1000\t0.628000\n","1500\t0.471400\n","2000\t0.412100\n","2500\t0.375200\n","3000\t0.381200\n"," [3125/3125 02:10]\n","[I 2025-02-08 16:35:31,950] Trial 5 finished with value: 0.84072 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.self.key_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.self.value_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.output.dense_type': 'LinearBlockLog', 'widths': 0, 'frac_widths': 2, 'stochastic': 0, 'bipolar': 0, 'data_in_exponent_bias_width': 0, 'bias_exponent_bias_width': 1, 'weight_exponent_bias_width': 0, 'data_in_exponent_width': 2, 'weight_exponent_width': 2, 'bias_exponent_width': 2, 'data_in_exponent_bias': 1, 'weight_exponent_bias': 0, 'bias_exponent_bias': 1, 'weight_block_size': 2, 'data_in_block_size': 1, 'bias_block_size': 2, 'bert.encoder.layer.0.intermediate.dense_type': 'LinearBlockLog', 'bert.encoder.layer.0.output.dense_type': 'LinearBlockLog', 'bert.encoder.layer.1.attention.self.query_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.key_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.value_type': 'LinearBlockLog', 'bert.encoder.layer.1.attention.output.dense_type': 'LinearBlockLog', 'bert.encoder.layer.1.intermediate.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.output.dense_type': 'LinearBlockLog', 'bert.pooler.dense_type': 'torch.nn.Linear', 'classifier_type': 'torch.nn.Linear'}. Best is trial 3 with value: 0.84276.\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"," [3125/3125 03:43, Epoch 1/1]\n","Step\tTraining Loss\n","500\t0.693700\n","1000\t0.619800\n","1500\t0.475000\n","2000\t0.413000\n","2500\t0.376000\n","3000\t0.381400\n"," [3125/3125 03:03]\n","[I 2025-02-08 16:42:20,879] Trial 6 finished with value: 0.83784 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.self.key_type': 'LinearBlockLog', 'widths': 0, 'frac_widths': 0, 'stochastic': 0, 'bipolar': 1, 'data_in_exponent_bias_width': 1, 'bias_exponent_bias_width': 1, 'weight_exponent_bias_width': 2, 'data_in_exponent_width': 2, 'weight_exponent_width': 0, 'bias_exponent_width': 2, 'data_in_exponent_bias': 2, 'weight_exponent_bias': 2, 'bias_exponent_bias': 1, 'weight_block_size': 0, 'data_in_block_size': 1, 'bias_block_size': 2, 'bert.encoder.layer.0.attention.self.value_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.output.dense_type': 'LinearBlockLog', 'bert.encoder.layer.0.intermediate.dense_type': 'LinearBlockLog', 'bert.encoder.layer.0.output.dense_type': 'LinearBlockLog', 'bert.encoder.layer.1.attention.self.query_type': 'LinearBlockLog', 'bert.encoder.layer.1.attention.self.key_type': 'LinearBlockLog', 'bert.encoder.layer.1.attention.self.value_type': 'LinearBlockLog', 'bert.encoder.layer.1.attention.output.dense_type': 'LinearBlockLog', 'bert.encoder.layer.1.intermediate.dense_type': 'LinearBlockLog', 'bert.encoder.layer.1.output.dense_type': 'torch.nn.Linear', 'bert.pooler.dense_type': 'LinearBlockLog', 'classifier_type': 'torch.nn.Linear'}. Best is trial 3 with value: 0.84276.\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"," [3125/3125 02:55, Epoch 1/1]\n","Step\tTraining Loss\n","500\t0.693800\n","1000\t0.624800\n","1500\t0.474700\n","2000\t0.412300\n","2500\t0.373900\n","3000\t0.380400\n"," [3125/3125 02:16]\n","[I 2025-02-08 16:47:34,469] Trial 7 finished with value: 0.84028 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'LinearBlockLog', 'widths': 2, 'frac_widths': 0, 'stochastic': 1, 'bipolar': 1, 'data_in_exponent_bias_width': 2, 'bias_exponent_bias_width': 0, 'weight_exponent_bias_width': 2, 'data_in_exponent_width': 1, 'weight_exponent_width': 1, 'bias_exponent_width': 0, 'data_in_exponent_bias': 0, 'weight_exponent_bias': 1, 'bias_exponent_bias': 1, 'weight_block_size': 1, 'data_in_block_size': 2, 'bias_block_size': 2, 'bert.encoder.layer.0.attention.self.key_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.self.value_type': 'LinearBlockLog', 'bert.encoder.layer.0.attention.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.intermediate.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.query_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.key_type': 'LinearBlockLog', 'bert.encoder.layer.1.attention.self.value_type': 'LinearBlockLog', 'bert.encoder.layer.1.attention.output.dense_type': 'LinearBlockLog', 'bert.encoder.layer.1.intermediate.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.output.dense_type': 'LinearBlockLog', 'bert.pooler.dense_type': 'torch.nn.Linear', 'classifier_type': 'torch.nn.Linear'}. Best is trial 3 with value: 0.84276.\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"," [3125/3125 02:51, Epoch 1/1]\n","Step\tTraining Loss\n","500\t0.694100\n","1000\t0.625900\n","1500\t0.474500\n","2000\t0.412100\n","2500\t0.378500\n","3000\t0.379100\n"," [3125/3125 02:15]\n","[I 2025-02-08 16:52:45,078] Trial 8 finished with value: 0.83984 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.self.key_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.self.value_type': 'LinearBlockLog', 'widths': 2, 'frac_widths': 1, 'stochastic': 0, 'bipolar': 1, 'data_in_exponent_bias_width': 2, 'bias_exponent_bias_width': 0, 'weight_exponent_bias_width': 2, 'data_in_exponent_width': 0, 'weight_exponent_width': 2, 'bias_exponent_width': 1, 'data_in_exponent_bias': 2, 'weight_exponent_bias': 0, 'bias_exponent_bias': 0, 'weight_block_size': 0, 'data_in_block_size': 1, 'bias_block_size': 2, 'bert.encoder.layer.0.attention.output.dense_type': 'LinearBlockLog', 'bert.encoder.layer.0.intermediate.dense_type': 'LinearBlockLog', 'bert.encoder.layer.0.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.query_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.key_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.value_type': 'LinearBlockLog', 'bert.encoder.layer.1.attention.output.dense_type': 'LinearBlockLog', 'bert.encoder.layer.1.intermediate.dense_type': 'LinearBlockLog', 'bert.encoder.layer.1.output.dense_type': 'torch.nn.Linear', 'bert.pooler.dense_type': 'torch.nn.Linear', 'classifier_type': 'LinearBlockLog'}. Best is trial 3 with value: 0.84276.\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"," [3125/3125 02:03, Epoch 1/1]\n","Step\tTraining Loss\n","500\t0.693600\n","1000\t0.613700\n","1500\t0.467200\n","2000\t0.405300\n","2500\t0.370700\n","3000\t0.372000\n"," [3125/3125 01:28]\n","[I 2025-02-08 16:56:19,547] Trial 9 finished with value: 0.843 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.self.key_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.self.value_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.intermediate.dense_type': 'LinearBlockLog', 'widths': 2, 'frac_widths': 0, 'stochastic': 1, 'bipolar': 1, 'data_in_exponent_bias_width': 1, 'bias_exponent_bias_width': 2, 'weight_exponent_bias_width': 1, 'data_in_exponent_width': 0, 'weight_exponent_width': 1, 'bias_exponent_width': 2, 'data_in_exponent_bias': 1, 'weight_exponent_bias': 0, 'bias_exponent_bias': 0, 'weight_block_size': 1, 'data_in_block_size': 1, 'bias_block_size': 1, 'bert.encoder.layer.0.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.query_type': 'LinearBlockLog', 'bert.encoder.layer.1.attention.self.key_type': 'LinearBlockLog', 'bert.encoder.layer.1.attention.self.value_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.intermediate.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.output.dense_type': 'torch.nn.Linear', 'bert.pooler.dense_type': 'LinearBlockLog', 'classifier_type': 'torch.nn.Linear'}. Best is trial 9 with value: 0.843.\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"," [3125/3125 02:09, Epoch 1/1]\n","Step\tTraining Loss\n","500\t0.693700\n","1000\t0.613500\n","1500\t0.465600\n","2000\t0.403800\n","2500\t0.370500\n","3000\t0.371700\n"," [3125/3125 01:31]\n","[I 2025-02-08 17:00:03,157] Trial 10 finished with value: 0.84332 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'LinearBlockLog', 'widths': 1, 'frac_widths': 0, 'stochastic': 1, 'bipolar': 1, 'data_in_exponent_bias_width': 1, 'bias_exponent_bias_width': 2, 'weight_exponent_bias_width': 1, 'data_in_exponent_width': 1, 'weight_exponent_width': 1, 'bias_exponent_width': 1, 'data_in_exponent_bias': 0, 'weight_exponent_bias': 2, 'bias_exponent_bias': 2, 'weight_block_size': 1, 'data_in_block_size': 2, 'bias_block_size': 0, 'bert.encoder.layer.0.attention.self.key_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.self.value_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.intermediate.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.query_type': 'LinearBlockLog', 'bert.encoder.layer.1.attention.self.key_type': 'LinearBlockLog', 'bert.encoder.layer.1.attention.self.value_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.intermediate.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.output.dense_type': 'torch.nn.Linear', 'bert.pooler.dense_type': 'LinearBlockLog', 'classifier_type': 'torch.nn.Linear'}. Best is trial 10 with value: 0.84332.\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"," [3125/3125 02:09, Epoch 1/1]\n","Step\tTraining Loss\n","500\t0.693700\n","1000\t0.613500\n","1500\t0.465600\n","2000\t0.403800\n","2500\t0.370600\n","3000\t0.371800\n"," [3125/3125 01:33]\n","[I 2025-02-08 17:03:49,038] Trial 11 finished with value: 0.84344 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'LinearBlockLog', 'widths': 1, 'frac_widths': 0, 'stochastic': 1, 'bipolar': 1, 'data_in_exponent_bias_width': 1, 'bias_exponent_bias_width': 2, 'weight_exponent_bias_width': 1, 'data_in_exponent_width': 1, 'weight_exponent_width': 1, 'bias_exponent_width': 1, 'data_in_exponent_bias': 0, 'weight_exponent_bias': 2, 'bias_exponent_bias': 2, 'weight_block_size': 1, 'data_in_block_size': 2, 'bias_block_size': 0, 'bert.encoder.layer.0.attention.self.key_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.self.value_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.intermediate.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.query_type': 'LinearBlockLog', 'bert.encoder.layer.1.attention.self.key_type': 'LinearBlockLog', 'bert.encoder.layer.1.attention.self.value_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.intermediate.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.output.dense_type': 'torch.nn.Linear', 'bert.pooler.dense_type': 'LinearBlockLog', 'classifier_type': 'torch.nn.Linear'}. Best is trial 11 with value: 0.84344.\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"," [3125/3125 02:10, Epoch 1/1]\n","Step\tTraining Loss\n","500\t0.693700\n","1000\t0.613500\n","1500\t0.465700\n","2000\t0.403800\n","2500\t0.370700\n","3000\t0.371700\n"," [3125/3125 01:33]\n","[I 2025-02-08 17:07:35,955] Trial 12 finished with value: 0.8432 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'LinearBlockLog', 'widths': 1, 'frac_widths': 0, 'stochastic': 1, 'bipolar': 1, 'data_in_exponent_bias_width': 1, 'bias_exponent_bias_width': 2, 'weight_exponent_bias_width': 1, 'data_in_exponent_width': 1, 'weight_exponent_width': 1, 'bias_exponent_width': 1, 'data_in_exponent_bias': 0, 'weight_exponent_bias': 2, 'bias_exponent_bias': 2, 'weight_block_size': 1, 'data_in_block_size': 2, 'bias_block_size': 0, 'bert.encoder.layer.0.attention.self.key_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.self.value_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.intermediate.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.query_type': 'LinearBlockLog', 'bert.encoder.layer.1.attention.self.key_type': 'LinearBlockLog', 'bert.encoder.layer.1.attention.self.value_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.intermediate.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.output.dense_type': 'torch.nn.Linear', 'bert.pooler.dense_type': 'LinearBlockLog', 'classifier_type': 'torch.nn.Linear'}. Best is trial 11 with value: 0.84344.\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"," [3125/3125 02:09, Epoch 1/1]\n","Step\tTraining Loss\n","500\t0.693700\n","1000\t0.613500\n","1500\t0.465600\n","2000\t0.403700\n","2500\t0.370600\n","3000\t0.371800\n"," [3125/3125 01:32]\n","[I 2025-02-08 17:11:19,770] Trial 13 finished with value: 0.84352 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'LinearBlockLog', 'widths': 1, 'frac_widths': 0, 'stochastic': 1, 'bipolar': 1, 'data_in_exponent_bias_width': 1, 'bias_exponent_bias_width': 2, 'weight_exponent_bias_width': 1, 'data_in_exponent_width': 1, 'weight_exponent_width': 0, 'bias_exponent_width': 1, 'data_in_exponent_bias': 0, 'weight_exponent_bias': 2, 'bias_exponent_bias': 2, 'weight_block_size': 1, 'data_in_block_size': 2, 'bias_block_size': 0, 'bert.encoder.layer.0.attention.self.key_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.self.value_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.intermediate.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.query_type': 'LinearBlockLog', 'bert.encoder.layer.1.attention.self.key_type': 'LinearBlockLog', 'bert.encoder.layer.1.attention.self.value_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.intermediate.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.output.dense_type': 'torch.nn.Linear', 'bert.pooler.dense_type': 'LinearBlockLog', 'classifier_type': 'torch.nn.Linear'}. Best is trial 13 with value: 0.84352.\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"," [3125/3125 02:09, Epoch 1/1]\n","Step\tTraining Loss\n","500\t0.693700\n","1000\t0.613500\n","1500\t0.465600\n","2000\t0.403700\n","2500\t0.370400\n","3000\t0.371800\n"," [3125/3125 01:32]\n","[I 2025-02-08 17:15:04,388] Trial 14 finished with value: 0.84348 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'LinearBlockLog', 'widths': 0, 'frac_widths': 0, 'stochastic': 1, 'bipolar': 1, 'data_in_exponent_bias_width': 1, 'bias_exponent_bias_width': 2, 'weight_exponent_bias_width': 1, 'data_in_exponent_width': 1, 'weight_exponent_width': 0, 'bias_exponent_width': 0, 'data_in_exponent_bias': 0, 'weight_exponent_bias': 2, 'bias_exponent_bias': 2, 'weight_block_size': 1, 'data_in_block_size': 2, 'bias_block_size': 1, 'bert.encoder.layer.0.attention.self.key_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.self.value_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.intermediate.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.query_type': 'LinearBlockLog', 'bert.encoder.layer.1.attention.self.key_type': 'LinearBlockLog', 'bert.encoder.layer.1.attention.self.value_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.intermediate.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.output.dense_type': 'torch.nn.Linear', 'bert.pooler.dense_type': 'LinearBlockLog', 'classifier_type': 'torch.nn.Linear'}. Best is trial 13 with value: 0.84352.\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"," [3125/3125 02:09, Epoch 1/1]\n","Step\tTraining Loss\n","500\t0.693700\n","1000\t0.613400\n","1500\t0.466200\n","2000\t0.403400\n","2500\t0.370700\n","3000\t0.371100\n"," [3125/3125 01:31]\n","[I 2025-02-08 17:18:47,669] Trial 15 finished with value: 0.84384 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'LinearBlockLog', 'widths': 0, 'frac_widths': 1, 'stochastic': 1, 'bipolar': 1, 'data_in_exponent_bias_width': 0, 'bias_exponent_bias_width': 1, 'weight_exponent_bias_width': 1, 'data_in_exponent_width': 1, 'weight_exponent_width': 0, 'bias_exponent_width': 0, 'data_in_exponent_bias': 0, 'weight_exponent_bias': 1, 'bias_exponent_bias': 2, 'weight_block_size': 1, 'data_in_block_size': 2, 'bias_block_size': 1, 'bert.encoder.layer.0.attention.self.key_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.self.value_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.intermediate.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.query_type': 'LinearBlockLog', 'bert.encoder.layer.1.attention.self.key_type': 'LinearBlockLog', 'bert.encoder.layer.1.attention.self.value_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.intermediate.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.output.dense_type': 'torch.nn.Linear', 'bert.pooler.dense_type': 'LinearBlockLog', 'classifier_type': 'torch.nn.Linear'}. Best is trial 15 with value: 0.84384.\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"," [3125/3125 02:09, Epoch 1/1]\n","Step\tTraining Loss\n","500\t0.693700\n","1000\t0.613100\n","1500\t0.466000\n","2000\t0.403200\n","2500\t0.369700\n","3000\t0.370700\n"," [3125/3125 01:31]\n","[I 2025-02-08 17:22:31,703] Trial 16 finished with value: 0.84384 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'LinearBlockLog', 'widths': 0, 'frac_widths': 1, 'stochastic': 1, 'bipolar': 1, 'data_in_exponent_bias_width': 0, 'bias_exponent_bias_width': 0, 'weight_exponent_bias_width': 1, 'data_in_exponent_width': 2, 'weight_exponent_width': 0, 'bias_exponent_width': 0, 'data_in_exponent_bias': 0, 'weight_exponent_bias': 1, 'bias_exponent_bias': 2, 'weight_block_size': 2, 'data_in_block_size': 2, 'bias_block_size': 1, 'bert.encoder.layer.0.attention.self.key_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.self.value_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.intermediate.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.query_type': 'LinearBlockLog', 'bert.encoder.layer.1.attention.self.key_type': 'LinearBlockLog', 'bert.encoder.layer.1.attention.self.value_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.intermediate.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.output.dense_type': 'torch.nn.Linear', 'bert.pooler.dense_type': 'LinearBlockLog', 'classifier_type': 'torch.nn.Linear'}. Best is trial 15 with value: 0.84384.\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"," [3125/3125 02:04, Epoch 1/1]\n","Step\tTraining Loss\n","500\t0.693700\n","1000\t0.613100\n","1500\t0.465900\n","2000\t0.403200\n","2500\t0.369900\n","3000\t0.370700\n"," [3125/3125 01:30]\n","[I 2025-02-08 17:26:08,896] Trial 17 finished with value: 0.8438 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'LinearBlockLog', 'widths': 0, 'frac_widths': 1, 'stochastic': 1, 'bipolar': 0, 'data_in_exponent_bias_width': 0, 'bias_exponent_bias_width': 0, 'weight_exponent_bias_width': 1, 'data_in_exponent_width': 2, 'weight_exponent_width': 0, 'bias_exponent_width': 0, 'data_in_exponent_bias': 1, 'weight_exponent_bias': 1, 'bias_exponent_bias': 2, 'weight_block_size': 2, 'data_in_block_size': 1, 'bias_block_size': 1, 'bert.encoder.layer.0.attention.self.key_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.self.value_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.intermediate.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.query_type': 'LinearBlockLog', 'bert.encoder.layer.1.attention.self.key_type': 'LinearBlockLog', 'bert.encoder.layer.1.attention.self.value_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.intermediate.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.output.dense_type': 'torch.nn.Linear', 'bert.pooler.dense_type': 'LinearBlockLog', 'classifier_type': 'torch.nn.Linear'}. Best is trial 15 with value: 0.84384.\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"," [3125/3125 02:13, Epoch 1/1]\n","Step\tTraining Loss\n","500\t0.693800\n","1000\t0.614300\n","1500\t0.468200\n","2000\t0.405000\n","2500\t0.370500\n","3000\t0.372400\n"," [3125/3125 01:35]\n","[I 2025-02-08 17:30:00,301] Trial 18 finished with value: 0.843 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'LinearBlockLog', 'widths': 0, 'frac_widths': 1, 'stochastic': 1, 'bipolar': 1, 'data_in_exponent_bias_width': 0, 'bias_exponent_bias_width': 0, 'weight_exponent_bias_width': 1, 'data_in_exponent_width': 2, 'weight_exponent_width': 0, 'bias_exponent_width': 0, 'data_in_exponent_bias': 0, 'weight_exponent_bias': 1, 'bias_exponent_bias': 2, 'weight_block_size': 2, 'data_in_block_size': 2, 'bias_block_size': 1, 'bert.encoder.layer.0.attention.self.key_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.self.value_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.intermediate.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.query_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.key_type': 'LinearBlockLog', 'bert.encoder.layer.1.attention.self.value_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.intermediate.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.output.dense_type': 'LinearBlockLog', 'bert.pooler.dense_type': 'LinearBlockLog', 'classifier_type': 'torch.nn.Linear'}. Best is trial 15 with value: 0.84384.\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"," [3125/3125 02:09, Epoch 1/1]\n","Step\tTraining Loss\n","500\t0.693700\n","1000\t0.613300\n","1500\t0.465800\n","2000\t0.403300\n","2500\t0.369900\n","3000\t0.370900\n"," [3125/3125 01:31]\n","[I 2025-02-08 17:33:43,906] Trial 19 finished with value: 0.84352 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'LinearBlockLog', 'widths': 0, 'frac_widths': 1, 'stochastic': 1, 'bipolar': 1, 'data_in_exponent_bias_width': 0, 'bias_exponent_bias_width': 1, 'weight_exponent_bias_width': 2, 'data_in_exponent_width': 2, 'weight_exponent_width': 0, 'bias_exponent_width': 0, 'data_in_exponent_bias': 0, 'weight_exponent_bias': 1, 'bias_exponent_bias': 1, 'weight_block_size': 2, 'data_in_block_size': 2, 'bias_block_size': 1, 'bert.encoder.layer.0.attention.self.key_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.self.value_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.intermediate.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.query_type': 'LinearBlockLog', 'bert.encoder.layer.1.attention.self.key_type': 'LinearBlockLog', 'bert.encoder.layer.1.attention.self.value_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.intermediate.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.output.dense_type': 'torch.nn.Linear', 'bert.pooler.dense_type': 'LinearBlockLog', 'classifier_type': 'torch.nn.Linear'}. Best is trial 15 with value: 0.84384.\n","[I 2025-02-08 17:33:44,032] A new study created in memory with name: bert-tiny-nas-LinearBinaryScaling\n","{'LinearMinifloatDenorm': [0.8438, 0.83252, 0.8426, 0.8408, 0.84124, 0.8414, 0.84128, 0.84388, 0.84268, 0.8192, 0.84316, 0.84212, 0.84212, 0.84472, 0.84448, 0.84472, 0.84472, 0.84472, 0.84328, 0.84472], 'LinearMinifloatIEEE': [0.84264, 0.8422, 0.84316, 0.8182, 0.5, 0.84032, 0.84132, 0.84216, 0.84248, 0.83932, 0.84156, 0.84244, 0.84232, 0.8432, 0.84148, 0.84336, 0.84356, 0.84336, 0.84344, 0.84264], 'LinearLog': [0.50124, 0.5, 0.49984, 0.6574, 0.50472, 0.5, 0.5, 0.5, 0.49604, 0.50052, 0.5, 0.4986, 0.50432, 0.5, 0.50244, 0.4958, 0.50392, 0.81572, 0.5, 0.5558], 'LinearBlockFP': [0.84208, 0.84268, 0.8428, 0.84168, 0.84136, 0.84184, 0.84196, 0.84248, 0.84296, 0.84344, 0.8426, 0.84312, 0.84384, 0.84212, 0.84208, 0.84212, 0.84344, 0.8448, 0.84192, 0.84468], 'LinearBlockLog': [0.8412, 0.84084, 0.8418, 0.84276, 0.8366, 0.84072, 0.83784, 0.84028, 0.83984, 0.843, 0.84332, 0.84344, 0.8432, 0.84352, 0.84348, 0.84384, 0.84384, 0.8438, 0.843, 0.84352]}\n","\n","Running study for LinearBinaryScaling...\n","\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"," [3125/3125 01:05, Epoch 1/1]\n","Step\tTraining Loss\n","500\t0.693800\n","1000\t0.621300\n","1500\t0.471600\n","2000\t0.405500\n","2500\t0.370400\n","3000\t0.376600\n"," [3125/3125 00:34]\n","[I 2025-02-08 17:35:25,923] Trial 0 finished with value: 0.8432 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'LinearBinaryScaling', 'widths': 2, 'frac_widths': 2, 'stochastic': 1, 'bipolar': 0, 'data_in_exponent_width': 1, 'weight_exponent_width': 0, 'bias_exponent_width': 0, 'data_in_exponent_bias': 0, 'weight_exponent_bias': 2, 'bias_exponent_bias': 0, 'binary_training': 1, 'bert.encoder.layer.0.attention.self.key_type': 'LinearBinaryScaling', 'bert.encoder.layer.0.attention.self.value_type': 'LinearBinaryScaling', 'bert.encoder.layer.0.attention.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.intermediate.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.output.dense_type': 'LinearBinaryScaling', 'bert.encoder.layer.1.attention.self.query_type': 'LinearBinaryScaling', 'bert.encoder.layer.1.attention.self.key_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.value_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.intermediate.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.output.dense_type': 'torch.nn.Linear', 'bert.pooler.dense_type': 'LinearBinaryScaling', 'classifier_type': 'LinearBinaryScaling'}. Best is trial 0 with value: 0.8432.\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"," [3125/3125 01:19, Epoch 1/1]\n","Step\tTraining Loss\n","500\t6.356300\n","1000\t6.596500\n","1500\t6.371200\n","2000\t6.297000\n","2500\t6.513800\n","3000\t6.476200\n"," [3125/3125 00:43]\n","[I 2025-02-08 17:37:31,186] Trial 1 finished with value: 0.4946 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.self.key_type': 'LinearBinaryScaling', 'widths': 0, 'frac_widths': 2, 'stochastic': 0, 'bipolar': 0, 'data_in_exponent_width': 2, 'weight_exponent_width': 2, 'bias_exponent_width': 1, 'data_in_exponent_bias': 1, 'weight_exponent_bias': 2, 'bias_exponent_bias': 1, 'binary_training': 0, 'bert.encoder.layer.0.attention.self.value_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.intermediate.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.output.dense_type': 'LinearBinaryScaling', 'bert.encoder.layer.1.attention.self.query_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.key_type': 'LinearBinaryScaling', 'bert.encoder.layer.1.attention.self.value_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.intermediate.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.output.dense_type': 'torch.nn.Linear', 'bert.pooler.dense_type': 'torch.nn.Linear', 'classifier_type': 'LinearBinaryScaling'}. Best is trial 0 with value: 0.8432.\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"," [3125/3125 01:05, Epoch 1/1]\n","Step\tTraining Loss\n","500\t0.693800\n","1000\t0.619500\n","1500\t0.470500\n","2000\t0.405500\n","2500\t0.369200\n","3000\t0.376100\n"," [3125/3125 00:33]\n","[I 2025-02-08 17:39:13,167] Trial 2 finished with value: 0.844 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.self.key_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.self.value_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.intermediate.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.output.dense_type': 'LinearBinaryScaling', 'widths': 2, 'frac_widths': 2, 'stochastic': 1, 'bipolar': 0, 'data_in_exponent_width': 2, 'weight_exponent_width': 0, 'bias_exponent_width': 1, 'data_in_exponent_bias': 1, 'weight_exponent_bias': 0, 'bias_exponent_bias': 2, 'binary_training': 1, 'bert.encoder.layer.1.attention.self.query_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.key_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.value_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.intermediate.dense_type': 'LinearBinaryScaling', 'bert.encoder.layer.1.output.dense_type': 'torch.nn.Linear', 'bert.pooler.dense_type': 'LinearBinaryScaling', 'classifier_type': 'LinearBinaryScaling'}. Best is trial 2 with value: 0.844.\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"," [3125/3125 01:30, Epoch 1/1]\n","Step\tTraining Loss\n","500\t6.542300\n","1000\t6.445500\n","1500\t6.599100\n","2000\t6.437200\n","2500\t6.442200\n","3000\t6.402600\n"," [3125/3125 00:52]\n","[I 2025-02-08 17:41:38,279] Trial 3 finished with value: 0.49964 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'LinearBinaryScaling', 'widths': 1, 'frac_widths': 0, 'stochastic': 0, 'bipolar': 0, 'data_in_exponent_width': 2, 'weight_exponent_width': 2, 'bias_exponent_width': 0, 'data_in_exponent_bias': 1, 'weight_exponent_bias': 0, 'bias_exponent_bias': 1, 'binary_training': 0, 'bert.encoder.layer.0.attention.self.key_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.self.value_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.output.dense_type': 'LinearBinaryScaling', 'bert.encoder.layer.0.intermediate.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.output.dense_type': 'LinearBinaryScaling', 'bert.encoder.layer.1.attention.self.query_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.key_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.value_type': 'LinearBinaryScaling', 'bert.encoder.layer.1.attention.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.intermediate.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.output.dense_type': 'LinearBinaryScaling', 'bert.pooler.dense_type': 'LinearBinaryScaling', 'classifier_type': 'LinearBinaryScaling'}. Best is trial 2 with value: 0.844.\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"," [3125/3125 01:06, Epoch 1/1]\n","Step\tTraining Loss\n","500\t0.693800\n","1000\t0.621700\n","1500\t0.470100\n","2000\t0.407000\n","2500\t0.372000\n","3000\t0.375400\n"," [3125/3125 00:32]\n","[I 2025-02-08 17:43:18,875] Trial 4 finished with value: 0.84228 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.self.key_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.self.value_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.output.dense_type': 'LinearBinaryScaling', 'widths': 0, 'frac_widths': 1, 'stochastic': 1, 'bipolar': 0, 'data_in_exponent_width': 2, 'weight_exponent_width': 1, 'bias_exponent_width': 1, 'data_in_exponent_bias': 2, 'weight_exponent_bias': 0, 'bias_exponent_bias': 2, 'binary_training': 1, 'bert.encoder.layer.0.intermediate.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.query_type': 'LinearBinaryScaling', 'bert.encoder.layer.1.attention.self.key_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.value_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.output.dense_type': 'LinearBinaryScaling', 'bert.encoder.layer.1.intermediate.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.output.dense_type': 'torch.nn.Linear', 'bert.pooler.dense_type': 'torch.nn.Linear', 'classifier_type': 'LinearBinaryScaling'}. Best is trial 2 with value: 0.844.\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"," [3125/3125 01:05, Epoch 1/1]\n","Step\tTraining Loss\n","500\t0.693800\n","1000\t0.621100\n","1500\t0.470000\n","2000\t0.408500\n","2500\t0.372700\n","3000\t0.376500\n"," [3125/3125 00:32]\n","[I 2025-02-08 17:44:59,904] Trial 5 finished with value: 0.84128 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.self.key_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.self.value_type': 'LinearBinaryScaling', 'widths': 2, 'frac_widths': 1, 'stochastic': 1, 'bipolar': 1, 'data_in_exponent_width': 1, 'weight_exponent_width': 2, 'bias_exponent_width': 2, 'data_in_exponent_bias': 1, 'weight_exponent_bias': 0, 'bias_exponent_bias': 1, 'binary_training': 1, 'bert.encoder.layer.0.attention.output.dense_type': 'LinearBinaryScaling', 'bert.encoder.layer.0.intermediate.dense_type': 'LinearBinaryScaling', 'bert.encoder.layer.0.output.dense_type': 'LinearBinaryScaling', 'bert.encoder.layer.1.attention.self.query_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.key_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.value_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.intermediate.dense_type': 'LinearBinaryScaling', 'bert.encoder.layer.1.output.dense_type': 'torch.nn.Linear', 'bert.pooler.dense_type': 'LinearBinaryScaling', 'classifier_type': 'torch.nn.Linear'}. Best is trial 2 with value: 0.844.\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"," [3125/3125 01:05, Epoch 1/1]\n","Step\tTraining Loss\n","500\t0.693800\n","1000\t0.621100\n","1500\t0.479400\n","2000\t0.411400\n","2500\t0.371100\n","3000\t0.378800\n"," [3125/3125 00:33]\n","[I 2025-02-08 17:46:41,312] Trial 6 finished with value: 0.84132 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.self.key_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.self.value_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.output.dense_type': 'LinearBinaryScaling', 'widths': 1, 'frac_widths': 2, 'stochastic': 1, 'bipolar': 1, 'data_in_exponent_width': 1, 'weight_exponent_width': 0, 'bias_exponent_width': 2, 'data_in_exponent_bias': 0, 'weight_exponent_bias': 0, 'bias_exponent_bias': 2, 'binary_training': 1, 'bert.encoder.layer.0.intermediate.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.query_type': 'LinearBinaryScaling', 'bert.encoder.layer.1.attention.self.key_type': 'LinearBinaryScaling', 'bert.encoder.layer.1.attention.self.value_type': 'LinearBinaryScaling', 'bert.encoder.layer.1.attention.output.dense_type': 'LinearBinaryScaling', 'bert.encoder.layer.1.intermediate.dense_type': 'LinearBinaryScaling', 'bert.encoder.layer.1.output.dense_type': 'torch.nn.Linear', 'bert.pooler.dense_type': 'torch.nn.Linear', 'classifier_type': 'LinearBinaryScaling'}. Best is trial 2 with value: 0.844.\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"," [3125/3125 01:05, Epoch 1/1]\n","Step\tTraining Loss\n","500\t0.693800\n","1000\t0.620400\n","1500\t0.471800\n","2000\t0.407400\n","2500\t0.370700\n","3000\t0.376300\n"," [3125/3125 00:32]\n","[I 2025-02-08 17:48:21,682] Trial 7 finished with value: 0.84416 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.self.key_type': 'LinearBinaryScaling', 'widths': 2, 'frac_widths': 0, 'stochastic': 0, 'bipolar': 1, 'data_in_exponent_width': 2, 'weight_exponent_width': 0, 'bias_exponent_width': 0, 'data_in_exponent_bias': 2, 'weight_exponent_bias': 2, 'bias_exponent_bias': 1, 'binary_training': 1, 'bert.encoder.layer.0.attention.self.value_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.intermediate.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.query_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.key_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.value_type': 'LinearBinaryScaling', 'bert.encoder.layer.1.attention.output.dense_type': 'LinearBinaryScaling', 'bert.encoder.layer.1.intermediate.dense_type': 'LinearBinaryScaling', 'bert.encoder.layer.1.output.dense_type': 'torch.nn.Linear', 'bert.pooler.dense_type': 'LinearBinaryScaling', 'classifier_type': 'torch.nn.Linear'}. Best is trial 7 with value: 0.84416.\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"," [3125/3125 01:37, Epoch 1/1]\n","Step\tTraining Loss\n","500\t6.441100\n","1000\t6.481600\n","1500\t6.471800\n","2000\t6.790900\n","2500\t6.604300\n","3000\t6.658700\n"," [3125/3125 00:58]\n","[I 2025-02-08 17:51:01,187] Trial 8 finished with value: 0.50632 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.self.key_type': 'LinearBinaryScaling', 'widths': 1, 'frac_widths': 2, 'stochastic': 1, 'bipolar': 0, 'data_in_exponent_width': 0, 'weight_exponent_width': 0, 'bias_exponent_width': 0, 'data_in_exponent_bias': 1, 'weight_exponent_bias': 0, 'bias_exponent_bias': 2, 'binary_training': 0, 'bert.encoder.layer.0.attention.self.value_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.output.dense_type': 'LinearBinaryScaling', 'bert.encoder.layer.0.intermediate.dense_type': 'LinearBinaryScaling', 'bert.encoder.layer.0.output.dense_type': 'LinearBinaryScaling', 'bert.encoder.layer.1.attention.self.query_type': 'LinearBinaryScaling', 'bert.encoder.layer.1.attention.self.key_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.value_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.output.dense_type': 'LinearBinaryScaling', 'bert.encoder.layer.1.intermediate.dense_type': 'LinearBinaryScaling', 'bert.encoder.layer.1.output.dense_type': 'LinearBinaryScaling', 'bert.pooler.dense_type': 'LinearBinaryScaling', 'classifier_type': 'LinearBinaryScaling'}. Best is trial 7 with value: 0.84416.\n","/content/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n","[W 2025-02-08 17:51:03,440] Trial 9 failed with parameters: {'bert.encoder.layer.0.attention.self.query_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.self.key_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.self.value_type': 'torch.nn.Linear', 'bert.encoder.layer.0.attention.output.dense_type': 'LinearBinaryScaling', 'widths': 0, 'frac_widths': 2, 'stochastic': 1, 'bipolar': 1, 'data_in_exponent_width': 0, 'weight_exponent_width': 1, 'bias_exponent_width': 1, 'data_in_exponent_bias': 0, 'weight_exponent_bias': 1, 'bias_exponent_bias': 2, 'binary_training': 0, 'bert.encoder.layer.0.intermediate.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.0.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.query_type': 'torch.nn.Linear', 'bert.encoder.layer.1.attention.self.key_type': 'LinearBinaryScaling', 'bert.encoder.layer.1.attention.self.value_type': 'LinearBinaryScaling', 'bert.encoder.layer.1.attention.output.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.intermediate.dense_type': 'torch.nn.Linear', 'bert.encoder.layer.1.output.dense_type': 'LinearBinaryScaling', 'bert.pooler.dense_type': 'LinearBinaryScaling', 'classifier_type': 'torch.nn.Linear'} because of the following error: IndexError('Dimension out of range (expected to be in range of [-3, 2], but got 3)').\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n","    value_or_values = func(trial)\n","                      ^^^^^^^^^^^\n","  File \"<ipython-input-14-2c770aca15cb>\", line 18, in objective\n","    trainer.train()\n","  File \"/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\", line 2171, in train\n","    return inner_training_loop(\n","           ^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\", line 2531, in _inner_training_loop\n","    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)\n","                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\", line 3675, in training_step\n","    loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\", line 3731, in compute_loss\n","    outputs = model(**inputs)\n","              ^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n","    return forward_call(*args, **kwargs)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\", line 1665, in forward\n","    outputs = self.bert(\n","              ^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n","    return forward_call(*args, **kwargs)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\", line 1142, in forward\n","    encoder_outputs = self.encoder(\n","                      ^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n","    return forward_call(*args, **kwargs)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\", line 695, in forward\n","    layer_outputs = layer_module(\n","                    ^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n","    return forward_call(*args, **kwargs)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\", line 585, in forward\n","    self_attention_outputs = self.attention(\n","                             ^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n","    return forward_call(*args, **kwargs)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\", line 524, in forward\n","    attention_output = self.output(self_outputs[0], hidden_states)\n","                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n","    return forward_call(*args, **kwargs)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\", line 466, in forward\n","    hidden_states = self.dense(hidden_states)\n","                    ^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n","    return forward_call(*args, **kwargs)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/content/mase/src/chop/nn/quantized/modules/linear.py\", line 321, in forward\n","    return linearBinaryScaling(x, self.weight, self.bias, self.config)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/content/mase/src/chop/nn/quantized/functional/linear.py\", line 496, in linearBinaryScaling\n","    x = x_quantizer(x)\n","        ^^^^^^^^^^^^^^\n","  File \"/content/mase/src/chop/nn/quantizers/binary.py\", line 51, in binary_quantizer\n","    x = binarised_bipolar_op(x, 0) if bipolar else binarised_zeroScaled_op(x, 0)\n","                                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/torch/autograd/function.py\", line 575, in apply\n","    return super().apply(*args, **kwargs)  # type: ignore[misc]\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/content/mase/src/chop/nn/quantizers/utils.py\", line 163, in forward\n","    alpha = BinaryZeroScaled.alpha(input)\n","            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/content/mase/src/chop/nn/quantizers/utils.py\", line 158, in alpha\n","    alpha = absvalue.mean(dim=(1, 2, 3), keepdims=True)\n","            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","IndexError: Dimension out of range (expected to be in range of [-3, 2], but got 3)\n","[W 2025-02-08 17:51:03,451] Trial 9 failed with value None.\n","---------------------------------------------------------------------------\n","IndexError                                Traceback (most recent call last)\n","<ipython-input-16-ce43a8ac2a22> in <cell line: 0>()\n","     54\n","     55     # Run the study\n","---> 56     study.optimize(objective, n_trials=20)\n","     57\n","     58     # Store raw results for this precision\n","\n","35 frames\n","/content/mase/src/chop/nn/quantizers/utils.py in alpha(tensor)\n","    156     def alpha(tensor):  # determine batch means\n","    157         absvalue = tensor.abs()\n","--> 158         alpha = absvalue.mean(dim=(1, 2, 3), keepdims=True)\n","    159         return alpha.view(-1, 1)\n","    160\n","\n","IndexError: Dimension out of range (expected to be in range of [-3, 2], but got 3)\n"],"metadata":{"id":"0-nvOUcFqy2l"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"},"widgets":{"application/vnd.jupyter.widget-state+json":{"8e1087806b4a40f3bf6eed3fa3b57445":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_bba89ab2dbc149429d5225960b9dcf9f","IPY_MODEL_867b3cabf4034aa7bc0b2f693fbb130a","IPY_MODEL_a6c6d31160e240628c1bd7fd19750833"],"layout":"IPY_MODEL_2d949af6aaa04e6aa47391a46c1448af"}},"bba89ab2dbc149429d5225960b9dcf9f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1bcdf61d1c6f4e33965526ed9c22656a","placeholder":"​","style":"IPY_MODEL_ba3bbe67a1a5411cb883ba46ee103a40","value":"Downloading builder script: 100%"}},"867b3cabf4034aa7bc0b2f693fbb130a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_612c42def73041cbbd9309b328e13e9b","max":4203,"min":0,"orientation":"horizontal","style":"IPY_MODEL_bf82fc3729114b639a155e45c0b3f149","value":4203}},"a6c6d31160e240628c1bd7fd19750833":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d08868fb9ce54f31be7f6a4bbaeacbf8","placeholder":"​","style":"IPY_MODEL_670cc9cb18364fb0ae92d35ba5640185","value":" 4.20k/4.20k [00:00&lt;00:00, 315kB/s]"}},"2d949af6aaa04e6aa47391a46c1448af":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1bcdf61d1c6f4e33965526ed9c22656a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ba3bbe67a1a5411cb883ba46ee103a40":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"612c42def73041cbbd9309b328e13e9b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bf82fc3729114b639a155e45c0b3f149":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d08868fb9ce54f31be7f6a4bbaeacbf8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"670cc9cb18364fb0ae92d35ba5640185":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4985c1de4e904c0a8aae3e3f0c7186cf":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7e781d40e5a44d0cac226ff41e70e6b9","IPY_MODEL_8720e0da5f364049a1d2dec3f9b44f93","IPY_MODEL_d512ede0a3fc47f9afa699ad051e8bab"],"layout":"IPY_MODEL_4ca4446475e1456ab2f4e7d48af66e24"}},"7e781d40e5a44d0cac226ff41e70e6b9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0fcf9268779641e0a613da8f05708821","placeholder":"​","style":"IPY_MODEL_a1aebf08073f4683a7a3f9d96d24b23a","value":"config.json: 100%"}},"8720e0da5f364049a1d2dec3f9b44f93":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b23cb28f50404483bd04a6109eefc522","max":285,"min":0,"orientation":"horizontal","style":"IPY_MODEL_bf8ffae4f7fb4faf8e9245e0f68edc31","value":285}},"d512ede0a3fc47f9afa699ad051e8bab":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_237a89d8e7ea44629b78c59113edc41b","placeholder":"​","style":"IPY_MODEL_d8e822c46f7c4c6c9378ae907d26a86a","value":" 285/285 [00:00&lt;00:00, 8.59kB/s]"}},"4ca4446475e1456ab2f4e7d48af66e24":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0fcf9268779641e0a613da8f05708821":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a1aebf08073f4683a7a3f9d96d24b23a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b23cb28f50404483bd04a6109eefc522":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bf8ffae4f7fb4faf8e9245e0f68edc31":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"237a89d8e7ea44629b78c59113edc41b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d8e822c46f7c4c6c9378ae907d26a86a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4bdb10caca49409e95903539ef3a2d57":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d817aa555c074874920d80a31da20404","IPY_MODEL_185f544c0d4a4277993c022abc595a4d","IPY_MODEL_ce2fb24ff435467fa85f917ea3ac9f79"],"layout":"IPY_MODEL_d0aacec934814dbb81d52f289614b6e5"}},"d817aa555c074874920d80a31da20404":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_104919436f284f588dc83b95271eeb2f","placeholder":"​","style":"IPY_MODEL_83360339bab6416a994916e3f33c5e0e","value":"pytorch_model.bin: 100%"}},"185f544c0d4a4277993c022abc595a4d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7de5b532363b4beea02f209be88d91e6","max":17756393,"min":0,"orientation":"horizontal","style":"IPY_MODEL_70daed713f62419699fd08e162a4cb4b","value":17756393}},"ce2fb24ff435467fa85f917ea3ac9f79":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7e9b88c1ab93441fa50ab70cf5a1eeed","placeholder":"​","style":"IPY_MODEL_8f13bcf9392441f5ad086384eedc64ae","value":" 17.8M/17.8M [00:00&lt;00:00, 95.6MB/s]"}},"d0aacec934814dbb81d52f289614b6e5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"104919436f284f588dc83b95271eeb2f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"83360339bab6416a994916e3f33c5e0e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7de5b532363b4beea02f209be88d91e6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"70daed713f62419699fd08e162a4cb4b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7e9b88c1ab93441fa50ab70cf5a1eeed":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8f13bcf9392441f5ad086384eedc64ae":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8fa7f69d42644eb4b01a3286aa4a48fa":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fbfb5b5b524b4f2bb6a0972ca5c2ee9b","IPY_MODEL_b77870a34d2c44d391bec148149301c3","IPY_MODEL_6ba6b74022d8432ab1422531b16c8054"],"layout":"IPY_MODEL_a44170557bcc4441b8781e00952d25a9"}},"fbfb5b5b524b4f2bb6a0972ca5c2ee9b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9c96f0c74ed34f898078432a419326ec","placeholder":"​","style":"IPY_MODEL_7cb7c6fd261b41cb9272d09195a26d11","value":"model.safetensors: 100%"}},"b77870a34d2c44d391bec148149301c3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a86a468908a74815b85f6e8980ad6c24","max":17743328,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5eb2734f479b46c99f5d3595a626b195","value":17743328}},"6ba6b74022d8432ab1422531b16c8054":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_04360d9bcbc645f7b044e86d77b8150a","placeholder":"​","style":"IPY_MODEL_0d7e4dd56cd74d7dbd556a3f69850cf1","value":" 17.7M/17.7M [00:00&lt;00:00, 79.7MB/s]"}},"a44170557bcc4441b8781e00952d25a9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9c96f0c74ed34f898078432a419326ec":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7cb7c6fd261b41cb9272d09195a26d11":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a86a468908a74815b85f6e8980ad6c24":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5eb2734f479b46c99f5d3595a626b195":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"04360d9bcbc645f7b044e86d77b8150a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0d7e4dd56cd74d7dbd556a3f69850cf1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"daf5e89ca30041e3a34c655982ac2ba5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_23a32ae335014fa38b8b6dddb6910a5e","IPY_MODEL_faf5d52ed1ab4629b1d771a7584a95ae","IPY_MODEL_ffcb98c663d347f4b63cab0cf9248bc1"],"layout":"IPY_MODEL_8fa4bbe3202a41f982936520dab17183"}},"23a32ae335014fa38b8b6dddb6910a5e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5b12419e752045a18987ee7843ba1627","placeholder":"​","style":"IPY_MODEL_9fdc97e32f0649cba37b9d2a3e08b2be","value":"README.md: 100%"}},"faf5d52ed1ab4629b1d771a7584a95ae":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_33dc22689933474e8b301bb77d046402","max":7809,"min":0,"orientation":"horizontal","style":"IPY_MODEL_86bcf5428a4247bf8a56ddd7d73f528c","value":7809}},"ffcb98c663d347f4b63cab0cf9248bc1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ded28a3588644a468f9449b985d94162","placeholder":"​","style":"IPY_MODEL_dfac797828554062a5e32e5e59cf20d8","value":" 7.81k/7.81k [00:00&lt;00:00, 147kB/s]"}},"8fa4bbe3202a41f982936520dab17183":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5b12419e752045a18987ee7843ba1627":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9fdc97e32f0649cba37b9d2a3e08b2be":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"33dc22689933474e8b301bb77d046402":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"86bcf5428a4247bf8a56ddd7d73f528c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ded28a3588644a468f9449b985d94162":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dfac797828554062a5e32e5e59cf20d8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0606ff59d8e54d13bdbbf666a9c5d406":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6377be3b43c24160ba473463cc52a381","IPY_MODEL_667b4fbc6e1940e78da9dfdd811ea08f","IPY_MODEL_5e121a528ebd4ee7839337d108725711"],"layout":"IPY_MODEL_e50d5ac9eacd4570b2c62bac468f663f"}},"6377be3b43c24160ba473463cc52a381":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6054ea1b5c0f4b028c69fc73a5319104","placeholder":"​","style":"IPY_MODEL_41960a6da10a4abf99c48775e9945cb0","value":"train-00000-of-00001.parquet: 100%"}},"667b4fbc6e1940e78da9dfdd811ea08f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e919c893f9af4aae9a537532875681a1","max":20979968,"min":0,"orientation":"horizontal","style":"IPY_MODEL_67957df6de93437e858bbc48f256d5d4","value":20979968}},"5e121a528ebd4ee7839337d108725711":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a464203926c142268d1bf364073c8808","placeholder":"​","style":"IPY_MODEL_1d694e66fa47476e9e3bee7810fe5824","value":" 21.0M/21.0M [00:00&lt;00:00, 46.5MB/s]"}},"e50d5ac9eacd4570b2c62bac468f663f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6054ea1b5c0f4b028c69fc73a5319104":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"41960a6da10a4abf99c48775e9945cb0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e919c893f9af4aae9a537532875681a1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"67957df6de93437e858bbc48f256d5d4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a464203926c142268d1bf364073c8808":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1d694e66fa47476e9e3bee7810fe5824":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"001c2f555cda4cdfb3405a5a80591962":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_029eb6fc9e474aeda4eda8c351a99642","IPY_MODEL_ac56983ef01f4f3aa807fc458312b85b","IPY_MODEL_d85779dbe0844791a24e32b734f451a3"],"layout":"IPY_MODEL_343d2025538840b7ba3dfd9c13d73afa"}},"029eb6fc9e474aeda4eda8c351a99642":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_486e8403fd1b4652a8b6e5601ed9b3c1","placeholder":"​","style":"IPY_MODEL_cd88b97c62294ac0a9e1b57374ae3888","value":"test-00000-of-00001.parquet: 100%"}},"ac56983ef01f4f3aa807fc458312b85b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_19ebaf168afb4f90bd95ebd6856170be","max":20470363,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0c00c33bb0f74840a87e9d298ad7c5ef","value":20470363}},"d85779dbe0844791a24e32b734f451a3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e57bcc4a8bc941bfbaf86a3ba61de7e6","placeholder":"​","style":"IPY_MODEL_e3896ee1199c41fabea1833ff55b5b95","value":" 20.5M/20.5M [00:00&lt;00:00, 114MB/s]"}},"343d2025538840b7ba3dfd9c13d73afa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"486e8403fd1b4652a8b6e5601ed9b3c1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cd88b97c62294ac0a9e1b57374ae3888":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"19ebaf168afb4f90bd95ebd6856170be":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0c00c33bb0f74840a87e9d298ad7c5ef":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e57bcc4a8bc941bfbaf86a3ba61de7e6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e3896ee1199c41fabea1833ff55b5b95":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"eb2310b0751e435a9f00b150363abe2f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_885cf7ab6f8149aeaa7c36b4df50e886","IPY_MODEL_4d97c46543c447d8b648969da34db675","IPY_MODEL_4ceae4161a8944f085fc876791d1a6bc"],"layout":"IPY_MODEL_5bd8b360bf1e4ea8b3c35fd29599182e"}},"885cf7ab6f8149aeaa7c36b4df50e886":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e8715b393767400f8a7b97e32354f16d","placeholder":"​","style":"IPY_MODEL_874d30946b614543818170db6ba1a085","value":"unsupervised-00000-of-00001.parquet: 100%"}},"4d97c46543c447d8b648969da34db675":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_89617f043d3345b7983ceab94e8258c7","max":41996509,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4912a318cef94789ac637ab4b60cbd51","value":41996509}},"4ceae4161a8944f085fc876791d1a6bc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fa5ade32f5af42958f69b84f1119b628","placeholder":"​","style":"IPY_MODEL_c3e1f43856e840d7bac85a32b2a07557","value":" 42.0M/42.0M [00:00&lt;00:00, 188MB/s]"}},"5bd8b360bf1e4ea8b3c35fd29599182e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e8715b393767400f8a7b97e32354f16d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"874d30946b614543818170db6ba1a085":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"89617f043d3345b7983ceab94e8258c7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4912a318cef94789ac637ab4b60cbd51":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"fa5ade32f5af42958f69b84f1119b628":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c3e1f43856e840d7bac85a32b2a07557":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"833e8d089ed44a6c881bacab8ffce4f1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6a6ef2f9a543456ab1c8caae6a67139c","IPY_MODEL_f4c2cc4ff73c4d67a880145315d14295","IPY_MODEL_522d31f9d89142f4a0047e822d08a445"],"layout":"IPY_MODEL_a36729c0c60049149a5453d6db17d9cc"}},"6a6ef2f9a543456ab1c8caae6a67139c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_63334775b8e54b52b175c7cceb48ff3b","placeholder":"​","style":"IPY_MODEL_e06c01a92fd447a8b8983cc446441cf2","value":"Generating train split: 100%"}},"f4c2cc4ff73c4d67a880145315d14295":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_cfb78767d0dd4f76902a07003a4a7287","max":25000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_42e5ac852ccf477ba5e33bef71d9cb8e","value":25000}},"522d31f9d89142f4a0047e822d08a445":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_38e3da9545f4429b94e8db46f676def8","placeholder":"​","style":"IPY_MODEL_d0fbfb436d584904a3c82e3742079107","value":" 25000/25000 [00:00&lt;00:00, 55151.43 examples/s]"}},"a36729c0c60049149a5453d6db17d9cc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"63334775b8e54b52b175c7cceb48ff3b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e06c01a92fd447a8b8983cc446441cf2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cfb78767d0dd4f76902a07003a4a7287":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"42e5ac852ccf477ba5e33bef71d9cb8e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"38e3da9545f4429b94e8db46f676def8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d0fbfb436d584904a3c82e3742079107":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"66ab779a352e4b58994f67f048980cfe":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ba868c25adfa4bc19152744cf7fa4dd6","IPY_MODEL_d029eaff40674a3db02226e424ca9f58","IPY_MODEL_3845c408b3b14433a30d652af354b132"],"layout":"IPY_MODEL_9897cc4887ea4a3bb7e05cdc01864045"}},"ba868c25adfa4bc19152744cf7fa4dd6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4cf49eb808e94e1fac62ba0bbdd636f7","placeholder":"​","style":"IPY_MODEL_beca4a8d79bc4d0089db961e425e4ee8","value":"Generating test split: 100%"}},"d029eaff40674a3db02226e424ca9f58":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9d13780667e14063b826338b01b6ec31","max":25000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_099e9b67eeb04294b68538359a1d064a","value":25000}},"3845c408b3b14433a30d652af354b132":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8ac84c47d93d42699926940569ccac1c","placeholder":"​","style":"IPY_MODEL_20037fe3308043ecae050e8e31ff2cfb","value":" 25000/25000 [00:00&lt;00:00, 81838.10 examples/s]"}},"9897cc4887ea4a3bb7e05cdc01864045":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4cf49eb808e94e1fac62ba0bbdd636f7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"beca4a8d79bc4d0089db961e425e4ee8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9d13780667e14063b826338b01b6ec31":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"099e9b67eeb04294b68538359a1d064a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8ac84c47d93d42699926940569ccac1c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"20037fe3308043ecae050e8e31ff2cfb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5a9e25e127a749fd958bec97da0ad93b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ba700e0a69cf46efa70426312e6a3140","IPY_MODEL_b67c180c1e0945a48b2fcf9f517ad232","IPY_MODEL_b52ef5c2124c40ef8e7a661e947d3d35"],"layout":"IPY_MODEL_60b6b91d91444aafa036c359a9db872c"}},"ba700e0a69cf46efa70426312e6a3140":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_719bc133c26c478b8cdf288a4bfa170f","placeholder":"​","style":"IPY_MODEL_d71099f8ba774d5983bea01c2dfe57b5","value":"Generating unsupervised split: 100%"}},"b67c180c1e0945a48b2fcf9f517ad232":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e310978bdd924e0daf7434a8a7fae101","max":50000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7106f3946d18433fad639e5c397b4d85","value":50000}},"b52ef5c2124c40ef8e7a661e947d3d35":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ae28b7d6ea9a4a5dad73214c10510390","placeholder":"​","style":"IPY_MODEL_e0bf2769dec34e058ac7d13efa321fd8","value":" 50000/50000 [00:00&lt;00:00, 99432.86 examples/s]"}},"60b6b91d91444aafa036c359a9db872c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"719bc133c26c478b8cdf288a4bfa170f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d71099f8ba774d5983bea01c2dfe57b5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e310978bdd924e0daf7434a8a7fae101":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7106f3946d18433fad639e5c397b4d85":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ae28b7d6ea9a4a5dad73214c10510390":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e0bf2769dec34e058ac7d13efa321fd8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"221c08d4b7d54506ac8b849a95667f81":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3ac7c83f653c429c91bdb95228c69ba5","IPY_MODEL_16624b2fa5b2449ebc28b6572d025ad1","IPY_MODEL_d412ed81601b457e8b77df2a657fa968"],"layout":"IPY_MODEL_c51782c6333f4d779bd251b9f17ae0c5"}},"3ac7c83f653c429c91bdb95228c69ba5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3ae7939c82244f68a298be42e269ec31","placeholder":"​","style":"IPY_MODEL_048429b8243c48579f0bcab5dc789527","value":"tokenizer_config.json: 100%"}},"16624b2fa5b2449ebc28b6572d025ad1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ffa1654320564abaae7e68df8445d5b7","max":48,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ffd53cbd56564cca94fb1aec7da28e20","value":48}},"d412ed81601b457e8b77df2a657fa968":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4bfaa17f80404bdba2ff4b4097393076","placeholder":"​","style":"IPY_MODEL_f2501667fd554c3b8c5f87e3cf3a24ef","value":" 48.0/48.0 [00:00&lt;00:00, 1.68kB/s]"}},"c51782c6333f4d779bd251b9f17ae0c5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3ae7939c82244f68a298be42e269ec31":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"048429b8243c48579f0bcab5dc789527":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ffa1654320564abaae7e68df8445d5b7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ffd53cbd56564cca94fb1aec7da28e20":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4bfaa17f80404bdba2ff4b4097393076":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f2501667fd554c3b8c5f87e3cf3a24ef":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"613a3db399af439eb1f3499dd5d87c15":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2f7282a9ae824298a3725ca663e74ca7","IPY_MODEL_4182d7dc94a34cdf95fe003314a9f554","IPY_MODEL_847f5334548946cb9ee26f2f6e1ba664"],"layout":"IPY_MODEL_0673c93bd00f4a609a86df105ad1db9f"}},"2f7282a9ae824298a3725ca663e74ca7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dea1cb33f7804aff97f29cd65fb65f1f","placeholder":"​","style":"IPY_MODEL_57afed6d5b7d45e3ac909199e003176d","value":"config.json: 100%"}},"4182d7dc94a34cdf95fe003314a9f554":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d1273d3b7e984566af499694b72b97d9","max":570,"min":0,"orientation":"horizontal","style":"IPY_MODEL_acbaf720bac24105b8b57fad9cb55578","value":570}},"847f5334548946cb9ee26f2f6e1ba664":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a0c918cdf14b4d3781468f9007c9536a","placeholder":"​","style":"IPY_MODEL_fdc08856ee7c49e3b1ddf508afd76ecc","value":" 570/570 [00:00&lt;00:00, 18.0kB/s]"}},"0673c93bd00f4a609a86df105ad1db9f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dea1cb33f7804aff97f29cd65fb65f1f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"57afed6d5b7d45e3ac909199e003176d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d1273d3b7e984566af499694b72b97d9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"acbaf720bac24105b8b57fad9cb55578":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a0c918cdf14b4d3781468f9007c9536a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fdc08856ee7c49e3b1ddf508afd76ecc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4725d84ef9eb4e749f8b896224eb3115":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_bdf57014546c43d1808779847f0daf99","IPY_MODEL_b77b1066179b4bceb599ef598dbbf6fa","IPY_MODEL_9d4423b01c5545e896a1503b24a89071"],"layout":"IPY_MODEL_faee065eba7e42d0bfd00154e19b5f36"}},"bdf57014546c43d1808779847f0daf99":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e67c3df7dbe143f4982e9ca77a28697f","placeholder":"​","style":"IPY_MODEL_5aebef14b2a14c0db50012cc6f4a9d54","value":"vocab.txt: 100%"}},"b77b1066179b4bceb599ef598dbbf6fa":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c371dd51e9e94b9ba48a9ee429db8813","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_671e3aef8fbf488f86e2b72d058fd892","value":231508}},"9d4423b01c5545e896a1503b24a89071":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e95d193ec23d4a26baf5b7859ca24327","placeholder":"​","style":"IPY_MODEL_671fd81bb57145eab93a79ca8e68ac1a","value":" 232k/232k [00:00&lt;00:00, 536kB/s]"}},"faee065eba7e42d0bfd00154e19b5f36":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e67c3df7dbe143f4982e9ca77a28697f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5aebef14b2a14c0db50012cc6f4a9d54":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c371dd51e9e94b9ba48a9ee429db8813":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"671e3aef8fbf488f86e2b72d058fd892":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e95d193ec23d4a26baf5b7859ca24327":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"671fd81bb57145eab93a79ca8e68ac1a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"173076887af346e0a10810fc54918346":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9b49631babb64dc8b69d41586741853e","IPY_MODEL_647ab9973c36439290efe3d990deda22","IPY_MODEL_1c7323cdbce0453890853a25c74d76fd"],"layout":"IPY_MODEL_9d1c9690070a43829a4af992f6cf3ef0"}},"9b49631babb64dc8b69d41586741853e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_012c04c5a88045ff936476ab10281a7b","placeholder":"​","style":"IPY_MODEL_5156dd5956054e67b07762bc74808e4f","value":"tokenizer.json: 100%"}},"647ab9973c36439290efe3d990deda22":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1b22ff7ccdd9446ea83127bf311e9be2","max":466062,"min":0,"orientation":"horizontal","style":"IPY_MODEL_29a00b7411954f17a427081fc6bd031c","value":466062}},"1c7323cdbce0453890853a25c74d76fd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2a8081c61dbd4799a3bdc356a0eaa628","placeholder":"​","style":"IPY_MODEL_37bf44fd484b404f90b1b2ed018f15ef","value":" 466k/466k [00:00&lt;00:00, 1.08MB/s]"}},"9d1c9690070a43829a4af992f6cf3ef0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"012c04c5a88045ff936476ab10281a7b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5156dd5956054e67b07762bc74808e4f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1b22ff7ccdd9446ea83127bf311e9be2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"29a00b7411954f17a427081fc6bd031c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2a8081c61dbd4799a3bdc356a0eaa628":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"37bf44fd484b404f90b1b2ed018f15ef":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0cea9626ff464b67a5da45d08ac6c359":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_eb3bde2a638d4dbfb6c617dd9cf03a06","IPY_MODEL_b2a6a85bbbff498ba5155609fac9f5af","IPY_MODEL_49b08c28396a41948a0bb34230827071"],"layout":"IPY_MODEL_321bad42444f4ff69b237225d03c0fb0"}},"eb3bde2a638d4dbfb6c617dd9cf03a06":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_91497854ecc7414fae1f5ad8965dd415","placeholder":"​","style":"IPY_MODEL_1991157bc6d64185946313288b6aae03","value":"Map: 100%"}},"b2a6a85bbbff498ba5155609fac9f5af":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9627a8c90ad045e9ae6665b25981f15e","max":25000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3d685520be0e45efb940a0bceb0f2e0d","value":25000}},"49b08c28396a41948a0bb34230827071":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3f3bafee9e2b49ec8db578739d744917","placeholder":"​","style":"IPY_MODEL_2f2b00df348c4443a38159b554c4da34","value":" 25000/25000 [00:25&lt;00:00, 911.02 examples/s]"}},"321bad42444f4ff69b237225d03c0fb0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"91497854ecc7414fae1f5ad8965dd415":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1991157bc6d64185946313288b6aae03":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9627a8c90ad045e9ae6665b25981f15e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3d685520be0e45efb940a0bceb0f2e0d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3f3bafee9e2b49ec8db578739d744917":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2f2b00df348c4443a38159b554c4da34":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d07ba3e3ac5a460ba74447cfee7411b2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a54f76afbcdb464caf49f9de39bbbe51","IPY_MODEL_afe712f073a04ee0bee6c26d82932a3a","IPY_MODEL_6c6432f06bd14e548bda87ac145dca56"],"layout":"IPY_MODEL_87896aaa439040409e851f8e14c6465d"}},"a54f76afbcdb464caf49f9de39bbbe51":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_43ce4e82cca64d18a996f2e5b95b7281","placeholder":"​","style":"IPY_MODEL_2dbdcb3d788544a881d4b202bb9dd523","value":"Map: 100%"}},"afe712f073a04ee0bee6c26d82932a3a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_46d515aa470844beaea1c6cdc91374ce","max":25000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f43c68f6b37041e89af6129466080977","value":25000}},"6c6432f06bd14e548bda87ac145dca56":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f2e8ea4b505b41aeb2c56e58084b8868","placeholder":"​","style":"IPY_MODEL_d9e17d38545848af90b79f202006525d","value":" 25000/25000 [00:22&lt;00:00, 1239.07 examples/s]"}},"87896aaa439040409e851f8e14c6465d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"43ce4e82cca64d18a996f2e5b95b7281":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2dbdcb3d788544a881d4b202bb9dd523":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"46d515aa470844beaea1c6cdc91374ce":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f43c68f6b37041e89af6129466080977":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f2e8ea4b505b41aeb2c56e58084b8868":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d9e17d38545848af90b79f202006525d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"90e20fd5ada84b2597137a40ff09d413":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_db2bec212a1f4301a3505b77e981788d","IPY_MODEL_cfff565bb63e4154926d31b3be2ec9b6","IPY_MODEL_7768885e97cd4b618358cd37ef9d3261"],"layout":"IPY_MODEL_5ffc443c38ae41389cbf9fae71443aa8"}},"db2bec212a1f4301a3505b77e981788d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b362eb62a87f4f6594e448dc2e21d5ec","placeholder":"​","style":"IPY_MODEL_c6d8352dc6354b4d9ee781b866ffca5f","value":"Map: 100%"}},"cfff565bb63e4154926d31b3be2ec9b6":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_23651dd9b6684d68988bedab0c94cdd3","max":50000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_436897133fbf46689fba6b5928c0bff5","value":50000}},"7768885e97cd4b618358cd37ef9d3261":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dfa4f8ddfa4c4081a00df888bb0cb166","placeholder":"​","style":"IPY_MODEL_715147986d324ec3a95323640086e395","value":" 50000/50000 [00:51&lt;00:00, 748.94 examples/s]"}},"5ffc443c38ae41389cbf9fae71443aa8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b362eb62a87f4f6594e448dc2e21d5ec":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c6d8352dc6354b4d9ee781b866ffca5f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"23651dd9b6684d68988bedab0c94cdd3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"436897133fbf46689fba6b5928c0bff5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"dfa4f8ddfa4c4081a00df888bb0cb166":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"715147986d324ec3a95323640086e395":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}